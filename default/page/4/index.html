<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">



  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Chen's Homepage" type="application/atom+xml" />






<meta property="og:type" content="website">
<meta property="og:title" content="Chen&#39;s Homepage">
<meta property="og:url" content="http://example.com/default/page/4/index.html">
<meta property="og:site_name" content="Chen&#39;s Homepage">
<meta property="og:locale">
<meta property="article:author" content="Chen jiayuan">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/page/4/"/>





  <title>Chen's Homepage</title>
  








<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    
    <a target="_blank" rel="noopener" href="https://github.com/Bbchen229" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chen's Homepage</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Hello AI</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/09/p1-position-encoding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/09/p1-position-encoding/" itemprop="url">Encoding Word Order in Complex Embeddings [ICLR2020]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-09T21:29:54+08:00">
                2021-10-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  1.6k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  6
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.12333.pdf">Encoding Word Order in Complex Embeddings</a><br />
Code: <a target="_blank" rel="noopener" href="https://github.com/iclr-complex-order/complex-order">link</a></p>
<h2 id="概要">概要</h2>
<p>针对位置编码提出的改进，切入点新颖高效且为位置编码带来了一定的具体意义和可解释性。传统的位置嵌入捕获单个单词的位置，而不是单个单词位置之间的有序关系(例如邻接关系或优先级)。本文提出的方法建模单词的全局绝对位置和它们的顺序关系，将以前定义为独立向量的词嵌入推广到变量(位置)上的连续词函数。每个单词的表示会随着位置的增加而移动。因此，在连续函数中，不同位置的词表示可以相互关联。将这些函数的通解推广到复值域，得到了更丰富的表示。作者在文本分类、机器翻译和语言模型方面进行实验，取得了良好的表现。</p>
<h2 id="positional-encoding">Positional encoding</h2>
<p>Positional encoding 位置编码在transformer中用于存储位置信息（由于self-attention没法获取序列位置的信息），此外BERT中encoding部分也包含了位置编码。对于位置编码，本能的想法是针对序列中的每个位置必须是独一无二的，且不受序列长度的影响。常见的positional encoding的方法有:</p>
<ul>
<li>绝对（正弦）位置编码（Sinusoidal Position Encoding）</li>
<li>相对位置编码（Relative Position Representations）</li>
<li>可学习位置编码</li>
</ul>
<h3 id="正弦位置编码">正弦位置编码</h3>
<p>Transformer中使用的就是这种编码，实际上具体编码过程使用了正弦和余弦。具体公式为： <span class="math display">\[
\begin{aligned}
P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}} \right) \\
P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}} \right)
\end{aligned}
\]</span> 其中<span class="math inline">\(d_{model}\)</span>为输入词向量的维度。如d(model)=128,那么位置3对应的位置向量为 <span class="math display">\[
\left[\sin \left(3 / 10000^{0 / 128}\right), \cos \left(3 / 10000^{1 / 128}\right), \sin \left(3 / 10000^{2 / 28}\right), \cos \left(3 / 10000^{3 / 28}\right), \ldots\right]
\]</span> 在具体的应用时可能前一部分用正弦后一部分用余弦。</p>
<p><img src="/images/pe4.png" title="Bert中的位置编码" /></p>
<h3 id="相对位置编码">相对位置编码</h3>
<p>Todo<br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a></p>
<h3 id="可学习位置编码">可学习位置编码</h3>
<p>Todo</p>
<h2 id="intro">Intro</h2>
<p>本文的重点在于建模文本信息中额外的词的内部顺序和相邻关系，对比原本位置编码方式仅编码词的位置。模型将之前定义为独立向量的词嵌入扩展为位置自变量上的连续函数。在一个连续函数中，不同位置的词表示可以相互关联。</p>
<p><img src="/images/pe1.png" /></p>
<h2 id="methodology">Methodology</h2>
<p>类似于Word Embedding，位置编码（PE）定义了一个映射关系，将词的序列索引映射为一个向量。<span class="math inline">\(f_{n e}: \mathbb{N} \rightarrow \mathbb{R}^{D}\)</span>。最终某个词的embedding通常表示为为词向量和位置向量的和： <span class="math display">\[f(j, p o s)=f_{w e}(j)+f_{p e}(p o s)\]</span></p>
<p>论文中提出了一个位置独立问题（position independence problem），即位置编码无法捕获相邻词以及其顺序之间的潜在关系。而当后续用于特征处理的网络对这类信息不敏感时，这一问题就会限制整个模型的表达能力。相对位置编码针对这一问题进行了一定的研究，但其无法涵盖整个序列域。</p>
<h3 id="性质">性质</h3>
<p>论文指出了在位置编码中建立词序模型所必需的性质。<br />
由于位置向量中每个维度的值都是根据离散的位置index得到的，这使得位置间有序关系建模变得困难，因此需要根据位置索引构建一个连续的函数（以在每个维度中表示一个特定的单词？） <span class="math display">\[
f(j, \text { pos })=\boldsymbol{g}_{j}(\text { pos }) \in \mathbb{R}^{D}
\]</span> <span class="math inline">\(g_j\)</span>即<span class="math inline">\(\boldsymbol{g}_{w e}(j) \in(\mathcal{F})^{D}\)</span>，词<span class="math inline">\(w_j\)</span>在pos位置可以表示为 <span class="math display">\[
\left[g_{j, 1}(\operatorname{pos}), g_{j, 2}(\operatorname{pos}), \ldots, g_{j, D}(\text { pos })\right] \in \mathbb{R}^{D}
\]</span> 当词<span class="math inline">\(w_j\)</span>从pos位置转到pos’位置时，只需要改变自变量的值而不需要改变映射函数<span class="math inline">\(g_j\)</span>。</p>
<h3 id="函数">函数</h3>
<p>由于实数也被囊括在复数域中，且前人有相关工作（详见论文原文Section2.2）验证了复数域所具有的更强大的表达能力，作者将模型拓展到了复数域。对于理想的映射函数，论文中提出了两条性质，即:</p>
<ul>
<li>Position-free offset transformation</li>
<li>Boundedness</li>
</ul>
<p>变换函数<span class="math inline">\(Transform\)</span>需满足对于任何pos，有 <span class="math display">\[
g(p o s+n)=\operatorname{Transform}_{n}(g(p o s))
\]</span> 满足等式的变换函数被称为witness，而满足这一条件的映射函数<span class="math inline">\(g_j\)</span>则被称为<em>linearly witnessed</em>。规定Transform <span class="math inline">\((n\)</span>, pos <span class="math inline">\()=\)</span> Transform <span class="math inline">\(_{n}(\)</span> pos <span class="math inline">\()=w(n)\)</span>。另外，映射函数<span class="math inline">\(g_j\)</span>需要有界。</p>
<p>而后作者证明了满足上述性质的映射函数唯一解为 <span class="math display">\[
g(p o s)=z_{2} z_{1}^{p o s} \text { for } z_{1}, z_{2} \in \mathbb{C} \text { with }\left|z_{1}\right| \leq 1
\]</span> 对于任意的 <span class="math inline">\(z \in \mathbb{C}\)</span>, 我们可以写成 <span class="math inline">\(z=r e^{i \theta}=r(\cos \theta+i \sin \theta)\)</span>，因此上式可写为： <span class="math display">\[g(p o s)=z_{2} z_{1}^{p o s}=r_{2} e^{i \theta_{2}}\left(r_{1} e^{i \theta_{1}}\right)^{p o s}=r_{2} r_{1}^{p o s} e^{i\left(\theta_{2}+\theta_{1} p o s\right)} \quad$ subject to $\left|r_{1}\right| \leq 1\]</span></p>
<p>(...跳过证明和优化过程)</p>
<p>最终的位置编码函数<span class="math inline">\(f(j\)</span>, pos <span class="math inline">\()\)</span>为 <img src="/images/pe2.png" /> <span class="math inline">\(j\)</span>代表单词（索引），<span class="math inline">\(pos\)</span>表示位置索引。<br />
对于embedding中的每一维度，都有各自的参数，振幅r、频率p、初相<span class="math inline">\(\theta\)</span>，这些参数是trainable的。此外，周期/频率决定了单词对位置的敏感程度。当周期很短，则说明嵌入将对position高度敏感。注意，振幅、频率是与postion（自变量）无关的，与单词和维度有关。此时，word embedding可以用这些参数来表示（维度与positional embedding维度相同）。</p>
<h2 id="实验">实验</h2>
<p>作者在文本分类、机器翻译和语言模型几个任务上进行了实验，分别用Fasttext、LSTM、CNN、Transformer作为模型的backbone，而后使用不同的位置编码方法以及本文的Complex-order编码方法进行embedding，对比几个实验结果均取得了可观的提升。而计算开销（时间）上并没有显著的增加。 <img src="/images/pe3.png" title="部分实验结果" /> 实验基于tensorflow，目前没有pytorch版本，笔者将会尝试将其迁移到pytorch框架下并开源。</p>
<h2 id="相关工作">相关工作</h2>
<p>Vanilla Position Embeddings<br />
Trigonometric Position Embeddings<br />
Todo</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/29/textGCN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/29/textGCN/" itemprop="url">Understanding and Improving Graph Neural Network via Variational Inference [GNN]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-29T22:24:53+08:00">
                2021-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index">
                    <span itemprop="name">Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  9
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <center>
<font size = 4> <strong>Understanding and Improving Graph Neural Network via Variational Inference</strong></font>
</center>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/29/word2vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/29/word2vec/" itemprop="url">word2vec</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-29T00:10:00+08:00">
                2021-09-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  0
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/20/sort-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/20/sort-1/" itemprop="url">sort algorithm 1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-20T23:01:57+08:00">
                2021-09-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Leetcode/" itemprop="url" rel="index">
                    <span itemprop="name">Leetcode</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  187
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="sorti">Sort(i)</h1>
<p>三种复杂度为<span class="math inline">\(O(n^2)\)</span>的排序算法:</p>
<ul>
<li>冒泡排序</li>
<li>选择排序</li>
<li>插入排序</li>
</ul>
<h2 id="bubble-sort">Bubble sort</h2>
<p>“一趟一趟来” <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span>(<span class="params">ls</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-i-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> ls[j] &gt; ls[j+<span class="number">1</span>]:</span><br><span class="line">                ls[j+<span class="number">1</span>],ls[j]=ls[j],ls[j+<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span>(ls)</span><br></pre></td></tr></table></figure></p>
<h2 id="select-sort">Select sort</h2>
<p>&quot;一个一个排&quot; <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_sort</span>(<span class="params">ls</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-<span class="number">1</span>):</span><br><span class="line">        min_loc = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-<span class="number">1</span>-i):</span><br><span class="line">            <span class="keyword">if</span> ls[i+j]&lt;ls[min_loc]:</span><br><span class="line">                min_loc =i+j</span><br><span class="line">        ls[min_loc],ls[i] = ls[i],ls[min_loc]</span><br><span class="line">    <span class="built_in">print</span>(ls)</span><br></pre></td></tr></table></figure></p>
<h2 id="insert-sort">Insert sort</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_select</span>(<span class="params">ls</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(ls)):</span><br><span class="line">        j = i-<span class="number">1</span></span><br><span class="line">        tmp = ls[i]</span><br><span class="line">        <span class="keyword">while</span> j&gt;=<span class="number">0</span> <span class="keyword">and</span> ls[j]&gt;tmp:</span><br><span class="line">            ls[j+<span class="number">1</span>]=ls[j]</span><br><span class="line">            j-=<span class="number">1</span></span><br><span class="line">        ls[j+<span class="number">1</span>] = tmp</span><br><span class="line">    <span class="built_in">print</span>(ls)</span><br></pre></td></tr></table></figure>
<h1 id="sortii">Sort(ii)</h1>
<p>三种复杂度为<span class="math inline">\(O(nlogn)\)</span>的排序算法:</p>
<ul>
<li>快速排序</li>
<li>归并排序</li>
<li>堆排序</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/25/dae%E7%9A%84%E5%89%AF%E6%9C%AC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/08/25/dae%E7%9A%84%E5%89%AF%E6%9C%AC/" itemprop="url">Linear Regression (python & pytorch)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-08-25T23:21:47+08:00">
                2021-08-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Preliminary-AI/" itemprop="url" rel="index">
                    <span itemprop="name">Preliminary AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  0
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/25/test/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/08/25/test/" itemprop="url">search algorithm</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-08-25T22:42:55+08:00">
                2021-08-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Leetcode/" itemprop="url" rel="index">
                    <span itemprop="name">Leetcode</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  79
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="linear-search-and-binary-search">Linear Search and Binary Search</h1>
<h2 id="linear-search">Linear search</h2>
<p>Time complexity：<span class="math inline">\(O(n)\)</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_search</span>(<span class="params">ls,val</span>):</span></span><br><span class="line">    <span class="keyword">for</span> index,value <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        <span class="keyword">if</span> value == val:</span><br><span class="line">            <span class="keyword">return</span> index</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure></p>
<h2 id="binary-search">Binary search</h2>
<p>Time complexity：<span class="math inline">\(O(log n)\)</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span>(<span class="params">ls,val</span>):</span></span><br><span class="line">    left = <span class="number">0</span>  <span class="comment">#左指针</span></span><br><span class="line">    right = <span class="built_in">len</span>(ls)-<span class="number">1</span>  <span class="comment">#右指针</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = (left+right)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> ls[mid] == val:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="keyword">elif</span> ls[mid]&lt;val:</span><br><span class="line">            left = mid+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right = mid-<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/16/test-my-site/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/08/16/test-my-site/" itemprop="url">Transformer & Bert</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-08-16T22:58:35+08:00">
                2021-08-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  1.5k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  8
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>Pre-training of Deep Bidirectional Transformers for Language Understanding</strong></p>
<h1 id="transfromer">Transfromer</h1>
<p>关于Bert，多的不说了，网上的扫盲帖、详解帖也一大堆。我们针对源码来一步一步的看。<br />
Transformer部分的源码来自于<a target="_blank" rel="noopener" href="https://github.com/tunz/transformer-pytorch/tree/e7266679f0b32fd99135ea617213f986ceede056">pytorch-transformer</a></p>
<p>首先是Multi-head Self-Attention，这也是Transformer中最重要的一部分。 <span class="math display">\[
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\]</span></p>
<p><img src="/images/multihopQA/1.png" title="Self-Attention" /></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size, dropout_rate, head_size=<span class="number">8</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.head_size = head_size</span><br><span class="line"></span><br><span class="line">        self.att_size = att_size = hidden_size // head_size</span><br><span class="line">        self.scale = att_size ** -<span class="number">0.5</span>     <span class="comment"># d_k ^ 0.5</span></span><br><span class="line"></span><br><span class="line">        self.linear_q = nn.Linear(hidden_size, head_size * att_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.linear_k = nn.Linear(hidden_size, head_size * att_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.linear_v = nn.Linear(hidden_size, head_size * att_size, bias=<span class="literal">False</span>)</span><br><span class="line">        initialize_weight(self.linear_q)</span><br><span class="line">        initialize_weight(self.linear_k)</span><br><span class="line">        initialize_weight(self.linear_v)</span><br><span class="line"></span><br><span class="line">        self.att_dropout = nn.Dropout(dropout_rate)</span><br><span class="line"></span><br><span class="line">        self.output_layer = nn.Linear(head_size * att_size, hidden_size,</span><br><span class="line">                                      bias=<span class="literal">False</span>)</span><br><span class="line">        initialize_weight(self.output_layer)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q, k, v, mask, cache=<span class="literal">None</span></span>):</span></span><br><span class="line">        orig_q_size = q.size()</span><br><span class="line"></span><br><span class="line">        d_k = self.att_size</span><br><span class="line">        d_v = self.att_size</span><br><span class="line">        batch_size = q.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)</span></span><br><span class="line">        </span><br><span class="line">        q = self.linear_q(q).view(batch_size, -<span class="number">1</span>, self.head_size, d_k)</span><br><span class="line">        <span class="keyword">if</span> cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> <span class="string">&#x27;encdec_k&#x27;</span> <span class="keyword">in</span> cache:</span><br><span class="line">            k, v = cache[<span class="string">&#x27;encdec_k&#x27;</span>], cache[<span class="string">&#x27;encdec_v&#x27;</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            k = self.linear_k(k).view(batch_size, -<span class="number">1</span>, self.head_size, d_k)</span><br><span class="line">            v = self.linear_v(v).view(batch_size, -<span class="number">1</span>, self.head_size, d_v)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                cache[<span class="string">&#x27;encdec_k&#x27;</span>], cache[<span class="string">&#x27;encdec_v&#x27;</span>] = k, v</span><br><span class="line"></span><br><span class="line">        q = q.transpose(<span class="number">1</span>, <span class="number">2</span>)                  <span class="comment"># [b, h, q_len, d_k]</span></span><br><span class="line">        v = v.transpose(<span class="number">1</span>, <span class="number">2</span>)                  <span class="comment"># [b, h, v_len, d_v]</span></span><br><span class="line">        k = k.transpose(<span class="number">1</span>, <span class="number">2</span>).transpose(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># [b, h, d_k, k_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scaled Dot-Product Attention.</span></span><br><span class="line">        <span class="comment"># Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V</span></span><br><span class="line">        q.mul_(self.scale)</span><br><span class="line">        x = torch.matmul(q, k)  <span class="comment"># [b, h, q_len, k_len]</span></span><br><span class="line">        x.masked_fill_(mask.unsqueeze(<span class="number">1</span>), -<span class="number">1e9</span>)</span><br><span class="line">        x = torch.softmax(x, dim=<span class="number">3</span>)</span><br><span class="line">        x = self.att_dropout(x)</span><br><span class="line">        x = x.matmul(v)  <span class="comment"># [b, h, q_len, attn]</span></span><br><span class="line"></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous()  <span class="comment"># [b, q_len, h, attn]</span></span><br><span class="line">        x = x.view(batch_size, -<span class="number">1</span>, self.head_size * d_v)</span><br><span class="line"></span><br><span class="line">        x = self.output_layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> x.size() == orig_q_size</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_size, filter_size, dropout_rate</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)</span><br><span class="line">        self.self_attention_dropout = nn.Dropout(dropout_rate)</span><br><span class="line"></span><br><span class="line">        self.ffn_norm = nn.LayerNorm(hidden_size, eps=<span class="number">1e-6</span>)</span><br><span class="line">        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)</span><br><span class="line">        self.ffn_dropout = nn.Dropout(dropout_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span>  <span class="comment"># pylint: disable=arguments-differ</span></span><br><span class="line">        y = self.self_attention_norm(x)</span><br><span class="line">        y = self.self_attention(y, y, y, mask)</span><br><span class="line">        y = self.self_attention_dropout(y)</span><br><span class="line">        x = x + y   <span class="comment">#skip-connection </span></span><br><span class="line"></span><br><span class="line">        y = self.ffn_norm(x)</span><br><span class="line">        y = self.ffn(y)</span><br><span class="line">        y = self.ffn_dropout(y)</span><br><span class="line">        x = x + y</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>Decoder的结构与Encoder基本类似，不过输入还包含Encoder的输出以及之前序列的输出结果，另外Decoder的自注意力层只允许关注输出序列之前的位置（通过-inf在self-attention计算中的softmax部分进行mask）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, i_vocab_size, t_vocab_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                 n_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 hidden_size=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 filter_size=<span class="number">2048</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 dropout_rate=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 share_target_embedding=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 has_inputs=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 src_pad_idx=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 trg_pad_idx=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.emb_scale = hidden_size ** <span class="number">0.5</span></span><br><span class="line">        self.has_inputs = has_inputs</span><br><span class="line">        self.src_pad_idx = src_pad_idx</span><br><span class="line">        self.trg_pad_idx = trg_pad_idx</span><br><span class="line"></span><br><span class="line">        self.t_vocab_embedding = nn.Embedding(t_vocab_size, hidden_size)</span><br><span class="line">        nn.init.normal_(self.t_vocab_embedding.weight, mean=<span class="number">0</span>,</span><br><span class="line">                        std=hidden_size**-<span class="number">0.5</span>)</span><br><span class="line">        self.t_emb_dropout = nn.Dropout(dropout_rate)</span><br><span class="line">        self.decoder = Decoder(hidden_size, filter_size,</span><br><span class="line">                               dropout_rate, n_layers)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> has_inputs:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> share_target_embedding:</span><br><span class="line">                self.i_vocab_embedding = nn.Embedding(i_vocab_size,</span><br><span class="line">                                                      hidden_size)</span><br><span class="line">                nn.init.normal_(self.i_vocab_embedding.weight, mean=<span class="number">0</span>,</span><br><span class="line">                                std=hidden_size**-<span class="number">0.5</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.i_vocab_embedding = self.t_vocab_embedding</span><br><span class="line"></span><br><span class="line">            self.i_emb_dropout = nn.Dropout(dropout_rate)</span><br><span class="line"></span><br><span class="line">            self.encoder = Encoder(hidden_size, filter_size,</span><br><span class="line">                                   dropout_rate, n_layers)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># For positional encoding</span></span><br><span class="line">        num_timescales = self.hidden_size // <span class="number">2</span></span><br><span class="line">        max_timescale = <span class="number">10000.0</span></span><br><span class="line">        min_timescale = <span class="number">1.0</span></span><br><span class="line">        log_timescale_increment = (</span><br><span class="line">            math.log(<span class="built_in">float</span>(max_timescale) / <span class="built_in">float</span>(min_timescale)) /</span><br><span class="line">            <span class="built_in">max</span>(num_timescales - <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        inv_timescales = min_timescale * torch.exp(</span><br><span class="line">            torch.arange(num_timescales, dtype=torch.float32) *</span><br><span class="line">            -log_timescale_increment)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;inv_timescales&#x27;</span>, inv_timescales)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, targets</span>):</span></span><br><span class="line">        enc_output, i_mask = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> self.has_inputs:</span><br><span class="line">            i_mask = utils.create_pad_mask(inputs, self.src_pad_idx)</span><br><span class="line">            enc_output = self.encode(inputs, i_mask) </span><br><span class="line"></span><br><span class="line">        t_mask = utils.create_pad_mask(targets, self.trg_pad_idx)</span><br><span class="line">        target_size = targets.size()[<span class="number">1</span>]</span><br><span class="line">        t_self_mask = utils.create_trg_self_mask(target_size,</span><br><span class="line">                                                 device=targets.device)</span><br><span class="line">        <span class="keyword">return</span> self.decode(targets, enc_output, i_mask, t_self_mask, t_mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, inputs, i_mask</span>):</span></span><br><span class="line">        <span class="comment"># Input embedding</span></span><br><span class="line">        input_embedded = self.i_vocab_embedding(inputs)</span><br><span class="line">        input_embedded.masked_fill_(i_mask.squeeze(<span class="number">1</span>).unsqueeze(-<span class="number">1</span>), <span class="number">0</span>)</span><br><span class="line">        input_embedded *= self.emb_scale</span><br><span class="line">        input_embedded += self.get_position_encoding(inputs)</span><br><span class="line">        input_embedded = self.i_emb_dropout(input_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.encoder(input_embedded, i_mask)</span><br><span class="line">    <span class="comment">#mask: sequence mask + padding mask</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, targets, enc_output, i_mask, t_self_mask, t_mask,</span></span></span><br><span class="line"><span class="params"><span class="function">               cache=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># target embedding</span></span><br><span class="line">        target_embedded = self.t_vocab_embedding(targets)</span><br><span class="line">        target_embedded.masked_fill_(t_mask.squeeze(<span class="number">1</span>).unsqueeze(-<span class="number">1</span>), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Shifting</span></span><br><span class="line">        target_embedded = target_embedded[:, :-<span class="number">1</span>]</span><br><span class="line">        target_embedded = F.pad(target_embedded, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        target_embedded *= self.emb_scale</span><br><span class="line">        target_embedded += self.get_position_encoding(targets)</span><br><span class="line">        target_embedded = self.t_emb_dropout(target_embedded)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decoder</span></span><br><span class="line">        decoder_output = self.decoder(target_embedded, enc_output, i_mask,</span><br><span class="line">                                      t_self_mask, cache)</span><br><span class="line">        <span class="comment"># linear</span></span><br><span class="line">        output = torch.matmul(decoder_output,</span><br><span class="line">                              self.t_vocab_embedding.weight.transpose(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_position_encoding</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        max_length = x.size()[<span class="number">1</span>]</span><br><span class="line">        position = torch.arange(max_length, dtype=torch.float32,</span><br><span class="line">                                device=x.device)</span><br><span class="line">        scaled_time = position.unsqueeze(<span class="number">1</span>) * self.inv_timescales.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)],</span><br><span class="line">                           dim=<span class="number">1</span>)</span><br><span class="line">        signal = F.pad(signal, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, self.hidden_size % <span class="number">2</span>))</span><br><span class="line">        signal = signal.view(<span class="number">1</span>, max_length, self.hidden_size)</span><br><span class="line">        <span class="keyword">return</span> signal</span><br></pre></td></tr></table></figure>
<h1 id="bert">Bert</h1>
<p>先来看一下论文的摘要：</p>
<blockquote>
<p>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from <strong>unlabeled text</strong> by jointly conditioning on both <strong>left and right context in all layers</strong>. As a result, the pre-trained BERT model can be <strong>finetuned with just one additional output layer</strong> to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.</p>
</blockquote>
<p>这里可以把Bert的核心概括为几个点，首先是Pre-trained model——借助未标记数据进行训练，针对具体任务做fine-tune，其次是编码上下文，而不是单侧的。</p>
<p><img src="/images/multihopQA/9.png" /></p>
<p>上图展现了几个Pre-train model的结构对比，ELMo借助了LSTM作为编码器，介于LSTM对比Self-attention的劣势，这一模型可以看作只关注上下文的Bert，而GPT则是单向语言模型，对比Bert相当于把Encoder layer换为Decoder layer，也就获得了对应的单向视野。而Bert的Transformer，其实也就是相当于n个Encoder的堆叠。</p>
<blockquote>
<p>the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bert中的Transform block</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> .attention <span class="keyword">import</span> MultiHeadedAttention</span><br><span class="line"><span class="keyword">from</span> .utils <span class="keyword">import</span> SublayerConnection, PositionwiseFeedForward</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Bidirectional Encoder = Transformer (self-attention)</span></span><br><span class="line"><span class="string">    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden, attn_heads, feed_forward_hidden, dropout</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param hidden: hidden size of transformer</span></span><br><span class="line"><span class="string">        :param attn_heads: head sizes of multi-head attention</span></span><br><span class="line"><span class="string">        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size</span></span><br><span class="line"><span class="string">        :param dropout: dropout rate</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)</span><br><span class="line">        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)</span><br><span class="line">        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)</span><br><span class="line">        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        x = self.input_sublayer(x, <span class="keyword">lambda</span> _x: self.attention.forward(_x, _x, _x, mask=mask))</span><br><span class="line">        x = self.output_sublayer(x, self.feed_forward)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>在Bert原论文中，Bert的Bidirectional还体现在具体的训练任务上：</p>
<blockquote>
<p>Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer.</p>
</blockquote>
<p>具体来说，训练任务包括两个，一是Masked Language Model，另一个是Next Sentence Prediction用于理解两个句子间的关系</p>
<p>对于具体下游任务来说，句子层面的我们可以取[CLS]的vector并做Softmax之类的操作。利用bert获取上下文相关的词向量则可以针对某个token，取其所在位置对应的某些层的hidden states，然后做特征融合（Feature-based）。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/16/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/08/16/hello-world/" itemprop="url">Markdown语法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-08-16T22:25:40+08:00">
                2021-08-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/" itemprop="url" rel="index">
                    <span itemprop="name">杂七杂八</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  360
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>由于Hexo博客的撰写需要用Markdown，虽然比Latex要简单点，但是平时用的比较少，这些杂七杂八的语法很难一下子全部记住，因此在这页博客中记录一下</p>
<h2 id="文字">文字</h2>
<h3 id="标题">标题</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="居中">居中</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;center&gt;这一行需要居中&lt;/center&gt;</span><br></pre></td></tr></table></figure>
<h3 id="字体">字体</h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">**（）** 加粗</span><br><span class="line">*（）* 斜体</span><br><span class="line">～～（）～～ 删除线</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;font face= “黑体” color=red size=7&gt;字体设置&lt;/font&gt; #size 1-7，浏览器默认3</span><br></pre></td></tr></table></figure>
<h2 id="引用">引用</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;这是引用文字</span><br></pre></td></tr></table></figure>
<h2 id="分割线">分割线</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">***</span><br></pre></td></tr></table></figure>
<h2 id="图片">图片</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![图片alt](图片地址 &#x27;&#x27;图片title&#x27;&#x27;)</span><br></pre></td></tr></table></figure>
<p>图片alt就是显示在图片下面的文字，相当于对图片内容的解释。 图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加</p>
<h2 id="链接">链接</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[超链接名](超链接地址 &quot;超链接title&quot;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">站内链接</span><br><span class="line">&#123;% post_link [博客名] title %&#125;</span><br></pre></td></tr></table></figure>
<h2 id="页内跳转">页内跳转</h2>
<p>分成两部：定义锚点、使用markdown语法跳转 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;span id=&quot;jump&quot;&gt; （跳转到的地方） &lt;/span&gt;</span><br><span class="line">[点击跳转](#jump)</span><br></pre></td></tr></table></figure></p>
<h2 id="列表">列表</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 列表内容</span><br><span class="line">+ 列表内容</span><br><span class="line">1. 列表内容</span><br></pre></td></tr></table></figure>
<h2 id="代码">代码</h2>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">`代码内容`</span><br><span class="line">\``` (防止打不出加个\转义一下)</span><br><span class="line">  代码...</span><br><span class="line">  代码...</span><br><span class="line">  代码...</span><br><span class="line">\```</span><br></pre></td></tr></table></figure>
<h2 id="数学公式">数学公式</h2>
<p>公式、希腊字母、上标下标等基本语法与latex类似，可参考<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/a0aa94ef8ab2">markdown数学公式</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">正文中$...$</span><br><span class="line">单行显示$$...$$</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/24/kg/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/05/24/kg/" itemprop="url">漫威人物知识图谱-MARVEL Knowledge Graph</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-05-24T18:16:44+08:00">
                2021-05-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Project/" itemprop="url" rel="index">
                    <span itemprop="name">Project</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  3.8k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  13
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="简介">简介</h1>
<p>本项目构建的漫威人物知识图谱基于美国漫威漫画公司制作的一系列漫画和电影组成的架空世界和共同世界。基于我们构建的知识图谱，用户可以结构化的查询人物信息，了解角色间的关系以及漫威超级英雄全貌，更深入地了解漫威宇宙。项目面向百科的中文人物领域知识构建图谱，所选人物范围为漫威人物，包括复仇者联盟、神盾局特工与反派等角色类别。最终通过neo4j数据库完成了整体框架的可视化。图谱数据基于百度百科、维基百科以及<a target="_blank" rel="noopener" href="https://graphics.straitstimes.com/STI/STIMEDIA/Interactives/2018/04/marvel-cinematic-universe-whos-who-interactive/index.html">漫威人物数据网页</a>。</p>
<h1 id="本体构建">本体构建</h1>
<p>本体是用于描述一个领域的术语集合，其组织结构是层次结构化的，可以作为一个知识库的骨架和基础。<br />
1. 领域：「漫威人物」；<br />
2. 基本术语： 「超级英雄」、「反派」等等概念<br />
3. 关系：超级英雄之间为朋友关系，反派与超级英雄之间为敌对关系，另外也存在一些情人/爱人关系。<br />
4.类别：复仇者联盟、神盾局特工、银河护卫队、超级反派、其他<br />
5.属性：中文/英文名、性别、登场作品、所属团队等</p>
<h1 id="事实抽取">事实抽取</h1>
<p>百科数据中，有许多关于漫威人物的信息，其中有不少冗余信息，如人物冗长的经历与生平等。以百度百科为例，有用的信息为人物之间的关系、阵营，人物的基本信息以及能力值的表格。</p>
<p>利用网上爬取的方式，在漫威人物网站中爬取人物人名信息。接着利用词条分类的方法，观察各个词条与漫威之间的关系，发现“漫威”二字本身不可用于唯一标识，但是“漫威旗下”或者“美国漫威漫画旗下”这些字眼却可以对漫威人物进行唯一性筛查。所以我们依据人名进入百度百科的多义词界面，根据关键词&quot;美国漫威漫画旗下&quot;或&quot;漫威旗下&quot;进行正则化筛选，从而得到有用的URL，将其保存于Excel表格之中，为之后的爬取工作提供正确的URL网址。 <img src="/images/kg/1.png" /></p>
<h2 id="人物信息抽取">人物信息抽取</h2>
<p>对于漫威人物的基本信息，我们首先利用百科的Infobox进行爬取。找到百科页面查看源网页得知，Infobox的值以basicInfo-item value作为标签存储，因此代码如下： <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">key = soup.find_all(class_=&quot;basicInfo-item name&quot;)</span><br><span class="line">val = soup.find_all(class_=&quot;basicInfo-item value&quot;)</span><br></pre></td></tr></table></figure> 这样一来，百度百科本身含有的人物基本信息就全部爬取下来了。除了基本信息以外，还有存储为表格格式的人物能力信息需要爬取。<br />
利用源网页信息中的标签可以知道，表格信息是存储在 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;table log-set-param=&quot;table\_view&quot; data-sort=&quot;sortDisabled&quot;&gt;</span><br></pre></td></tr></table></figure> 的标识里面，但用这样的方式直接爬取会爬取到所有的表格信息，从而带来了大量的无用信息。 因此我们制订了基于模版的筛选方法，只筛选符合模版智力-力量-速度-耐力-能量发射-战斗技能的表格，用该表格的表头信息进行字符串匹配以后成功得到了有用的能力值。</p>
<h2 id="人物图片爬取">人物图片爬取</h2>
<p>得到人物基本信息与关系三元组以后，我们将百科界面中漫威人物对应的图片爬取下来，存储于后端，在查询时显示出来，提供一个较好的可视化效果。 <img src="/images/kg/2.png" alt="&#39;爬取到的人物图片&#39;" /></p>
<h2 id="三元组">三元组</h2>
<p>对于构建的知识图谱中人物关系三元组，主要数据源于外网某一漫威人物数据库，我们爬取其网站中存储的三元组csv文件，构建如（ 美国队长, 朋友 , 钢铁侠 ）形式的人物关系三元组，但是由于该网站最后更新时间为2018年，且其人物关系是基于上映的漫威系列电影，导致该部分的人物关系存在时效滞后、内容不完整等问题。因此基于已有三元组关系，我们进一步通过DeepKE从爬取的百度百科中的角色经历中进行关系三元组抽取，将得到的三元组与原有的进行更正与补充。</p>
<p>DeepKE是基于深度学习的开源中文关系抽取工具，我们对其中基于CNN、Transformer、RNN三个网络的关系抽取模型进行实验。在去年知识表示与推理课程中，漆桂林老师带领我们进行了知识抽取框架实践，根据实验结果我们发现对于是否给定头尾实体，三元组抽取的效果会有很大差别。因此在进行抽取时，我们将已有三元组的主语和宾语作为head和tail。另外，在爬取的人物角色经历中，对于冗长的文本，我们使用正则表达式对主语和宾语都在某一句中出现的句子做关系抽取。</p>
<p>关系抽取数据集来源于苏州大学开源人物关系数据集。我们先对数据做预处理，筛除掉无效的字符标点，以及数字等冗余信息，并将实体对进行编码，这可以避免模型学习对实体词过分关注以至于导致bias。我们采用了one-hot编码，将每个token映射为一个id,构建词典存储所有的token。最终可以把关系抽取出来，此处展示几个范例。将得到的关系与原本的关系进行对比，若两者存在不同，则进行人工判断，确认哪个关系是准确的。</p>
<h1 id="类别推断">类别推断</h1>
<p>知识图谱中，类别信息(Type Information)是一种特殊的三元组， 其具体表现为: <span class="math display">\[
instance \stackrel{type}{\longrightarrow} concept
\]</span> 在项目构建的漫威人物知识图谱中，由于这一任务属于特定领域的知识构建，所以我们根据类别推断的方法和目标进行一定的延伸，使用类别推断来进行漫威人物所属的组织的判断，具体的类别包括复仇者联盟、神盾局特工、银河护卫队、超级反派等。</p>
<p>在知识图谱构建中，项目结合了两种类别信息推断的方式，包括: - Type Inference from Infoboxes - Type Inference from Text</p>
<p>分析从百度百科得到的infobox，部分漫威人物的infobox中有“所属团队”这一属性，对于有该属性的人物，我们通过爬取的网页信息，用xpath解析后使用正则表达式匹配这一属性，抽取出其属性值，即该人物所属团队。</p>
<p>部分人物的infobox中不存在“所属团队”这一属性。对于这些人物，我们选择从文本中进行类别提取，即通过识别轻量级语法模式进行提取。</p>
<p>从百度百科提取的人物infobox中，我们可以从人物经历中推断如否加入了诸如神盾局特工、复仇者联盟等团队，如 A 加入了 B（复仇者联盟、神盾局特工）,其判断的正则表达式如下： <span class="math display">\[
pattern = re.compile(&#39;(.*)[\u52a0][\u5165](.*)[\u590d][\u4ec7][\u8005](.*)\$&#39;)
\]</span></p>
<p>对所有数据使用上述算法处理后，对没有不符合上述类别的归类为其他，并对得到的类别信息进行数据清洗和检查，去除了百科文本中的几个错别字并进行同义词合并，得到五个类别，对类别进行编码后保存到json文件中，后续将使用该信息在Neo4j中进行可视化。</p>
<h1 id="知识融合">知识融合</h1>
<p>知识融合目标是融合各个层面（概念层、数据层）的知识 ，基本的问题都是研究怎样将来自多个来源的关于同一个实体或概念的描述信息融合起来</p>
<p>数据预处理阶段，原始数据的质量会直接影响到最终链接的结果，不同的知识库对同一实体的描述方式往往是不相同的，对这些数据进行归一化是提高后续链接精确度的重要步骤，我们用beautifulsoup解析爬取的网页，过滤掉多余信息，只保留infobox，基本信息等数据，整合为key-value的形式并保存在json文件中。得到json文件后，英文，注释，网页中原有的错误，空格，符号等问题使用正则表达式和手动替换的方式调整。<br />
由于中文维基百科的数据较少，所以我们的知识图谱以百度百科的数据为主体构建，将维基百科的数据融合进百度百科。</p>
<h1 id="问答">问答</h1>
<p>智能问答系统以一问一答形式，从数据库中定位用户所需要的提问知识，通过与用户进行交互，为用户提供个性化的信息服务。我们在知识抽取任务得到的三元组关系的基础上，附加知识融合得到的infobox以及从百科上爬取的漫威角色图片，实现了一个问答系统。</p>
<p>对于KB-QA问题，首要任务是如何将用户查询的语句转换成数据库可识别的问题，转换的同时还需要保证转换的高准确率。而高准确率的基础，则是表现良好的分词模型。</p>
<p>例如，当用户提出一个问题：&quot;绿巨人的爱人是谁？”时，我们需要对文本数据进行处理。通过进行分词、词性标注等操作，我们能提取出关键字。在上面的问句中，我们通过词性标注可以得到：</p>
<p><span class="math display">\[
[&#39;绿巨人/nr&#39;, &#39;的/deg&#39;, &#39;爱人/n&#39;, &#39;是/vc&#39;, &#39;谁/pn&#39;]
\]</span></p>
<p>类似地，用户所有的查询都可以进行同样操作。在阅读大量文献后，我们选择了两种模型进行实现。一种是被广泛使用的中文分词模型LTP，另一种是ACL2020新提出的命名实体识别模型FLAT。</p>
<p><strong>LTP</strong></p>
<p>LTP是基于词性标注、依存语法分析的一种分词模型。其本质可以视作一个感知机。这种通过结构化感知器训练的经典模型在多个中文分词数据集上都取得了较好的效果。</p>
<p><strong>FLAT</strong></p>
<p>在我们构建的知识图谱中，有许多实体词条在日常生活中易出现歧义。如对于&quot;美国队长&quot;一词，我们很容易将&quot;美国&quot;和&quot;队长&quot;分成两个实体。所以，在用户查询语句中，我们应该重点关注命名实体识别问题。</p>
<p>FLAT是一种表现SOTA的命名实体识别模型。它是一种基于汉字格结构和Transformer的模型。汉字格结构思想的提出已有较长时间。汉字格结构是指我们可以将一个句子与一个词典进行匹配，得到其中的潜词，从而得到一个有向无环图，其中每个节点都是一个字符或一个潜在的字。格包括句子中的一系列字符和可能的单词。它们不是按顺序排列的，单词的第一个字符和最后一个字符决定了它的位置。在汉字格结构的基础上，FLAT提出了一种新的编码方式，将一句话的格之间的位置编码并扩展成平面。句子的编码再送入transformer进行进行训练。<br />
我们在MSRA上进行了模型训练，模型测试准确率达到了0.94。最终我们结合FLAT、字典匹配、LTP进行人物名称识别和分词，共同完成了任务。</p>
<h1 id="推荐系统">推荐系统</h1>
<p>我们使用中文维基百科语料以及爬取到的漫威人物文本资料做了embedding，并根据word2vec构建了漫威人物推荐系统，输出与用户输入人物距离最近的人物。完整处理过程包括解析中文维基百科文件，繁体简体转化，数据清洗，中文分词，Gensim词嵌入模型训练。</p>
<p>对于中文分词模块，考虑人物名字以及特殊能力等名词分词存在较大难度，我们复现了SOTA中文分词，并与jieba分词的效果做了对比实验。</p>
<p>中文维基百科数据来源于 zhwiki-20210301-pages-articles.xml.bz2 文件。先用Gensim库中的WikiCorpus()函数对其进行处理，得到繁体中文维基语料的txt文件。对于繁体中文，我们需要将其转化为简体中文。应用python中的opencc库（Open Chinese convert）对繁体维基百科文件按行进行t2s处理，同时将txt文件中的 t, n等字符进行清洗，得到中文简体维基语料。另外在分词处理前，用iconv()函数处理文件中的非utf-8字符。</p>
<p>分词处理时，我们选择了Tian等人发表在ACL2020中的中文分词SOTA模型<span class="math inline">\(WMS_{EG}\)</span>，其在MSR、PKU等数据集上的OOV Recall比之前的模型提高了约3%。这一套分词模型在传统encoder-decoder模型中增加了Memory Network。我们复现模型时，选择ZEN作为encoder、crf作为decoder，并用项目提供的WMSeg.ZEN.PKU预训练模型进行分词，在应用前我们在简单语句上进行测试，分词结果良好。</p>
<p><img src="/images/kg/5.png" title="分词结果" /></p>
<p>根据得到的分词结果，我们使用开源的第三方Python工具包Gensim进行word2vec模型训练，设定词向量维度为400，模型为CBOW连续词袋模型。在服务器上，workers数量为32，训练时长约1h。</p>
<h1 id="可视化">可视化</h1>
<p>知识图谱构建完成后，我们将其进行了可视化展示。可视化展示基于Neo4j图形数据库。在于Neo4j框架下，每一个实体都以一个节点的形式呈现，整体数据以结构化数据存储在网络中。在图形数据库的基础上，我们进行了实体与实体间关系、知识问答的可视化实现。同时，为了能够更好地让知识图谱服务于用户，我们以网页的形式展示图形数据库。在前端部分，我们进行了一些CSS修饰工作。 <img src="/images/kg/3.png" /> <img src="/images/kg/4.png" title="人物关系全貌" /></p>
<h1 id="后记">后记</h1>
<p>本项目为知识图谱课程设计的小组作品。整个项目数据量不大，且所实现的功能也没有涉及到一些sota的模型或现在主流的技术，也借鉴了部分网上的开源代码，最后课程得分也只有88（跟我大学四年均分一样）。不过这个课程设计让我结实了一群有趣的朋友，收获了珍贵友谊，与之对于什么项目经历倒也显得微不足道了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/09/25/dae/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/09/25/dae/" itemprop="url">DAE for Action Quality Assessment(AQA)  [CV]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-09-25T23:21:47+08:00">
                2020-09-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index">
                    <span itemprop="name">Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  801
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><font  size=5>Auto-Encoding Score Distribution Regression for Action Quality Assessment</font></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.11029" class="uri">https://arxiv.org/abs/2111.11029</a></p>
<p><em>Action quality assessment (AQA) from videos is a challenging vision task since the relation between videos and action scores is difficult to model. Thus, action quality assessment has been widely studied in the literature. Traditionally, AQA task is treated as a regression problem to learn the underlying mappings between videos and action scores. More recently, the method of uncertainty score distribution learning (USDL) made success due to the introduction of label distribution learning (LDL). But USDL does not apply to dataset with continuous labels and needs a fixed variance in training. In this paper, to address the above problems, we further develop Distribution Auto-Encoder (DAE). DAE takes both advantages of regression algorithms and label distribution learning (LDL). Specifically, it encodes videos into distributions and uses the reparameterization trick in variational auto-encoders (VAE) to sample scores, which establishes a more accurate mapping between videos and scores. Meanwhile, a combined loss is constructed to accelerate the training of DAE. DAE-MT is further proposed to deal with AQA on multi-task datasets. We evaluate our DAE approach on MTL-AQA and JIGSAWS datasets. Experimental results on public datasets demonstrate that our method achieves state-of- the-arts under the Spearman’s Rank Correlation: 0.9449 on MTL-AQA and 0.73 on JIGSAWS.</em></p>
<h2 id="aqa">AQA</h2>
<p>Action Quality Accessment (AQA) automatically scores the quality of actions by analyzing features extracted from videos and images. It’s different from conventional action recognition problem. In the past few years, much work has been devoted to different AQA tasks, such as healthcare, sports video analysis and many others.</p>
<h2 id="related-work">Related Work</h2>
<p>Parmar <em>et al.</em> [1] proposed C3D-SVR and C3D-LSTM to predict the score of the Olympic events. Additionally, incremental-label training method was introduced to train the LSTM model based on the hypothesis that the final score is an aggregation of the sequential sub-action scores.</p>
<p>Tang <em>et al.</em> noticed the underlying ambiguity of action scores and then proposed an improved approach: uncertainty-aware score distribution learning (USDL) [2] to address this problem. USDL is designed based on label distribution learning (LDL), a general learning paradigm to solve problems with uncertainty and answer how much each label describes the instance.</p>
<h2 id="methoddae">Method:DAE</h2>
<p><img src="/images/dae1.png" title="The pipeline of DAE architecture contains two segments: video features extraction network (orange) and label distribution encoding network (pink)." /></p>
<h3 id="video-feature-extraction">Video Feature Extraction</h3>
<p>The input video is divided into n small clips by down-sampling. Then the clips are sent into I3D ConvNets for extracting features. The final features are synthesized by three fully-connected layers.</p>
<h3 id="auto-encoder-for-distribution-learning">Auto-Encoder for Distribution Learning</h3>
<p>Compared with the regression-based method and the label distribution learning method, our approach combines the two methods’ characteristics comprehensively. The action features are encoded into score distribution, and the final result is sampled from the auto-encoders output. This architecture en- ables learning a continuous distribution without loss in training procedure and quantifies the uncertainty of action score with high accuracy.</p>
<p>The encoder uses a simple but quite an efficient neural network, namely multi-layered perceptrons (MLPs), to encode mean and variance simultaneously. The input 1024-dimensional feature vector x is encoded into the parameters <span class="math inline">\(μ(x)\)</span> and <span class="math inline">\(σ^2(x)\)</span> via a neural network.</p>
<p>We take the action score as a random variable. Treating the action score as a random variable, we need to learn its score distribution and then sample the predicted score from the obtained distribution. <span class="math display">\[
p\left(y ; \mu(\boldsymbol{x}), \sigma^{2}(\boldsymbol{x})\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}(\boldsymbol{x})}} \exp \left(-\frac{(y-\mu(\boldsymbol{x}))^{2}}{2 \sigma^{2}(\boldsymbol{x})}\right)
\]</span></p>
<p>To generate a sample from Gaussian distributed y as the predicted score and make full use of the two parameters in the score distribution at the same time, we invoke the reparameterization trick. According to reparameterization trick in VAE [3], assume that <span class="math inline">\(z\)</span> is a random variable, and <span class="math inline">\(z \sim q(z ; \phi), \phi\)</span> is its parameter. We can express <span class="math inline">\(z\)</span> as a deterministic variable, <span class="math inline">\(z=g(\epsilon ; \phi), \epsilon\)</span> is an auxiliary variable with independent marginal <span class="math inline">\(p(\epsilon)\)</span>, and <span class="math inline">\(g(\cdot ; \phi)\)</span> is a deterministic function parameterized by <span class="math inline">\(\phi\)</span>.<br />
<span class="math display">\[
y=\mu(\boldsymbol{x})+\epsilon * \sigma^{2}(\boldsymbol{x})
\]</span></p>
<h2 id="experiments">Experiments</h2>
<p>We use Spearman’s rank correlation to measure the performance of our methods between the ground-truth and predicted score series. Spearman’s correlation is defined as: <span class="math display">\[
\rho=\frac{\sum_{i}\left(p_{i}-\bar{p}\right)\left(q_{i}-\bar{q}\right)}{\sqrt{\sum_{i}\left(p_{i}-\bar{p}\right)^{2} \sum_{i}\left(q_{i}-\bar{q}\right)^{2}}}
\]</span> <img src="/images/dae2.png" title="The results on JIGSAWS." /> <img src="/images/dae3.png" title="The results on MTL-AQA." /></p>
<p><img src="/images/dae4.png" title="Comparison of different distribution of different videos on MTL-AQA dataset." /></p>
<p>References (incomplete)<br />
[1] What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment<br />
[2] Uncertainty-aware score distribution learning for action quality assessment<br />
[3] Auto-encoding variational bayes</p>
<p>Cooperate with zby (seu) supervisor: xyf (seu)<br />
<a target="_blank" rel="noopener" href="https://github.com/InfoX-SEU/DAE-AQA">Github link</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/default/page/3/">&lt;</a><a class="page-number" href="/default/">1</a><span class="space">&hellip;</span><a class="page-number" href="/default/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/default/page/5/">5</a><a class="extend next" rel="next" href="/default/page/5/">&gt;</a>
  </nav>




          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.JPG"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">42</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Bbchen229" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:1109441357@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen jiayuan</span>

  
</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
