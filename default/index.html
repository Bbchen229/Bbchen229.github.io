<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">



  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Chen's Homepage" type="application/atom+xml" />






<meta property="og:type" content="website">
<meta property="og:title" content="Chen&#39;s Homepage">
<meta property="og:url" content="http://example.com/default/index.html">
<meta property="og:site_name" content="Chen&#39;s Homepage">
<meta property="og:locale">
<meta property="article:author" content="Chen jiayuan">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>Chen's Homepage</title>
  








<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    
    <a target="_blank" rel="noopener" href="https://github.com/Bbchen229" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chen's Homepage</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Hello AI</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/11/gnn-lm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/05/11/gnn-lm/" itemprop="url">GNN-LM Language Modeling based on Global Contexts via GNN [ICLR22]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-05-11T20:54:00+08:00">
                2022-05-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  534
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>ICLR2022 spotlight</p>
<p>论文总的目的是以空间换时间和精度，而具体的空间开销以图的形式存储。</p>
<p>本文提出了GNN-LM，将图神经网络与语言模型相结合，通过在整个训练语料库中引用相似的上下文，扩展了传统的语言模型。使用k近邻检索与输入的表示最相似的邻居，模型为每个输入构建了一个有向异构图，其中节点是来自输入上下文或检索到的邻居上下文的token，边表示token之间的连接。然后利用图神经网络从检索到的上下文中聚合信息，以解码下一个token。实验结果表明，GNN-LM在标准数据集中优于强基线，并且通过与kNN-LM结合，能够在WikiText-103上取得最优效果。</p>
<p><strong>从闭卷考试到开卷考试</strong> 对于传统的LM，我们会令神经网络学习数据中的知识，而在测试时这些数据是不可见的，这就类似“闭卷考试”。这时，影响模型测试的准确率的因素，或者说影响我们“考试成绩”的因素，就是是否能够准确的记住所有的数据/“知识点”。而作者提到，开卷考试一般比闭卷考试简单，因此令训练数据在测试时是可见的，我们在面对“考试题”的时候会去“书本”上查找最相关的知识点来解答。而这里知识点匹配的算法则是KNN-LM，得到的知识点构建图来回答问题。</p>
<p><img src="/images/gnn-lm1.png" title="pipeline" /></p>
<p>如上图右侧所示，对于输入的context，首先借助KNN-LM生成近邻<span class="math inline">\(\mathcal{N}\left(\boldsymbol{c}_{t}\right)=\left\{\boldsymbol{c}_{t_{1}}^{(1)}, \ldots, \boldsymbol{c}_{t_{k}}^{(k)}\right\}\)</span>。而后对检索出来的语句和测试语句构建一个异构图，包含<span class="math inline">\(a_0\)</span>和<span class="math inline">\(a_n\)</span>两类节点，分别对应查询语句中的token和近邻的token，接下来对构建的图应用GNN，得到输入语句的最后一个token的representation用以预测</p>
<p><img src="/images/gnn-lm2.png" title="性能对比" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/11/22511/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/05/11/22511/" itemprop="url">HETERMPC A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations [ACL22]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-05-11T10:34:15+08:00">
                2022-05-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  1.5k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.08500.pdf">HETERMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations</a></p>
<h1 id="intro">Intro</h1>
<h2 id="multi-party-conversations">Multi-Party Conversations</h2>
<p>最基本的对话系统（dialogue system）基于两个对话者，这类对话被称为two-party conversation。与之相对应，更复杂、更实际的mult-party conversation意味着更多的对话参与者。在two-party conversation中，对话为一人一句交替发言的序列。而MPC中的话语可以被任何人说出，也可以在这个对话中对任何人讲话。</p>
<p><img src="/images/mpc/1.png" /></p>
<p>如上图所示，MPC更适合被建模为graphical information flow，而不是传统的sequence。因此，对比two-party conversation用的Seq2Seq模型，似乎用graph-structured network建模mpc更加合理。与Multi-hop的一些工作比较类似——从Entity-GCN这类同构图到后面的异构图模型——本文的主要改进也是立足于此。对比IJCAI19的模型GSN: A graph-structured network for multi-party dialogues.构建的以对话为节点的图，本文的模型构建了一个包含对话者和话语的异构图。对话者也是mpc的重要组成部分。对话者之间、话语与对话者之间存在着复杂的交互。因此，单纯的以话语为节点的图无法区分两个连接的话语节点之间的“应答”关系或“被应答”关系。</p>
<p>MPC中的问题生成任务是指给定对话历史，回复者以及回复的语句后，生成一个合适的回复<span class="math inline">\(r\)</span>，整个问题可以被表示为： <span class="math display">\[
\bar{r} =\underset{r}{\operatorname{argmax}} \log P(r \mid \mathbb{G}) =\underset{r}{\operatorname{argmax}} \sum_{k=1}^{|r|} \log P\left(r_{k} \mid \mathbb{G} r_{&lt;k}\right) .
\]</span> 其中G代表整个异构图，它包含了对话的历史信息以及待生成回复。回应的发出者和接受者是已知的，但具体内容被mask。回复语句的每个token借助自回归模型生成。<span class="math inline">\(r_k\)</span>和<span class="math inline">\(r_{&lt;k}\)</span> 表示第k个token和第k-1个token。</p>
<h2 id="heterogeneous-graph">Heterogeneous Graph</h2>
<p>模型提出了一种基于异构图的神经网络，称为HeterMPC。首先，设计的异构图包含两种类型的节点，分别表示话语和对话者。不同于以往只对话语进行同构图建模的方法，HeterMPC对话语和对话者同时建模，使对话者之间、话语之间、对话者和对话者之间的复杂互动能够被明确地描述。为了刻画对每个(源、边、目标)三元组的异构注意，在计算注意权值和传递消息时引入了依赖于这两类节点和边的模型参数。具体地说，我们引入了六种关系来建模不同的边连接，包括两个话语之间的“回答”和“被回答”，话语与说话者之间的“说话”和“被说话者”，以及话语与接收者之间的“称呼”和“被称呼”。有了这些节点-边类型相关的结构和参数，与传统的同构图相比，HeterMPC可以更好地利用会话的结构知识进行节点表示和响应生成。最后，HeterMPC用Transformer作为其backbone，它的模型参数可以用预训练模型进行初始化。</p>
<p><img src="/images/mpc/2.png" /></p>
<h1 id="hetermpc-model">HeterMPC Model</h1>
<p>HeterMPC采用一种Encder-Decoder架构，多个layers堆叠用于Graph2Seq学习。图编码器的目的是捕获对话结构，并输出图中所有节点的表示，这些节点提供给解码器以生成回复。</p>
<h2 id="graph-construction">Graph Construction</h2>
<p>异构图用来捕获对话人以及话语间的显式交互结构。图中节点包含两类： interlocutors I和utterances M。对于不同节点间的连接，模型设计了六种类型的初始边连接{reply, replied-by, speak, spoken-by, address, addressed-by}。比如语句节点n是对另一句话m的回复，则有边<span class="math inline">\(e_{n,m}=reply\)</span>，<span class="math inline">\(e_{m,n}=replied-by\)</span>。如果某一句话m是由谈话人i说出的，则有边<span class="math inline">\(e_{i,m}=speak\)</span>，<span class="math inline">\(e_{m,i}=spoken-by\)</span>。如果某一句话n是针对谈话人i的，则有边<span class="math inline">\(e_{n,i}=address\)</span>，<span class="math inline">\(e_{i,n}=addressed-by\)</span>。其他节点之间则没有边连接。</p>
<p>每个话语节点初始化时，节点开头会包含一个[cls]，结尾会包含[sep]。而后每个utterance会输入到transformer中进行编码。说话人并不是由token构成，因此直接根据他们的说话顺序的索引进行embedding，在端到端的学习中进行更新。</p>
<h2 id="node-updating">Node Updating</h2>
<p><img src="/images/mpc/4.png" /></p>
<p>初始化的节点表示输入到构建的图中获取上下文信息进行更新，主要借助图注意力和message passing。模型架构跟Transformer比较类似，对于(s,e,t)三元组根据边类型进行attention计算，而后组合成一个向量输入到一个前向神经网络，并引入一个残差连接。 <span class="math display">\[
\overline{\boldsymbol{h}}_{t}^{l}=\sum_{s \in S(t)} \operatorname{softmax}\left(w^{l}(s, e, t)\right) \overline{\boldsymbol{v}}^{l}(s)
\]</span> <span class="math display">\[
\boldsymbol{h}_{t}^{l+1}=F F N_{\tau(t)}\left(\overline{\boldsymbol{h}}_{t}^{l}\right)+\boldsymbol{h}_{t}^{l}
\]</span></p>
<h2 id="decoder">Decoder</h2>
<p><img src="/images/mpc/5.png" title="Decoder of HeterMPC" /></p>
<h1 id="exp">Exp</h1>
<p><img src="/images/mpc/6.png" /></p>
<p><img src="/images/mpc/3.png" title="Linux dataset" /></p>
<p>最后关于这篇论文絮絮叨叨几句。首先关于MPC这一问题，抛开生成任务不谈，其本身的信息处理过程跟multi-hop有异曲同工之处。而图结构在整个编码器中所起的作用，其实与图神经网络并不相同。与其说encoder中用的是message passing，不如说是用了attention，而在其中图结构相当于是引入了一个先验，一个拓扑结构的先验信息。虽然我们都希望大道至简，但异构图的复杂结构、边连接能比由单一类型节点构成的同构图包含更多的信息。或许我们也可以参考这类模型的设计流程，以图结构信息引入先验，而GNN则用表达能力更强的transformer的机制来替换。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/03/pgm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/05/03/pgm/" itemprop="url">Probabilistic Graphical Model</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-05-03T22:33:37+08:00">
                2022-05-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Preliminary-AI/" itemprop="url" rel="index">
                    <span itemprop="name">Preliminary AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  1.4k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  5
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="intro">Intro</h1>
<p>概率图模型（probabilistic graphical model）提供了一个概率与图结构结合的途径，因其灵活性、强大的表达能力和在大规模数据集中的学习和推理能力而受到广泛关注。从高层面来说，我们的目标是表示一个关于多元变量<span class="math inline">\(X = {X_1,X_2,...,X_n}\)</span>的联合概率分布<span class="math inline">\(P\)</span>，以及边缘分布<span class="math inline">\(P(X_i)\)</span>。但是，即便每个变量是二值变量（比如0/1），其联合概率分布的计算开销也是随着变量增加而指数级的增长。但是这些变量之间或多或少会存在一些联系，此时，我们就可以引入图结构来进行建模，借助图来直观的表示变量间的条件独立性关系，进而简化计算。两种最常见的PGM包括贝叶斯网络和马尔可夫随机场，分别为有向图和无向图。</p>
<p>概率图模型有三类主要问题：表示问题：对于一个概率模型，如何通过图结构来描述变量之间的依赖关系。学习问题：图模型的学习包括图结构的学习和参数的学习。推断问题：在已知部分变量时，计算其他变量的条件概率分布。</p>
<h1 id="representation">Representation</h1>
<h2 id="bayesian-networks">Bayesian Networks</h2>
<p><img src="/images/pgm/1.png" /></p>
<p>贝叶斯网络的核心结构在于有向无环图，图中的节点表示随机变量，而边则表示了变量间的关系。因此，贝叶斯网络中的节点都有一个相应的条件概率分布 <span class="math inline">\(P(X_i|Pa_{X_i})\)</span>，Pa表示父节点。此时，我们就可以得到贝叶斯网络的链式法则。 <span class="math display">\[
P_{\mathcal{B}}\left(X_{1}, \ldots, X_{n}\right)=\prod_{i=1}^{n} P\left(X_{i} \mid \mathbf{P} \mathbf{a}_{X_{i}}\right)
\]</span> 对于贝叶斯网络，我们有条件独立性假设，即 <span class="math display">\[
\left(X_{i} \perp \text { NonDescendants }_{X_{i}} \mid \mathbf{P} \mathbf{a}_{X_{i}}\right)
\]</span></p>
<p><img src="/images/pgm/2.png" /> 对于有向图，我们可以把变量间的概率视作一种流动的过程，考虑一个简单的三节点路径X - Y -Z如果条件概率影响可以通过Z从X流到Y，我们说路径X -Z - Y是激活的。借此可以给出在因果推断中常见的情况:</p>
<ul>
<li>Causal path X →Z →Y</li>
<li>Evidential path X ← Z ← Y</li>
<li>Common cause X ← Z → Y</li>
<li>Common effect X → Z ← Y</li>
</ul>
<h2 id="markov-networks">Markov Networks</h2>
<p>第二种常见的概率图模型称为马尔可夫网络或马尔可夫随机场，这些模型基于无向图。在独立性结构和推理任务方面，无向模型还为有向模型提供了不同的、通常更简单的视角。马尔可夫随机场的无向图允许自环存在，且满足局部马尔可夫性，即一个变量X 在给定它的邻居的情况下独立于所有其他变量。在马尔可夫随机场中，我们不能用参数化的概率或条件概率来表示一个变量，而是引入势能函数 potential function <span class="math display">\[
\begin{array}{c}
P_{\mathcal{H}}\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{Z} P^{\prime}\left(X_{1}, \ldots, X_{n}\right) \\
P_{\mathcal{H}}^{\prime}\left(X_{1}, \ldots, X_{n}\right)=\pi_{i}\left[\boldsymbol{D}_{1}\right] \times \pi_{2}\left[\boldsymbol{D}_{2}\right] \times \cdots \times \pi_{m}\left[\boldsymbol{D}_{m}\right]
\end{array}
\]</span> 联合分布被分解为连通子图/团的势能函数<span class="math inline">\(\pi\)</span>的乘积，并除以一个归一化因子Z。势能函数需要满足非负性，所以一般表示为： <span class="math display">\[
\pi[\boldsymbol{D}]=\exp (-\epsilon[\boldsymbol{D}])
\]</span> 其中<span class="math inline">\(\epsilon[\boldsymbol{D}]=-\ln \pi[\boldsymbol{D}]\)</span></p>
<p>由于无向图模型并不提供一个变量的拓扑顺序，因此无法用链式法则对<span class="math inline">\(P(X)\)</span> 进行逐一分解。无向图模型的联合概率一般以全连通子图为单位进行分解。 无向图中的一个全连通子图，称为团（Clique），即团内的所有节点之间都连边。</p>
<h1 id="inference">Inference</h1>
<p>有向和无向图模型都代表了多元变量上的完整联合概率分布。对于概率图上的推理问题，我们可以理解为使用联合概率分布来回答一些query。</p>
<p>最常见的conditional probability query <span class="math inline">\(P(Y|E=e)\)</span>由两部分组成：evidence E和query Y；另一种是寻找剩余变量最有可能的值 <span class="math inline">\(\operatorname{argmax}_{\boldsymbol{y}} P(\boldsymbol{y} \mid \boldsymbol{e})\)</span>。对于以上的一些查询，最容易想到的解决方法就是得到联合概率分布，而后针对特定变量（条件概率查询）进行求和，但这一过程的计算复杂度是我们不能接受的。在这种情况下，我们选择使用概率图结构特点来进行近似推断，并考虑它产生精确结果的条件-变分推断；或者使用基于采样的近似推断来应对小数据集（比如MCMC，从后验分布生成样本）。</p>
<h1 id="learning">Learning</h1>
<p>概率图模型的学习任务有两种:参数估计和结构学习。在参数估计任务中，我们假设图模型的结构是已知的。在这种情况下，学习任务只是学习CPD的参数或定义马尔可夫网络势能函数的参数。在结构学习任务中，不需要额外的输入(尽管用户可以提供关于结构的先验知识，例如，以约束的形式)。目标是从训练数据单独提取贝叶斯网络或马尔科夫网络结构和参数。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/12/iclr22-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/04/12/iclr22-1/" itemprop="url">GNN is a Counter? Revisiting GNN for Question Answering [ICLR22]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-04-12T20:00:00+08:00">
                2022-04-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  471
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="foreword">Foreword</h1>
<p>针对Knowledge-based QA问题，一般从问题和答案中抽取关键实体，对这些实体构建图，并在图上进行路径推理从而得到正确答案，GNN 在图上的信息传播过程相当于在图上找路径，这些路径可以解释答案得到的推理步骤。但是复杂的图结构是否是推理所必须的？作者根据模型剪枝的结果发现现有模型的结构太过复杂，实际上一维的图节点就可以完成KGQA任务。 <img src="/images/ICLR22/3.png" /></p>
<h1 id="分析">分析</h1>
<p>传统的模型使用预训练模型作为encoder，得到问题中的实体、答案的表示；而后使用知识图谱中显式的知识，从知识图谱中抽取问题相关的子图。接下来将节点表示、边的表示作为输入，过几层GNN后输出最终结果。 作者在这一baseline上使用Sparse Variational Dropout (SparseVD) 进行模型剪枝，具体来说就是寻找GNN各层中对结果没有影响的权重。作者在多个不同的模型上进行实验，结果显示，除了边的embedding之外，节点embedding、节点初始化对最终的结果影响都不大。 <img src="/images/ICLR22/4.png" /> <img src="/images/ICLR22/1.png" /></p>
<h1 id="gsc">GSC</h1>
<p>对于上面模型pruning的结果，作者设计了一套简化的GNN，将节点和边都变为1维，直接进行加减作为message passing；而实验结果表明这一简化版的GNN取得了非常好的效果。这个简化版的GNN-GSC主要是两个部分，包括边的encoder和Message passing。前面的实验证明了边的重要性，因此使用2层mlp输出最终为1维的Edge- embedding，作为边的重要性，而1维的message passing只是数值加减，所以可以看作一个计数器。设计的具体模型可以参考下图 <img src="/images/ICLR22/2.png" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/11/iclr22/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/04/11/iclr22/" itemprop="url">Revisiting Over-smoothing in BERT from the Perspective of Graph [ICLR22]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-04-11T20:00:00+08:00">
                2022-04-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  757
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  3
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="foreword">Foreword</h1>
<p>作者提到，基于Transformer的模型NLP以及CV中应用时也会出现over-smoothing问题。由于该问题最初出现于GNN中，也是GNN研究中非常重要的方向，因此作者从Graph machine learning中借鉴了一套处理思路，搬运到Transformer中进行魔改，发现效果不错。另外，作者对self-attention与GNN的内在联系也进行了分析。</p>
<h1 id="intro">Intro</h1>
<p>问题纷纷涌现出来：（1）Token uniformity: self-attention会使得token representations identical。（2）Over-smoothing problem for ViT: 不同的patch被映射到一个相似的潜在表示。（3）“overthinking” phenomenon: 在某些问题下，shallow representations比deep representations表现更好。总的来说，这些问题都可以被解释为过平滑问题，也就是token的representation会随着网络的加深趋向一致，丧失自身的特点。</p>
<p>over-smoothing最初在GNN中被提出。而另一方面self-attention matrix可以被看成加权图的normalized adjacency matrix，其中图节点就是句子的token。</p>
<h1 id="transformer">Transformer</h1>
<p>self-attention模块整个公式（包含残差）可以被写成如下： <span class="math display">\[
\operatorname{Attn}(\boldsymbol{X})=\boldsymbol{X}+\sum_{k=1}^{h} \sigma\left(\boldsymbol{X} \boldsymbol{W}_{k}^{Q}\left(\boldsymbol{X} \boldsymbol{W}_{k}^{K}\right)^{\top}\right) \boldsymbol{X} \boldsymbol{W}_{k}^{V} \boldsymbol{W}_{k}^{O \top}=\boldsymbol{X}+\sum_{k=1}^{h} \hat{\boldsymbol{A}}_{k} \boldsymbol{X} \boldsymbol{W}_{k}^{V O}
\]</span> 其中最关键的注意力系数矩阵可以被看成图的邻接矩阵： <span class="math display">\[
\hat{\boldsymbol{A}}=\sigma\left(\boldsymbol{X} \boldsymbol{W}^{Q}\left(\boldsymbol{X} \boldsymbol{W}^{K}\right)^{\top}\right)=\sigma\left(\boldsymbol{Q} \boldsymbol{K}^{\top}\right)
\]</span> 对应的transformer中feed-forward layer可以被写为图卷积（GCN）的形式： <span class="math display">\[
FF(\boldsymbol{X})=\operatorname{Attn}(\boldsymbol{X})+\operatorname{ReLU}\left(\operatorname{Attn}(\boldsymbol{X}) \boldsymbol{W}_{1}+\boldsymbol{b}_{1}\right) \boldsymbol{W}_{2}+\boldsymbol{b}_{2}
\]</span></p>
<p>ResGCN： <span class="math display">\[
\operatorname{ResGCN}(\boldsymbol{X})=\boldsymbol{X}+\operatorname{Re} L U\left(\boldsymbol{D}^{-1 / 2} \boldsymbol{A} \boldsymbol{D}^{-1 / 2} \boldsymbol{X} \boldsymbol{W}\right)=\boldsymbol{X}+\operatorname{Re} L U(\hat{\boldsymbol{A}} \boldsymbol{X} \boldsymbol{W})
\]</span></p>
<h1 id="over-smoothing">over-smoothing</h1>
<p>既然self-attention被表示为了针对句子构建的全连接图上的图神经网络，我们可以从图角度分析over-smoothing并借鉴其解决方案。在GNN上，节点通过message passing获取邻接点信息并聚合，而后更新自身特征。由于不断聚合，而且使用同一个邻接矩阵，导致节点相似度越来越高。作者用cosine similarity来评估over-smoothing程度： <span class="math display">\[
\operatorname{CosSim}=\frac{1}{n(n-1)} \sum_{i \neq j} \frac{\boldsymbol{h}_{i}^{\top} \boldsymbol{h}_{j}}{\left\|\boldsymbol{h}_{i}\right\|_{2}\left\|\boldsymbol{h}_{j}\right\|_{2}}
\]</span> 而后作者对BERT中的过平滑问题进行了理论分析，发现Bert中的过平滑主要是在post-normalization这一部分引入。</p>
<h1 id="hierarchical-fusion-strategy">hierarchical fusion strategy</h1>
<p>针对过平滑的分析，作者提出了一种方法来缓解bert中的over-smoothing（保留post-normalization）具体而言，就是针对第L层的representation，将其与前面K的结果按照一定的规则进行结合，包括：Concat Fusion/Max Fusion/Gate Fusion。</p>
<p><img src="/images/multihopQA/13.png" title="实验结果" /></p>
<p>其实这个方法的idea和早在18年关于图神经网络的模型Jumping Knowledge Networks (JKNet)非常相似</p>
<p><img src="/images/multihopQA/14.png" title="JKNet" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/10/multiqa2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/04/10/multiqa2/" itemprop="url">Multi-hop QA [2]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-04-10T20:00:00+08:00">
                2022-04-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  2k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  7
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="foreword">Foreword</h1>
<p>关于MUlti-QA的简介这里不多介绍，主流的Multi-QA模型大致可以根据是否包含显式/隐式的图结构分类两类，第一类借助Graph来将文本中离散的信息捕获并构建图结构（或隐式的构图），将先验信息融入图Message-passing中；第二类方法不借助图结构，而是通过Transformer直接处理长文本，或将问题分解后进行sub-question的问答。</p>
<h1 id="dynamically-fused-graph-network-for-multi-hop-reasoning">Dynamically Fused Graph Network for Multi-hop Reasoning</h1>
<p>在上一篇中（Multi-hop QA [1]）中介绍了一篇rethinking的文章，关于图结构在多跳推理中是否是必要的，那篇文章的起源大概可以追溯到这里。具体来说，包括Entity-GCN等一系列文章都没有开源，或者开源出部分代码导致无法完全复现，但是这篇DFGN的代码开源了并且结果是完全可复现的。在广大码农复现过程中，有人发现其中的Graph Fusion模块直接使用Transformer似乎效果会更好，于是后面Is Graph Structure Necessary for Multi-hop Question Answering? 这一文章参照了DFGN的结构设计了一个baseline进行测试，发现其中的GNN确实不如Transformer表现好。（开源确实能促进这一行业的技术发展）</p>
<blockquote>
<p>Inspired by human’s step-by-step reasoning behavior, DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents.</p>
</blockquote>
<p><img src="/images/multihopQA/10.png" /></p>
<p>上面两个图非常直观的展现整个模型的框架，借助BERT和Graph Attention及其交互机制来获取token的表示。模型通过Doc2Graph和Graph2Doc来进行序列与图的转换。首先，候选Context和问题query一起输入BERT得到Context和query的每个token的向量表示，随后，接了一个双向注意力得到二者相互attended的表示。对于token向量，模型进行pooling后得到entity向量，而后通过Query计算实体图中每个实体的重要性并以此构建实体图。在实体图上进行Graph- attention计算图注意力。在每一层Fusion Block结束后，还会使用新的实体表示通过Bi-Attention来更新Query的表示。最后从entity反变换为context的过程Graph2Doc则是将该层最初输入的Context的表示中的每个token与其对应的实体的表示拼接然后送入LSTM。</p>
<h1 id="multi-hop-reading-comprehension-across-multiple-documents-by-reasoning-over-heterogeneous-graphs">Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs</h1>
<p>基于异构的文档-实体图。图中节点包含文档节点、实体节点、候选节点。具体节点的初始化基于co-attention 和 self-attention (这两者在Single-doc QA中非常有效)。下面介绍一下模型具体的框架，包含Encoding和Graph reasoning两个部分。</p>
<p>首先对于给出的query，<span class="math inline">\(&lt;s,r,?&gt;\)</span>代表查询的主体，关系以及未知客体，<span class="math inline">\(S_q\)</span>代表文档集，<span class="math inline">\(C_q\)</span>代表候选文章。对于这三者，我们使用GloVe获取其各自的embedding <span class="math inline">\(X\)</span>并输入到Encoder中，具体为Bi-RNN+GRU来编码上下文信息，输出为<span class="math inline">\(H\)</span>。而后进行文档集S中的实体抽取，作为异构图中的实体节点。实体节点的embedding从相应的文档embedding中获取。接下来进行三类节点的co-attention计算，学习query和doc相互作用的embedding，具体的计算公式在这里不做呈现，总体思路跟self-attention非常类似，只不过把作用对象从token与token变为query与<span class="math inline">\(S_q/C_q\)</span>。共同注意力用来产生文档的query-aware embedding，而self-attention pooling被设计为通过选择重要的查询感知信息将顺序上下文表示转换为非顺序特征向量。以上是Encoding部分，总体流程可以概括为初始embedding+co-atten+self-atten三个流程；而后则是进行图上的推理。</p>
<p>构图时，节点可以分成三类，因此在这一图上，针对不同节点之间的边定义非常复杂。模型总体包含有7类边连接，其中大部分的边定义与前人的模型是相似的。作者在后续实验时验证了不同类型边的有效性。最后图神经网络则是选择Gated-GNN。在wikihop数据集上，对比Entity-GCN，模型的准确率从71.2提升到了74.3。</p>
<p><img src="/images/multihopQA/15.png" /></p>
<h1 id="linkbert-pretraining-language-models-with-document-links">LinkBERT: Pretraining Language Models with Document Links</h1>
<p>上文提到的模型都在整个框架中显式的构建了图结构，并通过图上的message passing来进行消息的传播，但是这篇模型则是借助图结构来辅助Bert进行更好的上下文理解，进而直接对多段文本进行学习。</p>
<p>目前的language model输入的文本仅来源于单一文档的上下文。LinkBert将不同的文档间建立连接，输入的上下文来源于相连的文档。另外，模型的训练将NSP任务 变为对应的document relation prediction (DRP)。对比原始的Bert，基于LinkBERT构建的LM对解决multi-hop QA有天然的优势。 <img src="/images/multihopQA/11.png" /></p>
<p>输入的数据为 $ [CLS]X_A[SEP]X_B[SEP]$ 形式，<span class="math inline">\(X_{A/B}\)</span>为文档片段，对于一个Segment A，其后接的Segment B有三种选择：1.A后的连续片段B；2.随机文档的片段B；3.与文档A有hyperlink的文档的片段B</p>
<p>文档片段之间的链接建立需要考虑一些信息，包括：关联性，可以通过使用超链接或词汇相似性度量来实现；链接的文档是否可以提供当前的LM可能看不到的新的、有用的知识。在这方面，超链接可能比词汇相似性链接更有优势；多样性。在文档图中，一些文档可能具有非常高的程度(例如，许多传入的超链接，如维基百科的美国页面)，而其他文档可能具有较低的程度。如果我们从每个锚段的链接文档中统一采样，可能会在整体训练数据中过于频繁地包含程度较高的文档，从而失去多样性。为了调整使所有文档在训练中以类似的频率出现，我们以与其程度成反比的概率对链接文档进行采样。</p>
<h1 id="ask-to-understand-question-generation-for-multi-hop-question-answering">Ask to Understand: Question Generation for Multi-hop Question Answering</h1>
<p>该模型解决Multi-hop QA的思路更贴近人类的逻辑推理过程，将问题根据线索进行分解，而后解答子问题。但是这于这类模型，子问题的生成质量非常关键，因此模型引入了额外的 QG 任务来训练模型的问题生成部分。具体来说，我 们在经典的基于GN的模块的基础上精心设计并添加了一个端到端的QG模块。对比传统的基于QD的方法只依赖问题带来的信息，我们提出的QG模型可以同时基于对原始上下文和问题的理解来生成流畅的、含有内在逻辑的子问题。 <img src="/images/multihopQA/12.png" /> 对于QA模块，模型采用常规的Encoder+GNN的结构，但是QG辅助的作用体现在Encoder会与QG模块贡献，GNN采用的是与DFGN相同的架构；QG模块的整体架构如上图，主要问题在于针对性的数据集的获取，即如何获取训练的子问题。作者提到HotpotQA数据集中大概可以被分类两类问题：Bridge 和 Comparison 其中第一种需要从first-hop中查找线索而后推理下一个目标，后者则是对query中提到的两个实体性质进行比较。</p>
<p>Kristine Moore Gebbie is a professor at a university founded in what year?这是一个Bridge问题，先找到university再找到成立年份；Do The Importance of Being Icelandic and The Five Obstructions belong to different film genres?这是comparison问题，包含The Importance of Being Icelandic、Five Obstructions、film genre三个实体，这时问题则会被分解为Do The Importance of Being Icelandic belong to which film genres? 与Do The Five Obstructions belong to which film genres?</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/29/nerf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/29/nerf/" itemprop="url">Nerf Compression using DCT [Nerf]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-29T22:24:53+08:00">
                2022-03-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index">
                    <span itemprop="name">Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  0
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/17/betterbert/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/17/betterbert/" itemprop="url">For Better Bert</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-17T00:06:58+08:00">
                2022-03-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  394
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  1
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>TODO</p>
<p>Self-Attention：计算负责度限制了长文本的输入（如文本生成任务）-&gt;Longformer；BigBird</p>
<p>T5：Transfer Text-to-Text Transformer<br />
将所有 NLP 任务都转化成 Text-to-Text（文本到文本）任务。模型架构：<strong>Encoder-Decoder</strong> vs Decoder；自监督训练方法：<strong>BERT-style，破坏一部分</strong> vs GPT（从左到右预测）vs shuffle（将文本打乱然后还原）；本文破坏的方式：mask（把一些token换成mask）vs<strong>替换一部分span</strong>vs drop一部分 ERNIE（百度）<br />
模型主要是针对BERT在中文NLP任务中表现不够好提出的改进，在训练时将短语、实体等先验知识进行mask，强迫模型对其进行建模，学习它们的语义表示。RNIE采用三种masking策略：Basic-Level Masking——跟bert一样对单字进行mask，很难学习到高层次的语义信息；Phrase-Level Masking——输入仍然是单字级别的，mask连续短语；Entity-Level Masking——首先进行实体识别，然后将识别出的实体进行mask。</p>
<p>multi-modal：VideoBert videoBert将视频转化为一系列“visual words”(可视化单词)。视频由一系列图片构成，一幅图片对应一帧，作者将n个连续的帧构成一个片段clip，使用cv领域的模型进行特征提取，最终抽取了特征向量，随后对所有特征向量做hierarchical vector quantization(分层矢量量化)，即聚类，得到20736 各类，每个视频都有属于自己的一个类，这个类就是文本处理时的token（visual token）。</p>
<p>模型压缩：TinyBERT(模型蒸馏)</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/03/07/MultihopQA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/03/07/MultihopQA/" itemprop="url">Multi-hop QA [1]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-03-07T22:49:35+08:00">
                2022-03-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  2k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  7
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.09920v3">Question Answering by Reasoning Across Documents with Graph Convolutional Networks</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.03096v2">Is Graph Structure Necessary for Multi-hop Question Answering?</a><br />
<a target="_blank" rel="noopener" href="http://arxiv.org/abs/2007.14062v2">Big Bird: Transformers for Longer Sequences</a></p>
<h1 id="图结构qa">图结构QA</h1>
<p><img src="/images/multihopQA/5.png" title="Multi-hop QA" /> 多跳问答可以看作一个多步推理以及信息结合的过程，Entity-GCN借助图结构的节点建模文本的实体，将其转化为图上的推理问题，取得了非常好的结果。另外，作者提到虽然multi-hop是一个非常具有实际意义的问题，但在此之前的模型仅仅是将文档连接为长文本，而后借助RNN类的模型进行处理。</p>
<p>对于QA问题，给定一个文档集合和查询，我们要从多个候选回答中选择正确的答案（实体）。其可以表示为<span class="math inline">\(\left\langle q, S_{q}, C_{q}, a^{\star}\right\rangle\)</span>，<span class="math inline">\(q\)</span>表示问题，<span class="math inline">\(S_{q}\)</span> 表示supporting document，<span class="math inline">\(C_{q}C\)</span>表示候选的entity 集合，<span class="math inline">\(a_*\)</span>是<span class="math inline">\(q\)</span>的答案。本文的目的是训练一个神经网络，给定一个查询<span class="math inline">\(q\)</span>，可以输出答案在<span class="math inline">\(C_{q}\)</span>上的一个概率分布。通过最大似然估计模型的参数，输出概率最大的结果作为预测的问题答案。</p>
<p><img src="/images/multihopQA/8.png" /></p>
<p>对于一个query，<span class="math inline">\(q=\langle s, r, ?\rangle\)</span>，我们根据supporting documents构建图，其中的节点或为查询的实体，或为候选实体。上图中同样颜色的节点代表同一实体，节点之间的边根据三种规则构建：在同一文档中共现(实线)，不同的mentions之间实体匹配(虚线)，coreference(红线)。接着借助构建的图进行图卷积（Gated-GCN），得到候选节点的特征，并将其与问题的特征结合，作为候选实体的估计。模型encoding的预处理用了ELMo。 <span class="math display">\[
P\left(c \mid q, C_{q}, S_{q}\right) \propto \exp \left(\max _{i \in \mathcal{M}_{c}} f_{o}\left(\left[\mathbf{q}, \mathbf{h}_{i}^{(L)}\right]\right)\right)
\]</span></p>
<h1 id="深入分析">深入分析</h1>
<p>Entity-GCN的成功以及当时GNN在NLP领域的热度让人们开始竞相探索图在多步推理上的应用。大部分的工作都将分布在不同段落间的实体抽取并建模为图结构。Is Graph Structure Necessary for Multi-hop Question Answering? 这篇文章的作者基于HotpotQA数据集构建了一个强大的基线模型并证明了，通过正确地使用预训练模型，图结构对于多步推理问答是不必要的。他们认为图结构和对应的邻接矩阵都可以被看作是一种任务相关的先验知识，并且图注意力可以被看作是自注意力的一种特例。实验和可视化分析都表明图注意力或整个图结构都可以被自注意力或Transformer替代。</p>
<p>论文提到基于图结构进行多跳问答的模型有很多变种，包括借助有向图的循环层来建模实体间的关系，使用动态实体图来解决抽取式的多步推理问答任务以及引入文档节点和问题节点将实体图拓展为异构图等方法。然而，作者在实验中发现移除图结构并不会影响模型的最终效果。</p>
<p>这里放一下原论文作者写的博客：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/mXLrcg0ZSaKF4w9pqv5_8w">EMNLP 2020 | 多步推理问答是否真的需要图结构？</a></p>
<p>这里简单介绍一下论文思路，具体细节可以参考原论文以及上面的博客。作者借助Bert和GCN构建了一个基线模型，如下图所示 <img src="/images/multihopQA/6.png" title="基线模型" /> 模型使用RoBERTa进行上下文的编码，借助Bert进行命名实体识别构建图结构，实体图的连接规则由以下两条规则确定：1）上下文中不同位置出现的相同实体之间有连接。2）同一个句子中出现的不同实体之间有连接。（这里有待考量，下文细🔒）。在HotpotQA数据集上，这个模型取得了非常好的表现： <img src="/images/multihopQA/7.png" title="实验结果" /> 基于这个模型进行后续实验时，作者发现在预训练模型以fine-tuning的方式使用时，包含和不包含图结构的模型都取得了相似的结果。而当我们固定预训练模型的参数后，EM和F1显著下降了9%和10%。如果此时进一步移除图结构，EM和F1会进一步下降4%左右。换句话说，只有当预训练模型以Feature-based的方式使用时，图结构才会起到比较明显的作用。而当预训练模型以Fine-tuning的方式使用时（这是较为通常的方式），图结构并没有对结果起到贡献，换句话说，图结构可能不是解决多步推理问题所必要的结构。</p>
<p>而后作者提到，图注意力是self-attention的一种特例，用transformer代替两层图神经网络也能取得非常接近的结果。作者指出邻接矩阵和图结构都可以被看作是一种任务相关的先验知识。</p>
<p>不过笔者私以为，既然作者认为图结构、领接矩阵是一种先验知识，那借助图神经网络可以很好的将这些先验融入到深度神经网络中。另一方面，其实图神经网络的表现很大程度上与图的构建有关，而作者在实验时只采用了这种非常naive的构图思路，而后说明这种图结构是没有作用的，感觉这里的论证略显单薄。另一方面，将先验知识结合到feature中，借助图神经网络的message passing机制进行学习和反向传播，从而将先验知识的融入纳入到神经网络这一框架下，也可以说是体现了图结构的意义。</p>
<h1 id="transformer-plus">Transformer plus</h1>
<p>上文提到，图注意力和图结构可以被自注意力或Transformer替代，也就是说基于图结构的模型本质上与Transformer是相似的。</p>
<p><img src="/images/multihopQA/1.png" title="Self-Attention" /> Transformer中最重要的就是Self-attention机制，从上图可以看到，而self-attention的计算包括Q、K、V三个矩阵以及矩阵乘法，此时就会引出一个复杂度问题。 <span class="math display">\[
\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
\]</span> Q、K点乘的内存、速度是序列长度的平方复杂度。对于输入为长文本时，我们一般做法是切成512的块，这种做法损失了块与块之间的信息，比如多跳QA问题或者长文本文本摘要，块与块之间的信息起了重要的作用。</p>
<p>针对这一问题，解决的方法主要可以分为两类，第一类接受长度的限制，寻求绕过这一问题的方法：比如对文本使用滑窗，或者从上下文中选择一个subset输入到transformer中，而后迭代不同的上下文。基于这一思路的模型有： SpanBERT, ORQA, REALM, RAG等。另一类方法认为full attention是没有必要的，这类 Sparse Attention Mechanism就包括Longformer、Big Bird。 <img src="/images/multihopQA/2.png" title="Longerformer" /> <img src="/images/multihopQA/3.png" title="Big Bird" /></p>
<p>从上图就可以看出，这些方法抛弃了原有的全局attention计算，变为几类attention，包括：</p>
<ul>
<li><p>random attention：对于每个Q，都等概率随机关注r个Key。</p></li>
<li><p>window attention：对于每个Q，都关注相邻的左边w/2个Key，右边w/2个key。这是因为直觉上告诉我们，多数NLP问题上下文更加重要。</p></li>
<li><p>global attention：在一些预先选择的输入位置上添加全局attention，使这些位置能够关注所有的信息，比如图上所显示的使传统Bert在做分类时，[CLS] token就使用了global attention。针对QA问题时，我们就可以令问题部分的token为global attention。</p></li>
</ul>
<p>Big Bird就是使用了这三类Attention，实验结果上也证明了这类模型比传统的Bert表现更好。 <img src="/images/multihopQA/4.png" title="Exp" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/17/dataaug/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/02/17/dataaug/" itemprop="url">A Survey of Data Augmentation Approaches for NLP</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-02-17T20:42:55+08:00">
                2022-02-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Words count in article&#58;</span>
                
                <span title="Words count in article">
                  2.7k
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">
                  9
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>link: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.03075v4" class="uri">https://arxiv.org/abs/2105.03075v4</a></p>
<h1 id="数据增强的背景">数据增强的背景</h1>
<p><strong>什么是数据增强？</strong></p>
<blockquote>
<p>Data augmentation (DA) refers to strategies for increasing the diversity of training examples without explicitly collecting new data.</p>
</blockquote>
<p>作者提到，数据增强是一种在不收集新数据的前提下增强训练样本多样性的策略。从这里可以看出，首先数据增强是根据已有的数据来创造新数据的过程。其次，数据增强并不意味着单纯地增多数据，而是为了模型训练进行服务的。也就是说，我们要在任务目标指导下借助已有的样本来创造新样本。</p>
<p><strong>为什么要做数据增强</strong><br />
当我们获得的数据有限，不足以训练出优秀的模型时，就需要获取更多的数据。但是采集新数据往往要消耗大量的人力物力资源，在这种情况下，数据增强技术就提供了一个很好的解决措施，尤其是现在NLP领域基本是大规模预训练模型大行其道，对大数据更加渴求。而令一方面，数据增强能够按照我们的意愿对数据做一些约束。换句话说，我们可以通过数据增强的方法引入一些先验知识或条件。</p>
<p><strong>数据增强的合理性</strong><br />
从我们直观的角度思考，扩充数据的分布与原始数据的分布既不应该太相似，也不应该太不同。我们设计的数据增强方法得到的新数据是尽可能服从实际情况下的分布。此时，在样本空间上更多的采样点有助于我们进一步探索真实的数据分布。</p>
<blockquote>
<p>data augmentation is typically performed in an ad- hoc manner with little understanding of the under- lying theoretical principles</p>
</blockquote>
<p>作者提到，目前关于DA为什么有效的研究工作主要停留在表层，对其理论基础和原理研究较少。一些现有的工作包括： 带噪声测试样本的训练可简化为Tikhonov正则化；DA可以增加分类器的positive margin，但只有在许多常见DA方法以指数方式做数据增强时才会如此；将DA转换视为kernels，并发现DA的两种帮助方式:特征平均和方差正则化。</p>
<p><strong>Data Augmentation in CV</strong></p>
<p>常见的CV中的数据增强包括：</p>
<ul>
<li><p>旋转、平移、翻折</p></li>
<li><p>缩放：图像可以被放大或缩小。放大时，放大后的图像尺寸会大于原始尺寸。大多数图像处理架构会按照原始尺寸对放大后的图像 进行裁切。</p></li>
<li><p>随机裁剪，我们随机从图像中选择一部分，然后降这部分图像裁剪出来，然后调整为原图像的大小</p></li>
<li><p>添加噪声： 过拟合通常发生在神经网络学习高频特征的时候 (因为低频特征神经网络很容易就可以学到，而高频特征只有在最后的时候才可以学到) 而这些特征对于神经网络所做的任务可能没有帮助，而且会对低频特征产生影响，为了消除高频特征我们随机加入噪声数据来消除这些特征。</p></li>
</ul>
<p><img src="/images/DA/1.png" title="Data Augmentation in CV vs NLP" /></p>
<p>相较于机器学习和计算机视觉领域，数据增强在NLP应用并没有前两者这么广泛。在NLP中，输入空间是离散的，我们需要关注如何生成有效的增广例子来捕获所需的不变性。因此它通常被比喻成“蛋糕上的樱桃”，只是提高有限的性能。</p>
<h1 id="主流技术">主流技术</h1>
<p>理想的DA技术应该既易于实现又能提高模型性能，但我们往往需要在这两者之间进行权衡。基于规则的技术很容易实现，但通常只能带来有限的性能改进。基于训练的模型的DA技术可能代价更大，但会引入更多的数据变化，导致更好的性能提升。为下游任务定制的基于模型的DA技术对性能有很强的影响，但很难开发和利用。</p>
<h2 id="rule-based-techniques">Rule-Based Techniques</h2>
<p>在特征空间内直接进行变化生成新的样本，比如在已知类别的数据之间进行“类比”转换，以扩充新的类；使用迭代的仿射变换和投影来沿着class-manifold最大限度地“拉伸”一个样本。</p>
<p>还有比如我们可以借助单词嵌入，如Word2Vec, GloVe, FastText, Sent2Vec，使用嵌入空间中最近邻的单词替换句子中的某个单词。 <img src="/images/DA/6.png" /></p>
<p>EASY DATA AUGMENTATION (EDA)：token-level的随机扰动操作，比如随机插入、删除、交换以及同义替换等。作者发现经过EDA后文本分类的准确率有了很大的提升。</p>
<p><img src="/images/DA/2.png" title="EDA" /></p>
<p>Unsupervised Data Augmentation (UDA)。一种基于无监督数据的数据增强方式，该方法通过对<span class="math inline">\((x, DA(x))\)</span>进行consistency training，生成无监督数据与原始无监督数据具备分布的一致性 <img src="/images/DA/3.png" title="UDA" /></p>
<p>在数据上构建带标记的图，将单个句子作为节点，配对的标签作为带标记的边。使用平衡理论和及物性从这个图中推断扩充句子对。 <img src="/images/DA/4.png" title="Graph Theory DA" /></p>
<p>Dependency tree morphing DA：受图像裁剪和旋转的启发，Şahin和Steedman提出了依赖树构建方法。对于带有依赖项注释的句子，可以交换或删除具有相同父节点的子节点。 <img src="/images/DA/5.png" title="Dependency tree morphing" /></p>
<h2 id="example-interpolation-techniques">Example Interpolation Techniques</h2>
<p>Mixed Sample Data Augmentation (MSDA) /MixUp。这种插值方法在图像处理领域应用非常广泛</p>
<p><span class="math inline">\(\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}, \quad\)</span> where <span class="math inline">\(x_{i}, x_{j}\)</span> are raw input vectors</p>
<p><span class="math inline">\(\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}, \quad\)</span> where <span class="math inline">\(y_{i}, y_{j}\)</span> are one-hot label encodings</p>
<p><img src="/images/DA/10.png" /></p>
<p>不同于MSDA这种连续插值，CUTMIX用从图像B中采样的一个patch替换图像A中的一个小的子区域，标签按照子区域大小的比例混合。对于NLP来说，例如涉及图像和文本的多模态问题可以借鉴这一工作的思想。</p>
<p>SEQ2MIXUP：（sequence-level variant of MixUp）对于sequence-to-sequence的模型，将输入/输出序列对进行一定的组合。 <span class="math display">\[
\begin{array}{r}
(\hat{X}, \hat{Y})=\left(m_{X} \odot X+\left(1-m_{X}\right) \odot X^{\prime}\right. \\
\left.m_{Y} \odot Y+\left(1-m_{Y}\right) \odot Y^{\prime}\right)
\end{array}
\]</span></p>
<p><span class="math inline">\(m=\left[m_{X}, m_{Y}\right]\)</span>是一个系数向量。这种方法对于基于transformer的机器翻译、语义解析等任务都有明显的提升。</p>
<h2 id="model-based-techniques">Model-Based Techniques</h2>
<p>Seq2seq model以及language model都可以被用来做数据增强。</p>
<p>DiPS 原本是用于Diverse Paraphrasing（复述）任务的模型，该任务的度量标准为语义的相似性以及句子本身的差异性，但是我们也可以借助这些模型来进行数据增强。 <img src="/images/DA/8.png" title="DiPS during decoding to generate k paraphrases" /></p>
<p>类似的基于Transformer、BERT的模型从预训练的嵌入空间中，使用上下文敏感的、基于注意力的语义邻居混合来增强单词表示。</p>
<p><img src="/images/DA/7.png" title="综合对比图。Ext.Know是指DA方法是否需要外部知识(如WordNet)，如果需要预训练模型(如BERT)，则需要预训练。Preprocess表示需要进行预处理，Level表示数据被DA修改的深度，Task-Agnostic表示DA方法是否可以应用于不同的任务。Ext.Know、KWE、tok、const和dep分别代表外部知识、关键字提取、字符化、分组解析和依赖解析。" /></p>
<h1 id="应用">应用</h1>
<h2 id="low-resource-languages-few-shot-learning">Low-Resource Languages &amp;&amp; Few-Shot Learning</h2>
<p>低资源语言是DA的一个重要和具有挑战性的应用，尤其是神经机器翻译(NMT)。使用外部知识的技术很难将高资源语言用于低资源语言，特别是当它们具有相似的语言属性时。<br />
<img src="/images/DA/9.png" title="Low-Resource Languages NMT" /> 如上图所示，我们可以借助一个high-resource language作为中枢来转化目标语言以及low-resource language。当high-resource language与low-resource language属于相同语系或具有类似性质时，我们将（HRL-ENG）数据集借助数据增强转化为（LRL-ENG） 数据集。对于其他任务，我们也可以通过DA来扩充low-resource language。</p>
<h2 id="mitigating-bias-fixing-class-imbalance">Mitigating Bias &amp;&amp; Fixing Class Imbalance</h2>
<p>以性别的偏差为例，可以借助DA来缓解性别偏见：创建一个与原始数据相同但偏向于未被充分代表的性别的增强数据集(使用实体的性别交换，如将“他”替换为“她”)来缓解引用解析中的性别偏见，并对这两个数据集进行联合训练。后续也有更好的工作比如缓解性别偏见的COUNTERFACTUAL DA (CDA)方法来打破性别词和中性词之间关联的因果干预。</p>
<p>类似也可以用于解决某些类别中的采样不足和采样过度问题。比如通过插值增强少数群体类的例子，以平衡多标签分类的分类。</p>
<h1 id="当前挑战和未来研究方向">当前挑战和未来研究方向</h1>
<p>应用到数据增强的NLP任务有很多，首先最基本的就是分类，这也是用来测试数据增强技术效果的基本方式之一。其他的任务包括摘要、问答、序列标注、Data-to-Text自然语言生成等以及多模态的任务如automatic speech recognition。</p>
<p>但是目前为止的研究存在以下方面的缺陷：</p>
<ul>
<li><p>明显缺乏关于DA为什么有效的研究。大多数研究都根据经验表明，以及基于一些实验表明DA技术是有效的，但目前很难在不诉诸于全面实验的情况下衡量技术的优劣。</p></li>
<li><p>多模态领域做数据增强。尽管多模态数据分析的工作有所增加，但许多研究都集中在单个模态或多个模态的扩展上。任需进一步探索诸如图像和文本同时增强的图像字幕方式。</p></li>
<li><p>基于范围的任务。例如，随机token替换可能是局部可接受的DA方法，但可能会破坏后面句子的共引用链。此时DA技术必须考虑文本中不同位置之间的依赖关系。</p></li>
<li><p>在专业领域工作，比如那些具有特定领域词汇和术语的领域(如医学)，许多预先训练的模型和外部知识不能被有效地使用。研究表明，当应用于特定领域数据时，DA变得不那么有效，这可能是因为增广数据的分布可能与原始数据有很大不同。</p></li>
<li><p>另一方面，针对数据增强所应对的少样本问题，我们还能从半监督学习角度考虑，结合数据增强与半监督学习技术是一个不错的选择。半监督学习能够充分利用大量未标注数据，同时能够使输入空间的变化更加平滑。</p></li>
</ul>
<h1 id="补充材料">补充材料</h1>
<p>一些关于NLP中数据增强方法具体操作的介绍： https://amitness.com/2020/05/data-augmentation-for-nlp/<br />
NLP数据增强工具包：<a target="_blank" rel="noopener" href="https://github.com/makcedward/nlpaug">nlpaug</a><br />
类似的综述性文章：<br />
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.11741.pdf">Data Augmentation Approaches in Natural Language Processing: A Survey</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/default/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/default/page/5/">5</a><a class="extend next" rel="next" href="/default/page/2/">&gt;</a>
  </nav>




          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.JPG"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">42</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Bbchen229" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:1109441357@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen jiayuan</span>

  
</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
