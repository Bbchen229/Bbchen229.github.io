<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">



  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Chen's Homepage" type="application/atom+xml" />






<meta property="og:type" content="website">
<meta property="og:title" content="Chen&#39;s Homepage">
<meta property="og:url" content="http://example.com/default/index.html">
<meta property="og:site_name" content="Chen&#39;s Homepage">
<meta property="og:locale">
<meta property="article:author" content="Chen jiayuan">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>Chen's Homepage</title>
  








<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    
    <a target="_blank" rel="noopener" href="https://github.com/Bbchen229" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chen's Homepage</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Hello AI</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/18/svgd-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/18/svgd-2/" itemprop="url">Untitled</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-18T10:04:35+08:00">
                2021-11-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/16/kernel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/16/kernel/" itemprop="url">Kernels and Hilbert Space</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-16T19:43:16+08:00">
                2021-11-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Preliminary-AI/" itemprop="url" rel="index">
                    <span itemprop="name">Preliminary AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Reference：<br />
‘Introduction to Hilbert Spaces with Application.’<br />
‘Introduction to RKHS, and some simple kernel algorithms.’</p>
<p>Since Kernel trick is one of the core methods in SVM and SVGD also involves expertise related to RKHS. I looked up several books on Kernel method, trying to get a systematic understanding of Kernel and Hilbert space. This blog can also be regarded as a summary and summary of the book ‘Introduction to Hilbert Spaces with Application ’.</p>
<h1 id="introduction">Introduction</h1>
<p><img src="/images/kernel/1.png" title="XOR example" /> <img src="/images/kernel/2.png" title="Document classification example" /></p>
<h2 id="kernel">Kernel</h2>
<p>Definition: Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[
k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}
\]</span></p>
<h1 id="normed-vector-spaces">Normed Vector Spaces</h1>
<p>First, the space defined in mathematics can be divided from simple to complex as:</p>
<ul>
<li><p>Vector Space<br />
a nonempty set <span class="math inline">\(E\)</span> with two operations: <em>addition</em> and <em>multiplication by scalars</em>.<br />
e.g. <span class="math inline">\(\mathbb{R}^{N}\)</span> <span class="math inline">\(\mathbb{C}^{N}\)</span></p></li>
<li><p>Normed Space<br />
norm is an abstract generalization of the length of a vector:<br />
function <span class="math inline">\(x \mapsto\|x\|\)</span> from a vector space <span class="math inline">\(E\)</span> into <span class="math inline">\(\mathbb{R}\)</span></p></li>
<li><p>Banach Space: complete normed space<br />
A normed space is complete if and only if every absolutely convergent series converges. (The contents of Cauchy sequence and Cauchy series are put in the appendix)<br />
Actually, Banach space introduces the concept of Limits</p></li>
<li><p>Inner Product Spaces<br />
The space that defines the <a href="#jump">inner product</a>.</p></li>
<li><p>Hilbert Spaces: A complete inner product space</p></li>
</ul>
<h1 id="hilbert-spaces">Hilbert Spaces</h1>
<h1 id="appendix">Appendix</h1>
<h2 id="cauchy-sequence-and-cauchy-series">Cauchy sequence and Cauchy series</h2>
<p>Definition of <strong><em>Cauchy sequence</em></strong>. A sequence <span class="math inline">\(\left\{f_{n}\right\}_{n=1}^{\infty}\)</span> of elements in a normed space <span class="math inline">\(\mathcal{H}\)</span> is said to be a Cauchy sequence if for every <span class="math inline">\(\epsilon&gt;0\)</span>, there exists <span class="math inline">\(N=N(\varepsilon) \in \mathbb{N}\)</span>, such that for all <span class="math inline">\(n, m \geq N,\left\|f_{n}-f_{m}\right\|_{\mathcal{H}}&lt;\epsilon\)</span></p>
<h2 id="inner-product">Inner product</h2>
<p><span id="jump"> </span> Definition of <strong><em>Inner product</em></strong>. Let <span class="math inline">\(\mathcal{H}\)</span> be a vector space over <span class="math inline">\(\mathbb{R}\)</span>. A function <span class="math inline">\(\langle\cdot, \cdot\rangle_{\mathcal{H}}: \mathcal{H} \times \mathcal{H} \rightarrow \mathbb{R}\)</span> is said to be an inner product on <span class="math inline">\(\mathcal{H}\)</span> if:</p>
<ul>
<li><p><span class="math inline">\(\left\langle\alpha_{1} f_{1}+\alpha_{2} f_{2}, g\right\rangle_{\mathcal{H}}=\alpha_{1}\left\langle f_{1}, g\right\rangle_{\mathcal{H}}+\alpha_{2}\left\langle f_{2}, g\right\rangle_{\mathcal{H}}\)</span></p></li>
<li><p><span class="math inline">\(\langle f, g\rangle_{\mathcal{H}}=\langle g, f\rangle_{\mathcal{H}}{ }^{1}\)</span></p></li>
<li><p><span class="math inline">\(\langle f, f\rangle_{\mathcal{H}} \geq 0\)</span> and <span class="math inline">\(\langle f, f\rangle_{\mathcal{H}}=0\)</span> if and only if <span class="math inline">\(f=0\)</span>.</p></li>
</ul>
<p>the inner product between matrices <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(B \in\)</span> <span class="math inline">\(\mathbb{R}^{m \times n}\)</span> is <span class="math display">\[
\langle A, B\rangle=\operatorname{trace}\left(A^{\top} B\right)
\]</span></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/09/svgd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/09/svgd/" itemprop="url">Stein variational gradient descent (NIPS2018)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-09T20:28:58+08:00">
                2021-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="intro">Intro</h1>
<p>这一工作是清华大学liu qiang老师提出的，相关论文从2016年开始也一直在更新，分别发表在NIPS、ICLR等顶会上。<br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.07520">Stein Variational Gradient Descent as Gradient Flow</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.11693">Stein Variational Gradient Descent as Moment Matching</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1608.04471.pdf">Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a></p>
<p>从定义上来说，SVGD是一种确定性的采样算法，用一组粒子来近似给定的分布。基于这两点，它和MCMC以及VI都有共通之处。但SVGD即保证了在大量数据下的计算速度，也比变分推断具有更高的准确性。</p>
<p>从整体上来看，这一工作通过引入Stein discrepancy来度量两个分布之间的距离，再借助RKHS使其容易计算，最后借助gradient descent进行优化。因此下文也就从这三部分一一介绍。</p>
<h1 id="background">Background</h1>
<h2 id="steins-method">Stein's method</h2>
<p>首先我们需要引入几个定义：</p>
<ul>
<li><p><em>Stein score function</em> <span class="math display">\[
\boldsymbol{s}_{p}=\nabla_{x} \log p(x)=\frac{\nabla_{x} p(x)}{p(x)}
\]</span> 这一函数被称为<span class="math inline">\(q(x)\)</span>的Stein score function</p></li>
<li><p><em>Stein class</em><br />
当函数<span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}\)</span>满足下式时则称其在stein class中 <span class="math display">\[
\int_{x \in \mathcal{X}} \nabla_{x}(f(x) p(x)) d x=0
\]</span> 其中<span class="math inline">\(\mathcal{X}\)</span> 是<span class="math inline">\(\mathbb{R}^{d}\)</span>下的子集，而<span class="math inline">\(p(x)\)</span>则是在<span class="math inline">\(\mathcal{X}\)</span> 下连续可微的分布。</p></li>
<li><p><em>Stein's operator</em>：作用在<span class="math inline">\(p\)</span>上的线性操作 <span class="math display">\[
\mathcal{A}_{p} f(x)=\boldsymbol{s}_{p}(x) f(x)+\nabla_{x} f(x)
\]</span> 其中<span class="math inline">\(s_{p}\)</span>和<span class="math inline">\(\mathcal{A}_{p} f\)</span> 都是<span class="math inline">\(d \times 1\)</span> 函数(mapping from <span class="math inline">\(\mathcal{X}\)</span> to <span class="math inline">\(\mathbb{R}^{d}.\)</span>)</p></li>
</ul>
<p>有了以上三个定义后，我们可以尝试得到stein discrepancy。首先作为一个度量手段，必然需要满足一些条件。<br />
当且仅当 <span class="math display">\[
\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]=0   \qquad   (1) 
\]</span> <span class="math inline">\(p(x)\)</span> 和 <span class="math inline">\(q(x)\)</span>是相等的。而当两个分布<span class="math inline">\(p=q\)</span>时又被称为stein identity。<br />
借助 (1) 式，我们可以定义Stein discrepancy来度量两个分布<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>之间的差异： <span class="math display">\[
\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]\right)^{2}
\]</span> 借助之前定义的stein operator，也可以把上式写为 <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\mathcal{A}_{q} f(x)\right]\right)^{2}\]</span></p>
<p><span class="math inline">\(\mathcal{F}\)</span>是一系列连续可微的且满足<span class="math inline">\(\mathbb{S}(p, q)\)</span>不为0(<span class="math inline">\(p \neq q\)</span>时)函数集合。当<span class="math inline">\(p \neq q\)</span>时，<span class="math inline">\(\mathbb{S}(p,q)&gt;0\)</span>，而<span class="math inline">\(max\)</span>则是因为我们希望距离尽可能明显。</p>
<p><span class="math inline">\(\mathbb{S}(p, q)\)</span>并没有被广泛应用在机器学习中，因为其计算和优化的复杂性: <span class="math inline">\(q(x)=f(x) / Z\)</span> 而<span class="math inline">\(Z=\int f(x) d x\)</span>的计算往往设计高维积分。<br />
但是论文提出了将函数<span class="math inline">\(\mathcal{F}\)</span>用核函数代替时，会得到易于计算的Stein discrepancy <span class="math inline">\(\mathbb{S}(p, q)\)</span>。具体而言，我们令<span class="math inline">\(\mathcal{F}\)</span>来源于希尔伯特再生核空间的一个球 (reproducing kernel Hilbert space (RKHS))。</p>
<h2 id="kernelized-stein-discrepancy">Kernelized Stein Discrepancy</h2>
<p>对于映射后的函数，对应的正定核<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>，我们有 <span class="math display">\[
\mathbb{S}(p, q)=\mathbb{E}_{x, x^{\prime} \sim p}\left[u_{q}\left(x, x^{\prime}\right)\right]
\]</span> 其中<span class="math inline">\(x, x^{\prime}\)</span>是<span class="math inline">\(p\)</span>中独立同分布的两个变量，函数<span class="math inline">\(u_{q}\left(x, x^{\prime}\right)\)</span>由<span class="math inline">\(q\)</span>确定，如果展开的话实际上是： <span class="math display">\[u_{q}\left(x, x^{\prime}\right)= \boldsymbol{s}_{q}(x)^{\top} k\left(x, x^{\prime}\right) \boldsymbol{s}_{q}\left(x^{\prime}\right)+\boldsymbol{s}_{q}(x)^{\top} \nabla_{x^{\prime}} k\left(x, x^{\prime}\right)+\nabla_{x} k\left(x, x^{\prime}\right)^{\top} \boldsymbol{s}_{q}\left(x^{\prime}\right)+\operatorname{trace}\left(\nabla_{x, x^{\prime}} k\left(x, x^{\prime}\right)\right)\]</span></p>
<p>当我们从未知分布<span class="math inline">\(p(x)\)</span>采样出一个样本<span class="math inline">\({x_i}\)</span>时，我们可以进行近似计算 <span class="math display">\[\hat{\mathbb{S}}(p, q)=\frac{1}{n(n-1)} \sum_{i \neq j} u_{q}\left(x_{i}, x_{j}\right)\]</span></p>
<p>接下来我们详细介绍上述的过程。</p>
<h3 id="kernels-and-reproducing-kernel-hilbert-spaces">Kernels and Reproducing Kernel Hilbert Spaces</h3>
<a href="/2021/11/16/kernel/" title="Kernel and Hilbert Spaces介绍 (未完待续)">Kernel and Hilbert Spaces介绍 (未完待续)</a>
<p>令<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>为一个正定核，根据Mercer’s theorem我们对其进行谱分解： <span class="math display">\[
k\left(x, x^{\prime}\right)=\sum_{j} \lambda_{j} e_{j}(x) e_{j}\left(x^{\prime}\right)
\]</span> 其中<span class="math inline">\(\left\{e_{j}\right\},\left\{\lambda_{j}\right\}\)</span>分别是正交特征函数和正特征值，满足<span class="math inline">\(\int e_{i}(x) e_{j}(x) d x=\mathbb{I}[i=j]\)</span>, for <span class="math inline">\(\forall i, j\)</span></p>
<p>对于一个正定核，它可以分解为RKHS中特征函数的线性组合（空间中任何一个函数可以用这组基的线性组合来表示）。由一个特定的核函数能产生一个唯一的Hilbert空间，有性质 <span class="math display">\[
f(x)=\langle f, k(\cdot, x)\rangle_{\mathcal{H}}, \quad k\left(x, x^{\prime}\right)=\left\langle k(\cdot, x), k\left(\cdot, x^{\prime}\right)\right\rangle_{\mathcal{H}}
\]</span> 当我们定义 <span class="math inline">\(\mathcal{H}^{d}=\mathcal{H} \times \mathcal{H} \times \cdots \mathcal{H}\)</span> 为 <span class="math inline">\(d\)</span> 维向量函数 <span class="math inline">\(\mathbf{f}=\left\{f_{i}: f_{i} \in \mathcal{H} \quad i=1, \cdots, d\right\}\)</span> 组成的 Hilbert空间, <span class="math inline">\(\mathcal{H}^{d}\)</span> 上的内积定义为 <span class="math inline">\(&lt;\mathbf{f}, \mathbf{g}&gt;_{\mathcal{H}^{d}}=\sum_{i=1}^{d}&lt;f_{i}, g_{i}&gt;_{\mathcal{H}}\)</span> 。如果觉得上述的介绍太过抽象，可以看附录部分关于RKHS的一个<a href="#jump">toy example</a></p>
<h3 id="lemmas">lemmas</h3>
<p><strong>Stein's Identity</strong> ： <span class="math display">\[
\mathbb{E}_{p}\left[\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)\right]=0
\]</span> 证明的话根据<span class="math inline">\(\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)=\nabla_{x}(\boldsymbol{f}(x) p(x)) / p(x)\)</span>和分布积分法则就可以推导出来。</p>
<p>有了上面的引理，我们可以得到 <span class="math display">\[\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)-\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\left(\boldsymbol{s}_{q}(x)-\boldsymbol{s}_{p}(x)\right) \boldsymbol{f}(x)^{\top}\right]\]</span></p>
<p>也就是说<span class="math inline">\(\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]\)</span>是一个由<span class="math inline">\(f(x)\)</span>加权的期望，对于<span class="math inline">\(\left(s_{q}(x)-s_{p}(x)\right)\)</span>的期望。</p>
<h1 id="stein-variational-gradient-descent-svgd">Stein Variational Gradient Descent (SVGD)</h1>
<p>SVGD的另一个核心公式在于 <span class="math display">\[
\left.\nabla_{\epsilon} \mathrm{KL}\left(q_{[T]} \| p\right)\right|_{\epsilon=0}=-\mathbb{E}_{x \sim q}\left[\operatorname{tr}\left(\mathcal{A}_{p} \phi(x)\right)\right]
\]</span> 也就是说KL散度变分求导等于KSD（具体推导过程见附录），这意味着KL散度变化最快的方向就是KSD所对应的向量函数 <span class="math inline">\(\phi^{*}=\boldsymbol{\beta} /\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}\)</span> <img src="/images/vimcmc/2.png" title="算法伪代码" /> 具体而言粒子<span class="math inline">\(\left\{x_{i}^{l}\right\}_{i=1}^{n}\)</span> 表示第<span class="math inline">\(l\)</span>次达代的第<span class="math inline">\(i\)</span>个粒子, 一共<span class="math inline">\(n\)</span> 个。粒子最开始是从分布<span class="math inline">\(q_{0}\)</span>中采样的, 最初的分布<span class="math inline">\(q\)</span>可以是任意 的。也就是说，该算法不依赖于初始的分布。</p>
<p>算法中的更新项包含了两个部分 <span class="math display">\[
k\left(x_{j}^{\ell}, x\right) \nabla_{x_{j}^{\ell}} \log p\left(x_{j}^{\ell}\right)+\nabla_{x_{j}^{\ell}} k\left(x_{j}^{\ell}, x\right)
\]</span> 其中第一项意味着粒子会朝<span class="math inline">\(p\)</span>分布概率高的地方移动，而第二项代表着粒子将会朝着远离当前迭代轮数$l ll的粒子，从而减轻局部最优的风险。</p>
<p><img src="/images/vimcmc/2.gif" title="SVGD拟合一维分布" /></p>
<h1 id="回顾与总结">回顾与总结</h1>
<p>从上文繁杂的推导中，SVGD算法确保粒子的移动是朝着KL散度的减小最快方向，而这个方向可以有核化的stein discrepancy导出 我们回看一下KL divergence的定义式： <span class="math display">\[\mathrm{KL}(P \| Q)=\int P(x) \log \frac{P(x)}{Q(x)} d x\]</span></p>
<p>对于目标分布<span class="math inline">\(p(x)\)</span>，变分推断 (VI)目标是从一类分布族<span class="math inline">\(\mathcal{Q}\)</span>中找到最优的<span class="math inline">\(q(x)\)</span> <span class="math display">\[
q^{*}=\underset{q \in \mathcal{Q}}{\arg \min }\left\{K L(q \| p)=\mathbb{E}_{q}[\log q(x)]-\mathbb{E}_{q}[\log \bar{p}(x)]+\log Z\right\}
\]</span> 而SVGD在再生核希尔伯特空间下给出了使得KL散度下降最快的确定性方向，类似经典的梯度下降算法，可以理解为迭代构建增量变化的方法。</p>
<h1 id="appendix">Appendix</h1>
<p><span id="jump"> </span></p>
<h2 id="the-reproducing-kernel-hilbert-space">The reproducing kernel Hilbert space</h2>
<p>先回顾一下kernel的定义： Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[
k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}
\]</span></p>
<p>在此基础上，我们用一个异或问题的例子来介绍RKHS。考虑特征映射</p>
<p><span class="math display">\[\phi: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}\]</span> <span class="math display">\[x=\left[\begin{array}{l}
x_{1} \\
x_{2}
\end{array}\right] \quad \mapsto \quad \phi(x)=\left[\begin{array}{c}
x_{1} \\
x_{2} \\
x_{1} x_{2}
\end{array}\right]\]</span> <img src="/images/vimcmc/4.png" title="特征空间和特征映射。希尔伯特空间的元素一般是函数，而函数可以被视为无穷维的向量。因此事实上希尔伯特空间的基底是一组无限维的函数，可以参考傅立叶变化或泰勒展开" /></p>
<p>kernel <span class="math display">\[
k(x, y)=\left[\begin{array}{c}
x_{1} \\
x_{2} \\
x_{1} x_{2}
\end{array}\right]^{\top}\left[\begin{array}{c}
y_{1} \\
y_{2} \\
y_{1} y_{2}
\end{array}\right]
\]</span></p>
<p>接下来我们可以定义一个特征函数： <span class="math display">\[
f(x)=a x_{1}+b x_{2}+c x_{1} x_{2}
\]</span> 这个函数属于从<span class="math inline">\(\mathcal{X}=\mathbb{R}^{2}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>的函数空间。此时，我们也可以把函数<span class="math inline">\(f\)</span>等价表示为： <span class="math display">\[
f(\cdot)=\left[\begin{array}{l}
a \\
b \\
c
\end{array}\right]
\]</span> 至此，我们可以把<span class="math inline">\(f(x)\)</span>写为： <span class="math display">\[
\begin{aligned}
f(x) &amp;=f(\cdot)^{\top} \phi(x) \\
&amp;:=\langle f(\cdot), \phi(x)\rangle_{\mathcal{H}}
\end{aligned}
\]</span> 也就是说，特征函数<span class="math inline">\(f\)</span>在<span class="math inline">\(x\)</span>的值可以被写为特征空间中的内积。<span class="math inline">\(\mathcal{H}\)</span>是一个将<span class="math inline">\(\mathbb{R}^{2}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>的函数空间。上面这些乱七八糟的怎么体现再生性呢？我们仔细看下面的等式 <span class="math display">\[
k(\cdot, y)=\left[\begin{array}{c}
y_{1} \\
y_{2} \\
y_{1} y_{2}
\end{array}\right]=\phi(y)
\]</span> 上式我们参考<span class="math inline">\(f(\cdot)\)</span>类似的定义。具体来说，如果我们令<span class="math inline">\(a=y_{1}, b=y_{2}\)</span>, and <span class="math inline">\(c=y_{1} y_{2}\)</span>，就有 <span class="math display">\[
\langle k(\cdot, y), \phi(x)\rangle_{\mathcal{H}}=a x_{1}+b x_{2}+c x_{1} x_{2}
\]</span></p>
<p>总的来说RKHS两个特性：<br />
每个点的特征映射在特征空间中 <span class="math display">\[
\forall x \in \mathcal{X}, \quad k(\cdot, x) \in \mathcal{H}
\]</span> 再生性：<br />
<span class="math display">\[
\forall x \in \mathcal{X}, \forall f \in \mathcal{H},\langle f, k(\cdot, x)\rangle_{\mathcal{H}}=f(x)
\]</span> <span class="math display">\[
k(x, y)=\langle k(\cdot, x), k(\cdot, y)\rangle_{\mathcal{H}}
\]</span></p>
<h2 id="kl散度一阶导与ksd">KL散度一阶导与KSD</h2>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/26/pd5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/26/pd5/" itemprop="url">Multi-Label Image Recognition with Graph Convolutional Networks [CVPR2019]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-26T23:25:47+08:00">
                2021-10-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03582">Multi-Label Image Recognition with Graph Convolutional Networks</a><br />
Code: <a target="_blank" rel="noopener" href="https://github.com/chenzhaomin123/ML_GCN">link</a></p>
<p>针对多标签图像识别 (multi-label image recognition) 问题，旷视研究院提出一种基于图卷积网络的模型取得了良好的表现，该模型包含一个CNN的图像特征提取模块和一个图卷积网络进行标签间关系提取模块。</p>
<h2 id="intro">Intro</h2>
<p>对于多标签图像的识别问题，传统的方法往往是对每个标签进行孤立的二分类，即预测每个物体是否出现。基于概率图模型或RNN模型的方法则考虑显式的建模标签之间的依赖关系。也有方法将图像区域划分后考虑区域间的局部相关性，从而隐式的建模标签相关性。本文提出的基于GCN的端到端模型将标签的表示映射到相互独立的对象分类器上。</p>
<h2 id="related-work">Related Work</h2>
<p>最简单的多标签识别方法就是为每个标签独立训练一个二分类器，这种模型没有考虑标签之间的关系。当数据集中可能的标签数量增长时，可能的标签组合就会指数级增长（当一个数据集包含20个标签，则标签组合就有<span class="math inline">\(2^{20}\)</span>种。基于RNN、LSTM之类的模型将标签嵌入为向量，从而发掘标签间的相关性。<br />
本文提出的模型将多标签构建为有向图，借助GCN在标签间的信息传播来学习图像标签间依赖、共现关系，并实现端到端训练。</p>
<h2 id="framework">Framework</h2>
<p><img src="/images/pd5/2.png" title="模型框架" /></p>
<h3 id="图像特征提取">图像特征提取</h3>
<p>论文用CNN进行图像特征提取，具体为ResNet-101的网络结构，输入图像<span class="math inline">\(I\)</span>，经过cnn和global max-pooling后得到2048维图像特征。 <span class="math display">\[
\boldsymbol{x}=f_{\mathrm{GMP}}\left(f_{\mathrm{cnn}}\left(\boldsymbol{I} ; \theta_{\mathrm{cnn}}\right)\right) \in \mathbb{R}^{D}
\]</span></p>
<h3 id="图卷积">图卷积</h3>
<p>卷积模块与最基本的卷积相同，如下式 <span class="math display">\[
\boldsymbol{H}^{l+1}=h\left(\widehat{\boldsymbol{A}} \boldsymbol{H}^{l} \boldsymbol{W}^{l}\right)
\]</span> 我们主要关注如何构图，在这一方面，本文的idea似乎有些超脱CV领域。模型针对图片数据集构建图，图中的节点为数据中的标签，并使用word embedding（pre-trained glove）对节点特征进行初始化。<br />
而对于图的边，也对应图卷积中的矩阵<span class="math inline">\(\boldsymbol{A}\)</span>（文中称其为相关系数矩阵），模型使用条件概率<span class="math inline">\(P\left(L_{j} \mid L_{i}\right)\)</span>进行建模，已期获得标签相关性信息。 <img src="/images/pd5/3.png" /> 具体而言，论文统计了数据集中的标签对的共现次数，然后构建共现矩阵，并设定一个阈值来进行二值化处理，借此过滤噪声边。 <img src="/images/pd5/1.png" title="基于多标签构建有向图" /></p>
<p>借助模型框架图可以看到，模型中图卷积模块起的是类似辅助分类器的作用，图中每个标签节点就是该标签的一个二分类器，将基于整个数据集训练的分类器<span class="math inline">\(\boldsymbol{W} \in \mathbb{R}^{C \times D}\)</span>与图像的特征<span class="math inline">\(x \in \mathbb{R}^{D}\)</span>进行点积，得到<span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^{C}\)</span>（C表示标签的总数）。图卷积利用的信息也只有图的边，也就是标签的共现，而后借助图卷积与图像特征提取进行共同训练，得到标签之间关系的隐式表示，最终推动更准确的多标签识别。</p>
<h2 id="实验">实验</h2>
<p><img src="/images/pd5/4.png" title="实验结果—非常不错 XD" /> 不过在尝试复现该模型时，本人试验了几个数据集似乎始终无法到达论文中的结果。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/24/pd6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/24/pd6/" itemprop="url">Multi-hop Question Generation with Graph Convolutional Network [Arxiv]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-24T23:56:42+08:00">
                2021-10-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.09240.pdf">Multi-hop Question Generation with Graph Convolutional Network</a><br />
Code: <a target="_blank" rel="noopener" href="https://github.com/HLTCHKUST/MulQG">link</a></p>
<h2 id="background">Background</h2>
<p>问题生成(QG)是一个从给定的上下文自动生成问题或答案的任务，而多跳问题生成 (Multi-hop Question Generation) 需要从多个不同的段落中推理生成与答案相关的问题。QG可以应用于教育系统，也可以结合QA模型作为双重任务来增强QA系统的推理能力。对于多跳问题生成，核心问题在于如何连接多个段落间的零散的信息以及答案。</p>
<p><img src="/images/pd6/1.png" title="多跳问题生成" /></p>
<h2 id="模型">模型</h2>
<p><img src="/images/pd6/2.png" title="模型框架" /></p>
<h3 id="multi-hop-encoder">Multi-hop Encoder</h3>
<p>对于输入的文本段落和答案，先分割成word-level的token，并分别用pre-trained Glove进行embedding，并在文本的token embedding中加入答案 embedding tag。对于得到的token embedding 输入到LSTM-RNN中学习初步的上下文相关的representation，再输入到Encoder中，模型的Encoder包括三个部分:</p>
<ul>
<li><p>Answer-aware context encoder 这一部分参考了阅读理解中的co-attention reasoning机制: <span class="math display">\[
\begin{aligned}
S &amp;=C_{0}^{T} A_{0} \in R^{n \times m} \\
S^{\prime} &amp;=\operatorname{softmax}(S) \in R^{n \times m} \\
S^{\prime \prime} &amp;=\operatorname{softmax}\left(S^{T}\right) \in R^{m \times n} \\
A_{0}^{\prime} &amp;=C_{0} \cdot S^{\prime} \in R^{d \times m} \\
\tilde{C}_{1} &amp;=\left[A_{0} ; A_{0}^{\prime}\right] \cdot S^{\prime \prime} \in R^{2 d \times n}\\
C_{1} &amp;=\operatorname{BiLSTM}\left(\left[\tilde{C}_{1} ; C_{0}\right]\right) \in R^{d \times n}
\end{aligned}
\]</span> 相关性矩阵S表示答案与上下文的相关性，整个过程比较复杂，这一模块的有效性在阅读理解任务中被验证，大致操作即将答案与文本计算attention后生成新的“答案”而后同样进行一遍相关性计算，最后输入Bi-LSTM中。</p></li>
<li><p>GCN-based entity-aware answer encoder 将上述encoder得到的embedding输入到GCN中进行多跳信息的嵌入。 <img src="/images/pd6/3.png" title="GCN-based entity-aware answer encoder" /> 图中的节点为文本中的命名实体（由BERT自动化提取），如果实体对在同一句子中，则为它们创建边。将上面Answer-aware context encoder 的结果结合到多跳图卷积中，并最终和图的结果结合，输入到Bi-attention模型，进一步得到token的representations <span class="math display">\[
A_{1}=\text { BiAttention }\left(A_{0}, E_{M}\right)
\]</span></p></li>
<li><p>Gated encoder reasoning layer 将前面得到的结果输入到门控网络进行特征融合，进行特征保留或遗忘，得到最终的Encoder结果。</p></li>
</ul>
<h3 id="maxout-pointer-decoder">Maxout Pointer Decoder</h3>
<p>模型采用单向LSTM作为解码器，而Maxout pointer这一模块也并不是由作者提出的，而是参考了他人的模型，用这一模块减少生成结果中的重复项。</p>
<h2 id="实验">实验</h2>
<p>实验部分，作者分别做了与现有multi-hop QG模型对比以及消融实验，取得了SOTA结果，并且证明了框架中每个模块的意义。 <img src="/images/pd6/4.png" title="纵向对比" /> <img src="/images/pd6/5.png" title="消融实验" /></p>
<h2 id="总结">总结</h2>
<p>本文提出的框架总体来说比较复杂。往牛了说可以理解为整个框架模拟了人类的问题生成的过程，包括整体文本和答案的阅读，进行大概了解，而后对文本和答案中的实体进行关注，并寻找他们的联系，最后在生成问题时确定核心和次要信息，生成相关的问题。不过事实上整个框架就是对几个现有模型中的部分模块进行组装，类似“搭积木”的过程。而新加入的multi-hop图卷积部分整体方法也不具有很亮点的想法，从消融实验结果中也可以看出这一模块对最终结果的提升也并不是很明显。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/23/maxp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/23/maxp/" itemprop="url">2021 MAXP命题赛 基于DGL的图机器学习任务</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-23T15:33:06+08:00">
                2021-10-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Project/" itemprop="url" rel="index">
                    <span itemprop="name">Project</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>使用<a target="_blank" rel="noopener" href="https://www.dgl.ai/">Deep Graph Library (DGL)</a>进行图节点分类任务，使用的图数据是基于微软学术文献生成的论文关系图，其中的节点是论文，边是论文间的引用关系。整个图包括约150万个节点，2000万条边。节点包含300维的特征，来自论文的标题和摘要等内容。节点属于约50个类别。<br />
比赛地址: <a target="_blank" rel="noopener" href="https://biendata.xyz/competition/maxp_dgl/">MAXP</a></p>
<p><img src="/images/maxp/1.png" title="比赛数据集" /></p>
<h2 id="数据预处理">数据预处理</h2>
<p>根据所给的数据集，我们需要读取节点及其对应的特征，以及边。根据论文id构建对应的节点id，并分配他们的特征和类别。读取边之后发现存在部分论文没有出现在论文数据中，这部分的节点id分配到最后，这类论文没有类别和特征，这些点的特征可以选择用邻节点特征均值进行赋值。节点特征使用Numpy保存为.npy格式，方便后续读取。<br />
对train文件中的数据进行Train/Valid分割，用于模型评估（9:1），将train/valid/test节点id和labels等数据保存为二进制文件方便快速读取 <img src="/images/maxp/3.png" /></p>
<h2 id="图构建">图构建</h2>
<p>根据上面的到的原论文id-引用论文id以及他们对应的节点id，借助DGL包构建论文引用关系图。 <img src="/images/maxp/2.png" title="构图" /> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">g = dgl.graph((u,v))</span><br><span class="line">g.nodes() <span class="comment">#获取节点id</span></span><br><span class="line">g.edges() <span class="comment">#获取边对应的节点（输出的是两个tensor）</span></span><br><span class="line">g.ndata(<span class="string">&#x27;feature&#x27;</span>) <span class="comment">#访问节点属性</span></span><br><span class="line">g.edata <span class="comment">#访问边属性</span></span><br></pre></td></tr></table></figure></p>
<h2 id="model_baseline">Model_baseline</h2>
<p>预处理和构图之后，我们模型输入的数据包括： <img src="/images/maxp/4.png" /></p>
<p>竞赛的baseline包括三个模型：</p>
<ul>
<li>graphsage</li>
<li>graphconvolution</li>
<li>graphattention</li>
</ul>
<p>其中各个网络由DGL.nn模块调库搭建。经过初步调参后发现网络深度为3层时三个模型结果最好，其中graphsage表现最好，在验证集上准确率接近54。 <img src="/images/maxp/5.png" title="可视化效果" /></p>
<h2 id="proposed-model">Proposed model</h2>
<p>数据中的节点特征已经给定，因此只能改变模型的网络结构，我参考了2020 acl的论文：Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks 所用的网络架构，具体采用的是三层图卷积后连接一层图注意力，并在图卷积与attention直接增加了skip-connection。在验证集上的到准确率为55+，目前排行榜上前10，后续将会做进一步调参来得到更好的结果。<br />
另外，根据给定的特征直接使用MLP等前向传播网络虽然无法利用引用信息，但可以用来辅助最终的分类。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/18/gnn-nlp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/18/gnn-nlp/" itemprop="url">A Survey of Graph Neural Networks in Natural Language Processing [Arxiv]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-18T13:58:53+08:00">
                2021-10-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>截止2021年初，关于现有的图神经网络应用在自然语言处理领域的综述<br />
link: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.06090">Graph Neural Networks for Natural Language Processing: A Survey</a></p>
<p>另外，本篇博客还包含另外几篇图相关的综述性文章内容：<br />
Graph Representation Learning<br />
Geometric Deep Learning</p>
<p>对于图的机器学习，非常容易想到的就是节点分类，边预测、以及图层级的分类等。对于传统的NLP问题，我们将输入的文本序列表示为图结构时，就可以借助图深度学习技术进行处理。整篇综述根据这一思路从图构造、图表示学习、基于图的Encoder-Decoder模型三方面进行介绍。</p>
<h2 id="图构造">图构造</h2>
<p>对于AI处理的数据类型，大概可以分类3类：Euclidean Structure、Sequence Structure、Graph Structure。<br />
Eucildean data比如图，Sequence data如文本，这类数据都有一个特点：规则，即排列整齐；而图结构这类非欧几何数据，样本是不规则的，每个样本的邻居节点数量都是不同的，因此图像中的卷积操作就无法在图结构中应用。</p>
<p>针对输入的数据，包括文本、树等，我们根据一定的规则自动化构造构建不同类型的图，如无向图、有向图、多关系图、异构图并使用特点的GNN结构来进行学习。</p>
<h3 id="静态图构建">静态图构建</h3>
<p>利用规则或现有的关系解析工具在文本预处理时构造图结构，常见的静态图构建有：</p>
<ul>
<li><p>Dependency Graph<br />
依赖图可以用于捕捉给定句子中两个主语之间的关系。对于给定的句子，可以借助现有的解析工具包得到dependency parsing tree，而后抽取出依赖关系，构建dependency graph</p></li>
<li><p>Constituency Graph<br />
语言学中constituency relation指符合短语结构语法的关系，比如主语NP和谓语VP的关系 <img src="/images/pd3_1.png" /></p></li>
<li><p>Information Extraction Graph<br />
IE Graph抽取出文本中跨越不同句的结构化的信息。构建IE图首先要提取三元组，而后通过不同三元组间共同参数来确定相同含义的实体进行合并，从而减少节点的数量，消除模糊性。 <img src="/images/pd3_2.png" /></p></li>
<li><p>Knowledge Graph<br />
知识图谱能捕获实体以及关系，被广泛用于推理、关系抽取等任务中。知识图谱可以作为文本到embedding之间的一个精练且可解释的中间表示。KG可以表示为 <span class="math inline">\(\mathcal{G}(\mathcal{V}, \mathcal{E})\)</span>，由三元组<span class="math inline">\(\left(e_{1}, r e l, e_{2}\right)\)</span>。KG在不同的下游任务中起不同的作用，如机器翻译可以用于数据增强，阅读理解中用于构建子图。 <img src="/images/pd3_3.png" /></p></li>
<li><p>Co-occurrence Graph</p></li>
</ul>
<p>共现关系描述了两个词在固定大小的上下文窗口内共现的频率，而后单词和词与词间共现频率构建图。<br />
除了上述几类图构建方法外，针对具体的任务还有很多不同的图构造方法。</p>
<h3 id="动态图构建">动态图构建</h3>
<p>静态图可以将数据的部分先验知识编码到图中，但是这需要大量的人力试验以及领域专业知识，且容易包含噪声。另外，静态图的构建是基于构建者自身的经验，得到的并不一定是对于某一下游任务最优的图。<br />
而动态图则是动态学习图结构（加权邻接矩阵），图构造模块和后续图表示学习一起针对下游任务联合优化。图结构学习也是机器学习领域研究的热点问题 <img src="/images/pd3_4.png" alt="动态图构建方法" /></p>
<p><strong>Graph similarity metric learning</strong><br />
图结构学习可以转化为节点相似度度量问题 (相似度矩阵<span class="math inline">\(S\)</span>)，对于相似度度量函数，可以分为两类：</p>
<ul>
<li>基于节点嵌入的相似度度量学习</li>
<li>基于结构感知的相似度度量学习</li>
</ul>
<p><em>基于节点嵌入的相似度函数</em>通过计算嵌入空间中节点的成对相似度来学习加权邻接矩阵。常见的度量函数包括基于注意力的度量函数和基于余弦的度量函数。 <span class="math display">\[
S_{i, j}=\operatorname{ReLU}\left(\vec{W} \vec{v}_{i}\right)^{T} \operatorname{ReLU}\left(\vec{W} \vec{v}_{j}\right)
\]</span> 上式为基于注意力的度量函数，<span class="math inline">\(\vec{W}\)</span>为可学习的权重。类似的，基于cosine的度量函数为： <span class="math display">\[
\begin{aligned}
S_{i, j}^{p} &amp;=\cos \left(\vec{w}_{p} \odot \vec{v}_{i}, \vec{w}_{p} \odot \vec{v}_{j}\right) \\
S_{i, j} &amp;=\frac{1}{m} \sum_{p=1}^{m} S_{i j}^{p}
\end{aligned}
\]</span></p>
<p><em>基于结构感知的相似性函数</em>在节点信息之外还考虑了边的信息，如 <span class="math display">\[
S_{i, j}^{l}=\operatorname{softmax}\left(\vec{u}^{T} \tanh \left(\vec{W}\left[\vec{h}_{i}^{l}, \vec{h}_{j}^{l}, \vec{v}_{i}, \vec{v}_{j}, \vec{e}_{i, j}\right]\right)\right)
\]</span> 其中<span class="math inline">\(\vec{v}_{i}\)</span> 代表节点i的embedding， <span class="math inline">\(i\vec{e}_{i, j}\)</span> 代表边的embedding $ _{i}^{l}$ 代表节点i在GNN中第i层的embedding， <span class="math inline">\(\vec{u}\)</span>和<span class="math inline">\(\vec{W}\)</span> 是可训练的权重。</p>
<p><strong>Graph sparsification</strong><br />
现实世界中大多数的图都是稀疏图，而通过相似度度量函数会得到任意两个节点之间的边，最终生成一个全连通图，这会极大增大开销，并引入噪声，因此需要进行图稀疏化处理。常用的方法包括取k个相似度最高的邻节点，或者给节点间的相似度设定一个阈值。</p>
<p>另外，静态图和动态图也可以结合起来，既可以加速训练，提高稳定性，也能提高下游任务的表现 <span class="math display">\[
\widetilde{A}=\lambda L^{(0)}+(1-\lambda) \mathrm{f}(A)
\]</span> 上式中<span class="math inline">\(L^{(0)}\)</span>表示静态图结构，<span class="math inline">\(\mathrm{f}(A)\)</span>表示可学习的动态图结构。</p>
<h2 id="图表示学习">图表示学习</h2>
<p>由于图的类型多种多样，如同构图、异构图、多关系图等等，这些不同的图上进行图表示学习的具体模型或有出入，但总体的步骤和思路基本类似。下面介绍的图表示学习方法是基于同构图，且节点与节点之间仅有一条无向边。</p>
<h3 id="basic-gnn">Basic GNN</h3>
<p>图神经网络对图中的节点进行embedding，并根据需求给出最终的node embedding或graph embedding。图神经网络的特征传播总体可以分成两个步骤，包括聚合-Aggregation和更新-Update。 <span class="math display">\[
\mathbf{m}_{\mathcal{N}(u)}=\operatorname{AGGREGATE}^{(k)}\left(\left\{\mathbf{h}_{v}^{(k)}, \forall v \in \mathcal{N}(u)\right\}\right)
\]</span> <span class="math display">\[
\operatorname{UPDATE}\left(\mathbf{h}_{u}, \mathbf{m}_{\mathcal{N}(u)}\right)=\sigma\left(\mathbf{W}_{\text {self }} \mathbf{h}_{u}+\mathbf{W}_{\operatorname{neigh}} \mathbf{m}_{\mathcal{N}(u)}\right)
\]</span></p>
<p>其中<span class="math inline">\(\mathcal{N}(u)\)</span>表示节点<span class="math inline">\(u\)</span>的邻节点，<span class="math inline">\(\mathbf{h}_{u}\)</span>则代表节点特征，<span class="math inline">\(\mathbf{W}\)</span>为可学习的权重矩阵。</p>
<h3 id="aggregation">Aggregation</h3>
<p>聚合操作将节点的邻节点特征进行汇总，常用的方法包括：</p>
<ul>
<li><p>Normalization：最基本的聚合方法就是对邻节点embedding求平均，并针对节点的度进行归一化 <span class="math display">\[
\mathbf{m}_{\mathcal{N}(u)}=\frac{\sum_{v \in \mathcal{N}(u)} \mathbf{h}_{v}}{|\mathcal{N}(u)|}
\]</span></p></li>
<li>Pooling：基于MLP这类的置换不变(permutation invariant)网络进行聚合，通用的pooling aggregator可以表示为： <span class="math display">\[
\mathbf{m}_{\mathcal{N}(u)}=\operatorname{MLP}_{\theta}\left(\sum_{v \in N(u)} \operatorname{MLP}_{\phi}\left(\mathbf{h}_{v}\right)\right)
\]</span> 另一种Janossy pooling则是赋予邻节点一个次序，并使用对于时序敏感的函数进行聚合 <span class="math display">\[
\mathbf{m}_{\mathcal{N}(u)}=\operatorname{MLP}_{\theta}\left(\frac{1}{|\Pi|} \sum_{\pi \in \Pi} \rho_{\phi}\left(\mathbf{h}_{v_{1}}, \mathbf{h}_{v_{2}}, \ldots, \mathbf{h}_{v_{|\mathcal{N}(u)|}}\right)_{\pi_{i}}\right)
\]</span></li>
<li><p>Attention： 对邻节点分配不同的权重，权重可以基于邻节点的embedding，也可以基于边的权值 <span class="math display">\[
\mathbf{m}_{\mathcal{N}(u)}=\sum_{v \in \mathcal{N}(u)} \alpha_{u, v} \mathbf{h}_{v}
\]</span> <span class="math display">\[
\alpha_{u, v}=\frac{\exp \left(\mathbf{a}^{\top}\left[\mathbf{W h}_{u} \oplus \mathbf{W h}_{v}\right]\right)}{\sum_{v^{\prime} \in \mathcal{N}(u)} \exp \left(\mathbf{a}^{\top}\left[\mathbf{W h}_{u} \oplus \mathbf{W h}_{v^{\prime}}\right]\right)}
\]</span></p></li>
</ul>
<h3 id="updates">Updates</h3>
<p>在经过多层图神经网络后，某些节点自身的特性会因为不断聚合邻节点的信息而淡化或被抹去，这就导致深度图神经网络的over-smoothing问题。<br />
为缓解over-smoothing问题的一些技巧，比如Concatenation和Skip-Connections等在节点信息聚合后的更新操作入手。 <span class="math display">\[
\text { UPDATE }_{\text {concat }}\left(\mathbf{h}_{u}, \mathbf{m}_{\mathcal{N}(u)}\right)=\left[\text { UPDATE }_{\text {base }}\left(\mathbf{h}_{u}, \mathbf{m}_{\mathcal{N}(u)}\right) \oplus \mathbf{h}_{u}\right]
\]</span></p>
<h3 id="graph-convolutional-networks-gcn">Graph Convolutional Networks (GCN)</h3>
<p>图卷积就如同CV中的卷积，被提出后受到了广泛关注和研究。欧氏空间中的离散卷积我们很好理解，而对于非欧数据中的卷积，它的提出流程可以概括为：图信号处理GSP学者提出图的Fourier Transformation，进而得到Graph convolution，从而拓展到神经网络的图卷积网络。</p>
<p>图的卷积定义在spectral domain，相应的邻接矩阵<span class="math inline">\(A\)</span>用图的Laplacian 矩阵<span class="math inline">\(L\)</span>替代。<span class="math inline">\(L = D - A\)</span>，<span class="math inline">\(D\)</span>为度矩阵。把传统的傅里叶变换以及卷积迁移到Graph上, 核心工作就是把拉普拉斯算子的特征函数 <span class="math inline">\(e^{-i \omega t}\)</span> 变为Graph对应的拉普拉斯矩阵的特征向量。这其中具体的推导过程在此不再赘述。基本的GCN中第k层可以写为下式： <span class="math display">\[
\mathbf{H}^{(k)}=\sigma\left(\tilde{\mathbf{A}} \mathbf{H}^{(k-1)} \mathbf{W}^{(k)}\right)
\]</span> 其中<span class="math inline">\(\tilde{\mathbf{A}}=(\mathbf{D}+\mathbf{I})^{-\frac{1}{2}}(\mathbf{I}+\mathbf{A})(\mathbf{D}+\mathbf{I})^{-\frac{1}{2}}\)</span>，是拉普拉斯矩阵的一个变形形式。</p>
<h3 id="graphsage">GraphSAGE</h3>
<p>GraphSAGE这一图模型是归纳式 (inductive) 学习。不同于之前的transducer模型，GraphSAGE的目标不是学习到每个节点的embedding，而是学习生成embedding的聚合函数。 <img src="/images/pd3_5.png" /> 整个框架如上图所示，包括采样、聚合、预测三个步骤。在采样时会选择恒定数量的邻节点，且不仅仅选择1-hop的节点，而是考虑multi-hop。</p>
<h2 id="基于图的encoder-decoder模型">基于图的Encoder-Decoder模型</h2>
<p>Encoder-Decoder是深度学习模型中非常常见的架构，因此到了图深度学习领域，图到树、graph-graph等模型也应运而生。</p>
<h3 id="graph-to-sequence-model">Graph-to-Sequence Model</h3>
<p>这类模型通常用GNN作为Encoder，RNN/Transformer作为Decoder。此外，这类模型中多使用CNN进行节点特征初始化，用于捕捉GNN不敏感的连续词的潜在信息。这类模型在多关系图或异构图的处理上有所局限。</p>
<h3 id="graph-to-tree-model">Graph-to-Tree Model</h3>
<p>类似端到端的模型，在NLP任务中，树也具有很强大的表达能力。由于树广义上来讲也是一种图，因此Graph-Tree这类模型的核心在于借助self-attention进行获取局部邻节点的权重，再由decoder 生成包含语义的tree结构。这类模型的应用比如语义解析、数学应用问题（模型输出为由树来表示的方程）。</p>
<h3 id="graph-to-graph-model">Graph-to-Graph Model</h3>
<figure>
<img src="/images/pd3_6.png" alt="图的生成式模型(VAE)" /><figcaption>图的生成式模型(VAE)</figcaption>
</figure>
<h2 id="补充">补充</h2>
<h3 id="图神经网络在nlp中的下游任务">图神经网络在NLP中的下游任务</h3>
<p>已有的基于图相关技术的NLP任务包括自然语言生成、机器翻译、情感分类、文本分类、知识图谱补全、信息抽取（命名实体识别、关系抽取）、自然语言推理、解数学问题（文本）等</p>
<h3 id="关于图的semi-supervised">关于图的semi-supervised</h3>
<p>对于监督学习，如一个分类问题，我们的样本需要满足i.i.d assumption：样本之间是独立同分布的（不然还需要建模样本之间的联系）。然而在图结构上做诸如节点分类问题时，节点之间相互联系，且这些联系在节点分类中起到了重要的作用。因此基于图的很多深度学习是semi-supervised。这意味着在训练图模型时，我们利用了测试节点的信息，但不包括label。</p>
<h3 id="深度学习模型的迁移">深度学习模型的迁移</h3>
<p>近几年随着图神经网络的兴起，许多人都涌向这块处女地，深度学习模型中的一些经典思想和模型也被迁移到图相关的模型中，比如基于self-attention的GAT、GraphGAN、Graph Transformer等。另外，在NLP落地的经典搜索、广告、推荐算法中，图神经网络也被广泛应用。<br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.08267">GraphGAN: Graph Representation Learning with Generative Adversarial Nets</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.07470.pdf">Graph Transformer for Graph-to-Sequence Learning</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/10/pd2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/10/pd2/" itemprop="url">Vocabulary Learning via Optimal Transport for Neural Machine Translation [ACL2021]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-10T18:22:13+08:00">
                2021-10-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.15671.pdf">Vocabulary Learning via Optimal Transport for Neural Machine Translation</a><br />
ACL2021 Best paper Code: <a target="_blank" rel="noopener" href="https://github.com/Jingjing-NLP/VOLT">link</a></p>
<p>这篇也就是被ICLR2021拒了后被评为ACL2021 best paper的文章，来自字节跳动的AI Lab。</p>
<h2 id="related-work">Related Work</h2>
<h3 id="subword-model">Subword model</h3>
<p>英文中传统分词方法基于空格进行tokenization。但这一方法面临OOV (Out Of Vocabulary)问题和同一单词的不同形态造成的冗余。因此如今BERT等模型多使用Subword模型，它的划分粒度介于词与字符之间。主流的(指某些中文网站上有博客介绍的）Subword model有Byte Pair Encoding (BPE), WordPiece和Unigram Language Model。</p>
<h3 id="byte-pair-encodingbpe">Byte-Pair-Encoding(BPE)</h3>
<p>&quot;Neural machine translation of rare words with subword units.&quot;arXiv preprint arXiv:1508.07909(2015).<br />
BPE算法被用于处理NMT (Neural Machine Translation)任务中的OOV问题。<br />
BPE是一种自下而上的压缩算法。将单词作为单词片段处理（word pieces），以便于处理未出现单词。</p>
<blockquote>
<p>we adopt BPE generated tokens as the token candidates.</p>
</blockquote>
<p>论文提出的算法要先用BPE...</p>
<h2 id="概要">概要</h2>
<p>机器翻译中，token vocabulary对最终结果会产生很大的影响。论文研究了词表的评价指标以及如何不通过训练直接找到最优的词表。文章的主要内容包括 1. 从信息论角度分析词表的作用 2.借助Optimal transport来找到最佳token词典 3. 更小的词表but更高的BLEU。</p>
<h2 id="intro">Intro</h2>
<p>词汇量（vocabulary size）会影响机器翻译任务的绩效，而通过遍历搜索来寻找最优的词汇量需要极高的计算开销，因此现有的研究大多采用统一的大小，如30k-40k。BPE通过选择频率最高的sub-words做为词典的token以进行数据压缩，以此减少熵。</p>
<p>语料熵随着词汇量的增加而减少，有利于模型学习。另一方面，过多的字符会导致字符稀疏化，这会损害模型学习。本文通过同时考虑熵和词汇量大小来探索自动词汇化，需要找到一个合适的目标函数来同时优化它们。其次，假设给出了适当的度量，由于指数搜索空间（<span class="math inline">\(2^N\)</span>)，解决这种离散优化问题仍然具有挑战性。</p>
<p>针对上述问题，论文提出VOcabulary Learning approach via optimal Transport, VOLT——最优传输的词汇学习方法</p>
<p>总的来说，论文的目标是1.得到“简洁而不臃肿”的词汇表 —— entropy-size trade off 2. 优化搜索过程。</p>
<h2 id="marginal-utility-of-vocabularization-muv">Marginal Utility of Vocabularization (MUV)</h2>
<p>借用经济学中的边际效应的概念，以词汇的边际效应（MUV）作为衡量标准， 然后将目标转向在可处理的时间复杂度中最大化 MUV。 <img src="/images/pd2_3.png" title="vocabulary的边际效益（没有显示给出MUV）" /></p>
<p>在经济学中，边际效应用于平衡收益和成本，因此论文使用 MUV 来平衡熵（收益）和词汇量（成本）。也就是从成本（大小）的增加中获得多大的收益（熵）。</p>
<p><img src="/images/pd2_1.png" title="MUV 与三分之二翻译任务的下游性能相关" /></p>
<h3 id="definition-of-muv">Definition of MUV</h3>
<p>MUV 表示熵对大小的负导数 <span class="math display">\[
\mathcal{M}_{v(k+m)}=\frac{-\left(\mathcal{H}_{v(k+m)}-\mathcal{H}_{v(k)}\right)}{m}
\]</span> 其中 <span class="math inline">\(v(k), v(k+m)\)</span> 是两个分别带有 <span class="math inline">\(k\)</span> 和 <span class="math inline">\(k+m\)</span> 个字符的词汇。<span class="math inline">\(\mathcal{H}_{v}\)</span> 表示词汇表 <span class="math inline">\(v\)</span> 语料库的樀，它由字符樀的总和定义。用字符的平均长度对熵进行归一化来避免字符长度的影响。最终的熵定义为： <span class="math display">\[
\mathcal{H}_{v}=-\frac{1}{l_{v}} \sum_{j \in v} P(j) \log P(j)
\]</span> <span class="math inline">\(P(i)\)</span> 是训练语料库中token <span class="math inline">\(i\)</span> 的相对频率, <span class="math inline">\(l_{v}\)</span> 是词汇表 <span class="math inline">\(v\)</span> 中token的平均长度。</p>
<h3 id="preliminary-results">Preliminary Results</h3>
<p>为了验证 MUV 作为词汇化衡量标准的有效性，作者对来自 TED 的 45 个语言对进行了实验，并计算了 MUV 和 BLEU 分数之间的Spearman相关系数(<span class="math inline">\(\rho\)</span>)。Spearman 得分为 0.4。</p>
<blockquote>
<p>We believe that it is a good signal to show MUV matters</p>
</blockquote>
<p>有了MUV作为评价指标，我们有两个选择来获得最终词表：搜索和学习。作者认为基于学习是更高效的，因此进一步探索了一种基于学习的解决方案 VOLT。（当然最终借助实验比较了 MUV-Search 和 VOLT的性能。）</p>
<h2 id="maximizing-muv-via-optimal-transport">Maximizing MUV via Optimal Transport</h2>
<h3 id="优化问题">优化问题</h3>
<p>首先引入一个辅助变量<span class="math inline">\(S\)</span>，<span class="math inline">\(\boldsymbol{S}=\{k, 2 \cdot k, \ldots,(t-1) \cdot k, \cdots\}\)</span>。 <span class="math inline">\(S\)</span>是一个递增序列，对于每个时间戳t，<span class="math inline">\(S[t]\)</span>代表<strong>不多于<span class="math inline">\(S[t]\)</span>个词条的词表集合</strong>。引入这一变量，根据递推关系来计算任意一个词表的MUV（借助前一个时间戳s[t-1]上的词表递进计算）</p>
<p><span class="math inline">\(k\)</span>代表前后两个词表<span class="math inline">\(v(t)\)</span>和<span class="math inline">\(v(t-1)\)</span>之间的大小差（size gap）。我们的目标是找到MUV最高的<span class="math inline">\(v[t]\)</span> <span class="math display">\[
\begin{array}{l}
\underset{t}{\arg \max } \underset{v(t-1) \in \mathbb{V}_{\boldsymbol{S}[t-1]}, v(t) \in \mathbb{V}_{S[t]}}{\arg \max } \mathcal{M}_{v(t)}= \\
\underset{t}{\arg \max } \underset{v(t-1) \in \mathbb{V}_{\boldsymbol{S}[t-1]}, v(t) \in \mathbb{V}_{S[t]}}{\arg \max }-\frac{1}{k}\left[\mathcal{H}_{v(t)}-\mathcal{H}_{v(t-1)}\right]
\end{array}
\]</span> <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t-1]}\)</span>和 <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t]}\)</span>表示两个词表的集合，其中每个词表大小的上界为<span class="math inline">\(s[t-1]\)</span>和<span class="math inline">\(s[t]\)</span></p>
<blockquote>
<p>The inner arg max represents that the target is to find the vocabulary from <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t]}\)</span> with the maximum MUV scores. The outer arg max means that the target is to enumerate all timesteps and find the vocabulary with the maximum MUV scores.</p>
</blockquote>
<p>遍历t，遍历<span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t-1]}\)</span>。<br />
（词表越大熵越小）上述公式意味着从v(t-1)这个词表，增加i个词/tokens之后，期望新得到的v(t)词表的熵降低的最多。即两个词表对应的熵的差值越大越好。</p>
<blockquote>
<p>Due to exponential search space, we propose to optimize its upper bound: <span class="math display">\[
\underset{t}{\arg \max } \frac{1}{k}\left[\underset{v(t) \in \mathbb{V}_{S[t]}}{\arg \max } \mathcal{H}_{v(t)}-\underset{v(t-1) \in \mathbb{V}_{S[t-11}}{\arg \max } \mathcal{H}_{v(t-1)}\right]
\]</span></p>
</blockquote>
<p>(论文ArXiv上的前一版本中写的还是lower bound...而最新版放的是upper bound...)<br />
anyway至此整个方法可以分成两个步骤：</p>
<ul>
<li>每个时间步t上，寻找最优的词表（按照最大化熵来寻找）</li>
<li>枚举每个时间步t，并输出满足上一个公式的词表（对应的就是时间步t的”最优词表“）</li>
</ul>
<p>step1的目标就是最大化： <span class="math display">\[
\underset{v(t) \in \mathbb{V}_{\boldsymbol{S}[t]}}{\arg \max }-\frac{1}{l_{v(t)}} \sum_{j \in v(t)} P(j) \log P(j)
\]</span> <span class="math inline">\(l_{v}\)</span>是每个token的平均字符长度，<span class="math inline">\(P(j)\)</span>是token j的概率（频率）</p>
<blockquote>
<p>However, notice that this problem is in general intractable due to the extensive vocabulary size. Therefore, we instead propose a relaxation in the formulation of discrete optimal transport, which can then be solved efficiently via the Sinkhorn algorithm</p>
</blockquote>
<p>借助最优传输OT的思想，松弛原优化问题，进而用信息论中的Sinkhorn algorithm求解。</p>
<h3 id="optimal-transport-不太懂">Optimal Transport （不太懂）</h3>
<p><img src="/images/pd2_2.png" title="寻找一个从“character分布、单字分布”到“词表词条分布”的一个最优的运输矩阵的过程" /></p>
<ul>
<li>每个transport matrix对应一个词表；</li>
<li>transport matrix决定有多少chars被“运输”到token候选（词条候选）；</li>
<li>长度为0的tokens（包含0个chars)，不会被增加到词表。</li>
</ul>
<p>不同的”运输矩阵“会带来不同的”运输开销”。而最优化运输（路径）问题的目标就是寻找一个“运输矩阵“，使得”运输开销“(即，负熵)最小化。</p>
<p>目标函数： <span class="math display">\[
\begin{array}{c}
\min _{v \in \mathbb{V}_{S[t]}} \frac{1}{l_{v}} \sum_{j \in v} P(j) \log P(j) \\
\text { s.t. } \quad P(j)=\frac{\operatorname{Token}(j)}{\sum_{j \in v} \operatorname{Token}(j)}, l_{v}=\frac{\sum_{j \in v} \operatorname{len}(j)}{|v|}
\end{array}
\]</span></p>
<p>近似(obtain a tractable lower bound of entropy) - 启发式规则(最长词条匹配原则) - 变换为两部分损失</p>
<p>复杂的推导后得到： <span class="math display">\[
\min _{\boldsymbol{P} \in \mathbb{R}^{m \times n}}\langle\boldsymbol{P}, \boldsymbol{D}\rangle-\gamma H(\boldsymbol{P})
\]</span></p>
<p><span class="math display">\[
\boldsymbol{D}(j, i)=\left\{\begin{array}{ll}
-\log P(i \mid j)=+\infty, &amp; \text { if } i \notin j \\
-\log P(i \mid j)=-\log \frac{1}{\operatorname{len}(j)}, &amp; \text { otherwise }
\end{array}\right.
\]</span></p>
<p><img src="/images/pd2_4.png" title="算法（不太复杂）" /></p>
<h2 id="实验">实验</h2>
<p>3个数据集上NMT任务的BLEU比较（双语语料、多语语料等）<br />
VOLT对比BPE、MUV search更加高效</p>
<p>最后，全论文的核心应该是MUV的提出以及用OT进行优化这一trick，实验结果也比较solid，虽然最终的算法并不复杂，但是OT部分Sinkhorn算法需要较强的信息论背景，本CS专业看了半年仍是一脸懵。<br />
指路一篇介绍Sinkhorn算法的链接： <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1803.00567.pdf" class="uri">https://arxiv.org/pdf/1803.00567.pdf</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/09/p1-position-encoding/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/09/p1-position-encoding/" itemprop="url">Encoding Word Order in Complex Embeddings [ICLR2020]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-09T21:29:54+08:00">
                2021-10-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.12333.pdf">Encoding Word Order in Complex Embeddings</a><br />
Code: <a target="_blank" rel="noopener" href="https://github.com/iclr-complex-order/complex-order">link</a></p>
<h2 id="概要">概要</h2>
<p>针对位置编码提出的改进，切入点新颖高效且为位置编码带来了一定的具体意义和可解释性。传统的位置嵌入捕获单个单词的位置，而不是单个单词位置之间的有序关系(例如邻接关系或优先级)。本文提出的方法建模单词的全局绝对位置和它们的顺序关系，将以前定义为独立向量的词嵌入推广到变量(位置)上的连续词函数。每个单词的表示会随着位置的增加而移动。因此，在连续函数中，不同位置的词表示可以相互关联。将这些函数的通解推广到复值域，得到了更丰富的表示。作者在文本分类、机器翻译和语言模型方面进行实验，取得了良好的表现。</p>
<h2 id="positional-encoding">Positional encoding</h2>
<p>Positional encoding 位置编码在transformer中用于存储位置信息（由于self-attention没法获取序列位置的信息），此外BERT中encoding部分也包含了位置编码。对于位置编码，本能的想法是针对序列中的每个位置必须是独一无二的，且不受序列长度的影响。常见的positional encoding的方法有:</p>
<ul>
<li>绝对（正弦）位置编码（Sinusoidal Position Encoding）</li>
<li>相对位置编码（Relative Position Representations）</li>
<li>可学习位置编码</li>
</ul>
<h3 id="正弦位置编码">正弦位置编码</h3>
<p>Transformer中使用的就是这种编码，实际上具体编码过程使用了正弦和余弦。具体公式为： <span class="math display">\[
\begin{aligned}
P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}} \right) \\
P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}} \right)
\end{aligned}
\]</span> 其中<span class="math inline">\(d_{model}\)</span>为输入词向量的维度。如d(model)=128,那么位置3对应的位置向量为 <span class="math display">\[
\left[\sin \left(3 / 10000^{0 / 128}\right), \cos \left(3 / 10000^{1 / 128}\right), \sin \left(3 / 10000^{2 / 28}\right), \cos \left(3 / 10000^{3 / 28}\right), \ldots\right]
\]</span> 在具体的应用时可能前一部分用正弦后一部分用余弦。</p>
<p><img src="/images/pe4.png" title="Bert中的位置编码" /></p>
<h3 id="相对位置编码">相对位置编码</h3>
<p>Todo<br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a></p>
<h3 id="可学习位置编码">可学习位置编码</h3>
<p>Todo</p>
<h2 id="intro">Intro</h2>
<p>本文的重点在于建模文本信息中额外的词的内部顺序和相邻关系，对比原本位置编码方式仅编码词的位置。模型将之前定义为独立向量的词嵌入扩展为位置自变量上的连续函数。在一个连续函数中，不同位置的词表示可以相互关联。</p>
<p><img src="/images/pe1.png" /></p>
<h2 id="methodology">Methodology</h2>
<p>类似于Word Embedding，位置编码（PE）定义了一个映射关系，将词的序列索引映射为一个向量。<span class="math inline">\(f_{n e}: \mathbb{N} \rightarrow \mathbb{R}^{D}\)</span>。最终某个词的embedding通常表示为为词向量和位置向量的和： <span class="math display">\[f(j, p o s)=f_{w e}(j)+f_{p e}(p o s)\]</span></p>
<p>论文中提出了一个位置独立问题（position independence problem），即位置编码无法捕获相邻词以及其顺序之间的潜在关系。而当后续用于特征处理的网络对这类信息不敏感时，这一问题就会限制整个模型的表达能力。相对位置编码针对这一问题进行了一定的研究，但其无法涵盖整个序列域。</p>
<h3 id="性质">性质</h3>
<p>论文指出了在位置编码中建立词序模型所必需的性质。<br />
由于位置向量中每个维度的值都是根据离散的位置index得到的，这使得位置间有序关系建模变得困难，因此需要根据位置索引构建一个连续的函数（以在每个维度中表示一个特定的单词？） <span class="math display">\[
f(j, \text { pos })=\boldsymbol{g}_{j}(\text { pos }) \in \mathbb{R}^{D}
\]</span> <span class="math inline">\(g_j\)</span>即<span class="math inline">\(\boldsymbol{g}_{w e}(j) \in(\mathcal{F})^{D}\)</span>，词<span class="math inline">\(w_j\)</span>在pos位置可以表示为 <span class="math display">\[
\left[g_{j, 1}(\operatorname{pos}), g_{j, 2}(\operatorname{pos}), \ldots, g_{j, D}(\text { pos })\right] \in \mathbb{R}^{D}
\]</span> 当词<span class="math inline">\(w_j\)</span>从pos位置转到pos’位置时，只需要改变自变量的值而不需要改变映射函数<span class="math inline">\(g_j\)</span>。</p>
<h3 id="函数">函数</h3>
<p>由于实数也被囊括在复数域中，且前人有相关工作（详见论文原文Section2.2）验证了复数域所具有的更强大的表达能力，作者将模型拓展到了复数域。对于理想的映射函数，论文中提出了两条性质，即:</p>
<ul>
<li>Position-free offset transformation</li>
<li>Boundedness</li>
</ul>
<p>变换函数<span class="math inline">\(Transform\)</span>需满足对于任何pos，有 <span class="math display">\[
g(p o s+n)=\operatorname{Transform}_{n}(g(p o s))
\]</span> 满足等式的变换函数被称为witness，而满足这一条件的映射函数<span class="math inline">\(g_j\)</span>则被称为<em>linearly witnessed</em>。规定Transform <span class="math inline">\((n\)</span>, pos <span class="math inline">\()=\)</span> Transform <span class="math inline">\(_{n}(\)</span> pos <span class="math inline">\()=w(n)\)</span>。另外，映射函数<span class="math inline">\(g_j\)</span>需要有界。</p>
<p>而后作者证明了满足上述性质的映射函数唯一解为 <span class="math display">\[
g(p o s)=z_{2} z_{1}^{p o s} \text { for } z_{1}, z_{2} \in \mathbb{C} \text { with }\left|z_{1}\right| \leq 1
\]</span> 对于任意的 <span class="math inline">\(z \in \mathbb{C}\)</span>, 我们可以写成 <span class="math inline">\(z=r e^{i \theta}=r(\cos \theta+i \sin \theta)\)</span>，因此上式可写为： <span class="math display">\[g(p o s)=z_{2} z_{1}^{p o s}=r_{2} e^{i \theta_{2}}\left(r_{1} e^{i \theta_{1}}\right)^{p o s}=r_{2} r_{1}^{p o s} e^{i\left(\theta_{2}+\theta_{1} p o s\right)} \quad$ subject to $\left|r_{1}\right| \leq 1\]</span></p>
<p>(...跳过证明和优化过程)</p>
<p>最终的位置编码函数<span class="math inline">\(f(j\)</span>, pos <span class="math inline">\()\)</span>为 <img src="/images/pe2.png" /> <span class="math inline">\(j\)</span>代表单词（索引），<span class="math inline">\(pos\)</span>表示位置索引。<br />
对于embedding中的每一维度，都有各自的参数，振幅r、频率p、初相<span class="math inline">\(\theta\)</span>，这些参数是trainable的。此外，周期/频率决定了单词对位置的敏感程度。当周期很短，则说明嵌入将对position高度敏感。注意，振幅、频率是与postion（自变量）无关的，与单词和维度有关。此时，word embedding可以用这些参数来表示（维度与positional embedding维度相同）。</p>
<h2 id="实验">实验</h2>
<p>作者在文本分类、机器翻译和语言模型几个任务上进行了实验，分别用Fasttext、LSTM、CNN、Transformer作为模型的backbone，而后使用不同的位置编码方法以及本文的Complex-order编码方法进行embedding，对比几个实验结果均取得了可观的提升。而计算开销（时间）上并没有显著的增加。 <img src="/images/pe3.png" title="部分实验结果" /> 实验基于tensorflow，目前没有pytorch版本，笔者将会尝试将其迁移到pytorch框架下并开源。</p>
<h2 id="相关工作">相关工作</h2>
<p>Vanilla Position Embeddings<br />
Trigonometric Position Embeddings<br />
Todo</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/09/29/textGCN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/09/29/textGCN/" itemprop="url">Improved GCN for Text Classification [GNN]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-09-29T22:24:53+08:00">
                2021-09-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index">
                    <span itemprop="name">Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <center>
<font size = 4> <strong>TextRGNN: Residual Graph Neural Networks for Text Classification</strong></font>
</center>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/default/page/2/">2</a><a class="extend next" rel="next" href="/default/page/2/">&gt;</a>
  </nav>




          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.JPG"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">20</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Bbchen229" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:1109441357@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen jiayuan</span>

  
</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
