<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">



  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Chen's Homepage" type="application/atom+xml" />






<meta property="og:type" content="website">
<meta property="og:title" content="Chen&#39;s Homepage">
<meta property="og:url" content="http://example.com/default/index.html">
<meta property="og:site_name" content="Chen&#39;s Homepage">
<meta property="og:locale">
<meta property="article:author" content="Chen jiayuan">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>Chen's Homepage</title>
  








<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    
    <a target="_blank" rel="noopener" href="https://github.com/Bbchen229" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chen's Homepage</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Hello AI</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/10/cws3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/02/10/cws3/" itemprop="url">中文分词｜Chinese-word-segmentation (3)-基于词</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-02-10T13:15:50+08:00">
                2022-02-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/09/cws2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/02/09/cws2/" itemprop="url">中文分词｜Chinese-word-segmentation (2)-基于字符</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-02-09T17:15:47+08:00">
                2022-02-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="背景">背景</h1>
<h1 id="section"></h1>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/07/cws/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/02/07/cws/" itemprop="url">中文分词｜Chinese-word-segmentation (1)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-02-07T21:01:01+08:00">
                2022-02-07
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="背景">背景</h1>
<p>中文文本中词与词之间没有明确的分割标记，而是以连续字符串形式呈现。所以，任何中文自然语言处理任务都必须解决中文序列切分的问题——中文分词。当然对于英文等语言来说自带分隔符，不需要分词。但在英语手写字识别时，由于分隔符没有那么明显，因此也需要使用类似中文分词的技术。</p>
<p>事实上不同的人对于同一个句子的分词结果并不一定相同，因此将机器分词的结果与人工分词的结果进行比对时，一个98%准确率的分词器与一个99%的分词器很难比较谁效果更好。在中文自然语言处理任务中，诸如机器翻译，自动问答，语音识别等任务，都需要分词技术做支撑。但是对于不同的子任务，分词的细粒度要求也不一样，尤其是对于一些复合词。比如在机器翻译时，清华大学就应该以整个“清华大学”而不是“清华-大学”。可以说中文分词在工程上是一项需要与任务要求紧密结合的技术。</p>
<p>中文分词存在两个难点，一是歧义，二是未登录词（OOV-out of vocabulary）。语言的歧义性一直伴随着语言学的发展进程，也自然限制着NLP技术。另一方面，词典的选择诸如词典中的复合词选择和词典的大小也影响着中文分词任务。举个直观的例子，对于Baidu搜索引擎的分词模型来说，基于人民日报的词典和基于用户搜索数据的词典得到的模型表现就会有很大差异。</p>
<h1 id="分词方法的演变">分词方法的演变</h1>
<h2 id="基于匹配的词典分词">基于匹配的词典分词</h2>
<p>基于匹配的词典分词是非常自然的想法，我们根据词典扫描一个句子，遇到词典中出现过的词就进行分割，这类方法又被称为机械分词。这类方法简单易实现，而且能取得不错的效果，其主要问题包括如何构建一个完备的词典、如何设计高效的匹配算法、匹配中出现的歧义切分。<br />
常见的匹配算法包括:</p>
<ul>
<li>正向最大匹配法</li>
<li>逆向最大匹配法</li>
<li>双向最大匹配法</li>
<li>最少切分</li>
<li>......</li>
</ul>
<p>最大匹配法从句子中寻找长词条进行查字典，若查不到则去掉最后一个字直到找到为止。其中双向最大匹配分别从左到右和从右到左进行两次扫描。最小切分则是寻找使每一个句子切出的词数量最少。</p>
<h2 id="基于标注的机器学习算法">基于标注的机器学习算法</h2>
<p>不同于基于匹配的机械分词，基于统计语言模型的分词技术有效提高了分词的准确率。其中基于标注的机器学习算法将中文分词转化为字序列标注问题。，B表示开始位置、M表示中间位置、E表示结束位置及S表示单字构词。机器学习算法 需要人工设计特征模板，指定窗口的大小。由于算法的复杂度以及对分词结果准确度要求等原因，窗口大小一般不超过5。下面介绍几个具有代表性的模型：</p>
<ul>
<li><p>隐马尔可夫模型（HMM）隐马尔可夫不是一个复杂的数学模型，但能解决大多数自然语言处理问题。其基本的思想是根据观测值序列找到隐状态值序列。在中文分词中，一段文字的每个字符可以看作是一个观测值，而这个字符的位置标签（BEMS）可以看作是隐状态。使用HMM的分词，通过对切分语料库进行统计，可以得到模型中5大要要素：起始概率矩阵，转移概率矩阵，发射概率矩阵，观察值集合，状态值集合。有了三个矩阵和两个集合后，HMM问题最终转化成求解隐藏状态序列最大值的问题，求解这个问题最常使用的是Viterbi算法。</p></li>
<li><p>最大熵马尔可夫模型（MEMM）把HMM模型和maximum-entropy模型的优点集合程一个产生式模型，这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度。</p></li>
<li><p>条件随机场（CRF）是用来标注和划分结构数据的概率化结构模型。和HMM类似，当对于给定的输入观测序列<span class="math inline">\(X\)</span>和输出序列<span class="math inline">\(Y\)</span>，CRF通过定义条件概率<span class="math inline">\(P(Y|X)\)</span>，而不是联合概率分布<span class="math inline">\(P(X,Y)\)</span>来描述模型。MEMM模型对每个节点进行独立归一化，存在偏置问题。条件随机场(CRF)结合了多方面优势，对所有特征进行全局归一化，避免了偏置问题，成为传统机器学习中应用最多、最具代表性的模型算法之一。条件随机场能够获得更高的分词准确率，但模型复杂导致分词效率略低。</p></li>
</ul>
<h2 id="基于理解的深度学习算法">基于理解的深度学习算法</h2>
<p>深度学习模型诸如CNN、GRU、LSTM、BiLSTM被引入中文分词，相对于机器学习而言，深度学习算法无需人工进行特征选择。在基础深度学习模型的基础上，有效结合预训练和后处理方式已成为深度学习的一种趋势，一般性流程如下图所示。 <img src="/images/cws/1.png" title="基于深度学习的中文分词" /> 预训练既可以根据领域需要和任务特点进行预训练，也可以直接使用现有的预训练结果进行微调。中文分词预训练的基本单位是词(字)的语义、偏旁、拼音和输人法等。语义表示的预训练模型包括与上下文无关的静态词向量训练模型Word2Vec、Glove以及与上下文相关的动态词向量训练模型ELMo、BERT、XLNet等。<br />
近几年的中文分词主要分为两类，一个是基于字符的中文分词（根据字所在词的位置，对每个字打上标签），一类是基于词的中文分词。</p>
<h1 id="分词工具">分词工具</h1>
<h2 id="jieba">jieba</h2>
<p>jieba库是一个简单实用的中文自然语言处理分词库，属于概率语言模型分词。<br />
jieba自带一个dict.txt的词典, 里面有2万多条词, 包含了词条出现的次数和词性。将句子根据给定的词典进行查词典操作, 生成所有可能的句子切分，而后根据动态规划查找最大概率路径, 找出基于词频的最大切分组合。对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。<br />
（jieba分词对“自然语言处理”的分词结果为 自然语言｜处理）</p>
<h2 id="ltp">LTP</h2>
<p>LTP是哈工大开源的一套中文语言处理系统，涵盖了基本功能：分词、词性标注、命名实体识别、依存句法分析、语义角色标注、语义依存分析等。LTP基于结构化感知器（Structured Perceptron,SP）属于基于字符的分词模型，以最大熵准则建模标注序列<span class="math inline">\(Y\)</span>在输入序列<span class="math inline">\(X\)</span>的情况下的score函数，分词结果则等同于最大score函数所对应的标注序列。 <span class="math display">\[
S(Y, X)=\sum_{s} \alpha_{s} \Phi_{s}(Y, X)
\]</span> <span class="math inline">\(\Phi_{s}(Y, X)\)</span>为特征函数，分词流程为先提取字符特征，计算特征权重值，然后Viterbi解码。</p>
<h2 id="thula">THULA</h2>
<p>THULA（THU Lexical Analyzer for Chinese）为清华大学推出的中文词法分析工具包，具有中文分词和词性标注功能，其原理与LTP非常相似，在字符特征选择方面有所不同。<br />
测试发现THULA没有针对python3.8进行维护，因此只支持3.7-版本。</p>
<h2 id="stanford-corenlp">Stanford CoreNLP</h2>
<p>CoreNLP的中文分词基于CRF模型： <span class="math display">\[
P_{w}(y \mid x)=\frac{\exp \left(\sum_{i} w_{i} f_{i}(x, y)\right)}{Z_{w}(x)}
\]</span></p>
<p><span class="math inline">\(f_{i}(x, y)\)</span>为特征函数，<span class="math inline">\(w\)</span>为模型参数。不同于其他分词器采用B、M、E、S四种label来做分词，CoreNLP的中文分词label只有两种，“1”表示当前字符与前一字符连接成词，“0”则表示当前字符为另一词的开始——换言之前一字符为上一个词的结尾。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/05/pd12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/02/05/pd12/" itemprop="url">Improving Chinese Word Segmentation with Wordhood Memory Networks [ACL20]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-02-05T23:41:35+08:00">
                2022-02-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>link: <a target="_blank" rel="noopener" href="https://aclanthology.org/2020.acl-main.734/" class="uri">https://aclanthology.org/2020.acl-main.734/</a><br />
<a target="_blank" rel="noopener" href="https://github.com/SVAIGBA/WMSeg">代码</a></p>
<h1 id="背景">背景</h1>
<p>在中文自然语言处理中，分词是一个非常重要的任务。中文分词技术存在两个主要难点:未登录词（OOV）和歧义消除问题。本文属于基于字符的分词模型，主要思想是利用键值记忆网络来辅助分词，使分词的语义更加完整。</p>
<h1 id="模型框架">模型框架</h1>
<p>模型将中文分词作为序列标注问题，核心思想是在传统NER模型的Encoder-Decoder之间添加一个memory network，Encoder可以用BERT或BiLSTM等将汉字序列表示为向量，Decoder可以是Softmax或者CRF。模型总体可以表示为： <span class="math display">\[
\widehat{\mathcal{Y}}=\underset{\mathcal{Y} \in \mathcal{T}^{l}}{\arg \max } p(\mathcal{Y} \mid \mathcal{X}, \mathcal{M}(\mathcal{X}, \mathcal{N}))
\]</span> 其中<span class="math inline">\(\mathcal{T}\)</span>表示句子中所有分词结果的标签集;<span class="math inline">\(l\)</span>是句子长度。<span class="math inline">\(\widehat{\mathcal{Y}}\)</span>为该模型得到的最佳结果，<span class="math inline">\(\mathcal{N}\)</span>为构造的词典，<span class="math inline">\(\mathcal{X}\)</span>为输入句，<span class="math inline">\(\mathcal{M}\)</span>为本文提出的模型。</p>
<p><img src="/images/cws/2.png" title="The architecture of $WMS_{EG}$." /></p>
<h2 id="词典构建">词典构建</h2>
<p>本文构建的词典实际上是一个N-gram词典，它包含了一个句子中所有可能的N-gram。模型利用前人的模型——Accessor Variety，找出输入句子中所有可能的n-gram集合。根据上图给出的例子，所构建的词典如图底部所示。</p>
<h2 id="wordhood-memory-networks">Wordhood Memory Networks</h2>
<p>这部分是本文最重要的部分。作者利用键值记忆网络将字符n-gram与它们的词伙（wordhood）度量相结合。其中key-value分别对应n-grams和wordhood。具体可以分成两个步骤：</p>
<ul>
<li><p>Key Addressing<br />
首先对该句子构建Lexicon，对每一个汉字<span class="math inline">\(x_i\)</span> ，有可能存在很多包含该汉字的n-gram。比如上面的句子中的第四个字&quot;民&quot;构建Lexicon，可以表示为： <span class="math display">\[
K_{4}=[\text {&quot;民&quot;,&quot; 居民&quot;, &quot; 民生&quot;, &quot; 居民生活&quot;] }
\]</span>然后将这些n-gram进行key embeding后<span class="math inline">\(e_{i, j}^{k}\)</span>再与Encoder传来的<span class="math inline">\(h_i\)</span> 相乘之后做softmax得到一个概率分布。概率大小就表明了相关程度： <span class="math display">\[
p_{i, j}=\frac{\exp \left(h_{i} \cdot e_{i, j}^{k}\right)}{\sum_{j=1}^{m_{i}} \exp \left(h_{i} \cdot e_{i, j}^{k}\right)}
\]</span></p></li>
<li><p>Value Reading<br />
先将每个<span class="math inline">\(k_i\)</span>映射到一个标注值V上，因为每个字在不同的n-gram中的位置不同，所以需要映射的值也不同，这里使用B I E S标记法：(B:begin ，I:inside，E:end，S:single)，还是上面的例子，对应Key Addressing时的<span class="math inline">\(K_4\)</span>，得到的value集合为： <span class="math display">\[
V_{4}=\left[V_{S}, V_{E}, V_{B}, V_{I}\right]
\]</span>而后将每个value进行embedding得到<span class="math inline">\(e_{i, j}^{v}\)</span>，与上一步得到的概率进行相乘累加，得到的结果再与Encoder传入的<span class="math inline">\(h_i\)</span>结合得到最终输出的vector <span class="math inline">\(a_i\)</span>。 <span class="math display">\[
O_{i}=\sum_{j=1}^{m_{i}} p_{i, j} e_{i, j}^{v}
\]</span></p></li>
</ul>
<p>接下来就可以和正常的NER模型一样使用一个decoder（softmax、crf等等） 得到一个句子的分词结果了。</p>
<h1 id="实验">实验</h1>
<p>1.消融实验。作者使用了5个基准数据集，分别是CTB6、MSR、PKU、AS和CITYU。将Wordhood Memory Networks加入到目前主流的分词模型中进行比较，实验结果表明，虽然原始的模型有很好的性能，但加入Wordhood Memory Networks后仍有很大的改进<br />
2.在五个基准数据集的测试集上，WMSEG和以前的SOTA模型的性能比较。经过实验比较，本文提出的模型在中文分词方面达到了SOTA水平，OOV的召回率有了很大的提高。</p>
<p><img src="/images/cws/3.png" title="Ablation experiments." /></p>
<h1 id="例子分析">例子分析</h1>
<p>作者用“他从小学习电脑技术”这一句子的分词结果进行可视化分析，发现通过Wordhood Memory Networks对n-gram语义的捕捉，给了“从小”一个较高的权重、&quot;小学&quot;一个较低的权重，得到了正确的分词结果。 <img src="/images/cws/4.png" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/26/pd11-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/01/26/pd11-1/" itemprop="url">Graph Wavelet Neural Network[ICLR2019]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-01-26T12:20:13+08:00">
                2022-01-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.07785.pdf">Graph Wavelet Neural Network</a><br />
Code: <a target="_blank" rel="noopener" href="https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork">link</a></p>
<p>基于小波变换的图卷积神经网络，其背后的思想及理论基础在这一篇论文中无法很详尽的阐述。不过本篇论文相当于把一套传统方法迁移到图上。</p>
<h1 id="intro">Intro</h1>
<p>作者提出了graph wavelet neural network (GWNN)，对于GCN基于传统的图傅里叶变换，GWNN利用graph wavelet transform，用图小波代替图拉普拉斯特征向量作为一组基，利用小波变换和卷积定理定义了卷积算子。使得计算更高效，且在Cora, Citeseer和Pubmed三个图的半监督分类数据集上取得了更好的表现。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/18/pd11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/01/18/pd11/" itemprop="url">Graphite-Iterative Generative Modeling of Graphs [ICML2019]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-01-18T21:47:01+08:00">
                2022-01-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>ICML2019的论文，作者为Stanford的phd。</p>
<h1 id="先说几句">先说几句</h1>
<p>看到摘要</p>
<blockquote>
<p>Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding.</p>
</blockquote>
<p>基本可以确定模型相当于variational graph autoencoders，但是<a href="/2022/01/18/pd10/" title="VGAE">VGAE</a>这名字已经被人抢了XD。不过对比VGAE，它用了更优的Decoder。另一方面，作者给出了图神经网络中message passing与平均场变分推理之间的理论联系。</p>
<h1 id="graphite">Graphite</h1>
<p><img src="/images/pd10/8.png" /> 模型的整体架构与VGAE相似。其中Encoder为 <span class="math display">\[
\boldsymbol{\mu}, \boldsymbol{\sigma}=\mathrm{GNN}_{\phi}(\mathbf{A}, \mathbf{X})
\]</span> 接下来我们重点来看模型的Decoder。在VGAE中，Decoder是节点隐变量的内积，而Graphite提出了reverse message passing作为Decoder。 <span class="math display">\[
\begin{aligned}
\widehat{\mathbf{A}} &amp;=\frac{\mathbf{Z Z}^{T}}{\|\mathbf{Z}\|^{2}}+\mathbf{1 1}^{T} \\
\mathbf{Z}^{*} &amp;=\operatorname{GNN}_{\theta}(\widehat{\mathbf{A}},[\mathbf{Z} \mid \mathbf{X}])
\end{aligned}
\]</span> 模型不断迭代上述两个步骤，也就是reverse message passing。 首先第一步借助隐变量矩阵的内积构建一个邻接矩阵（图）<span class="math inline">\(\widehat{\mathbf{A}} \in \mathbb{R}^{n \times n}\)</span>，加上单位矩阵保证非负。第二步中先将Z和X进行级联，而后与构建的图输入到GNN中。通过不断的重复来更新<span class="math inline">\(Z^*\)</span>，最后使用最终的Z进行内积得到生成的邻接矩阵<span class="math inline">\(\hat A\)</span>。另外，由于图学习通常是在大规模图上操作，在迭代过程中的求内积可以借助GNN这一步中的矩阵乘法来降低复杂度。</p>
<p><img src="/images/pd10/9.png" title="实验结果" /></p>
<h1 id="theoretical-analysis">Theoretical Analysis</h1>
<p>这一部分是作者对图神经网络的message passing与近似推断的关系。<br />
首先我们定义kennel，<span class="math inline">\(K: \mathcal{Z} \times \mathcal{Z} \rightarrow \mathbb{R}\)</span>；映射函数 <span class="math inline">\(T_{\psi}: \mathcal{P} \rightarrow \mathcal{H}\)</span> 其中<span class="math inline">\(\mathcal{P}\)</span>定义了<span class="math inline">\(\mathcal{Z}\)</span>上所有分布的空间。因此可以定义对变量Z的分布的映射，或者成为kernel embedding： <span class="math display">\[
T_{\psi}(p):=\mathbb{E}_{Z \sim p}[\psi(Z)]
\]</span> 我们令这种映射<span class="math inline">\(\psi\)</span>是单射的，即对于任意两个分布<span class="math inline">\(p_1,p_2\)</span>，当<span class="math inline">\(p_{1} \neq p_{2}\)</span>，有<span class="math inline">\(T_{\psi}\left(p_{1}\right) \neq T_{\psi}\left(p_{2}\right)\)</span>。接下来我们定义 函数<span class="math inline">\(\mathcal{O}: \mathcal{P} \rightarrow \mathbb{R}^{d}, d \in \mathbb{N}\)</span>，对于每个<span class="math inline">\(T_{\psi}\)</span>和<span class="math inline">\(\mathcal{O}\)</span>，都存在<span class="math inline">\(\tilde{\mathcal{O}}_{\psi}: \mathcal{H} \rightarrow \mathbb{R}^{d}\)</span>使得: <span class="math display">\[
\mathcal{O}(p)=\tilde{\mathcal{O}}_{\psi}\left(T_{\psi}(p)\right) \quad \forall p \in \mathcal{P}.
\]</span></p>
<p>在GNN中，我们假设<span class="math inline">\(X\)</span>和<span class="math inline">\(A\)</span>都是观测到且在隐变量的条件概率分布中的相互独立的。也就是说我们期望的图是满足，对于<span class="math inline">\(Z\)</span>上的条件分布，当<span class="math inline">\(A\)</span>、<span class="math inline">\(X\)</span>和由边集<span class="math inline">\(E\)</span>确定的节点i的邻接潜变量时，任意单个<span class="math inline">\(Z_i\)</span>与所有其他<span class="math inline">\(Z_j\)</span>都是独立的。这句话非常抽象，可以从下图来理解。 <img src="/images/pd10/10.png" /></p>
<p>当图<span class="math inline">\(G\)</span>满足这一条件是，我们就可以用平均场理论 (mean-feild) <span class="math display">\[
r\left(\mathbf{Z}_{1}, \cdots, \mathbf{Z}_{n} \mid \mathbf{A}, \mathbf{X}\right) \approx \prod_{i=1}^{n} q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)
\]</span> 其中<span class="math inline">\(\phi_{i}\)</span> 代表第i个变分边界的参数。而后我们用真实的条件概率分布与上式的KL散度来优化这些参数: <span class="math display">\[
\min _{\phi_{1}, \cdots, \phi_{n}} \mathrm{KL}\left(\prod_{i=1}^{n} q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right) \| r\left(\mathbf{Z}_{1}, \cdots, \mathbf{Z}_{n} \mid \mathbf{A}, \mathbf{X}\right)\right)
\]</span></p>
<p>而最优的变分边界满足以下形式（证明见论文） <span class="math display">\[
q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)=\mathcal{O}_{\mathcal{G}}^{M F}\left(\mathbf{Z}_{i},\left\{q_{\phi_{j}}\right\}_{j \in \mathcal{N}(i)}\right)
\]</span> 其中<span class="math inline">\(\mathcal{N}(i)\)</span>为<span class="math inline">\(\mathbf{Z}_{i}\)</span>的邻节点。<span class="math inline">\(\mathcal{O}\)</span>是一个由不动点方程确定的函数，它依赖于图自身的性质。这一形式意味着最优的<span class="math inline">\(q_{\phi_{i}}\)</span>的参数只与i的邻节点的q_{}有关。这就意味着平均场近似推断的迭代算法是在图上执行信息传递，直到收敛： <span class="math display">\[
q_{\phi_{i}}^{(l)}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)=\mathcal{O}_{\mathcal{G}}^{M F}\left(\mathbf{Z}_{i},\left\{q_{\phi_{j}}^{(l-1)}\right\}_{j \in \mathcal{N}(i)}\right) .
\]</span></p>
<p>令<span class="math inline">\(\boldsymbol{\mu}_{i}=\mathbb{E}_{\mathbf{Z}_{i} \sim q_{\phi_{i}}}\left[\psi\left(\mathbf{Z}_{i}\right)\right]\)</span>，根据上文提到的结论，我们就可以绕开 具体的<span class="math inline">\(\mathcal{O}\)</span>，将其变为 <span class="math display">\[
\boldsymbol{\mu}_{i}^{(l)}=\tilde{O}_{\psi, \mathcal{G}}^{M F}\left(\left\{\boldsymbol{\mu}_{j}^{(l-1)}\right\}_{j \in \mathcal{N}(i)}\right)
\]</span> 将上式在0处做一阶泰勒展开，有 <span class="math display">\[
\mu_{i}^{(l)} \approx \tilde{O}_{\psi, \mathcal{G}}(\mathbf{0})+\mathbf{N}_{i}^{(l-1)} \cdot \nabla \tilde{O}_{\psi, \mathcal{G}}(\mathbf{0})
\]</span> 这式从形式上就与message passing机制非常类似。 <span class="math display">\[
H_{i}^{(l)}=\eta_{l}\left(B_{l, i}+\sum_{f \in \mathcal{F}_{l}} f\left(\mathbf{A}_{i}\right) \mathbf{H}^{(l-1)} W_{l}\right)
\]</span> 由于在看推导过程时把一些分量的维度等忽略了，因此最后一步的具体细节直接在这里贴原文。</p>
<p><img src="/images/pd10/11.png" title="具体证明过程" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/01/18/pd10/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/01/18/pd10/" itemprop="url">Graph Auto-Encoders</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-01-18T21:39:46+08:00">
                2022-01-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Preliminary-AI/" itemprop="url" rel="index">
                    <span itemprop="name">Preliminary AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文来自Thomas Kipf的博士论文，其论文其他内容包括GCN、relational GCN、compositional imitation learning and execution等。</p>
<p>对于图<span class="math inline">\(G=(V,E)\)</span>，有<span class="math inline">\(N=|V|\)</span>，邻接矩阵<span class="math inline">\(A\)</span>为<span class="math inline">\(N \times N\)</span>，用<span class="math inline">\(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\)</span>来度量两个节点ij的相似度，其中<span class="math inline">\({z}_{i}, {z}_{j}\)</span>为节点的embedding</p>
<h1 id="gae">GAE</h1>
<p><img src="/images/pd10/1.png" title="encoder-decoder architecture" /></p>
<p>Graph Auto-Encoder同样采用encoder-decoder架构，其中scoring function <span class="math inline">\(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\)</span>作为decoder，其根据节点嵌入来重构邻接矩阵；Encoder的输入为图的邻接矩阵<span class="math inline">\(A\)</span>以及节点的特征向量<span class="math inline">\(\left\{\mathbf{x}_{i}\right\}_{i \in \mathcal{V}}\)</span>，输出为node representations <span class="math inline">\(\mathbf{Z}_{i}\)</span>。</p>
<ul>
<li><p>Encoder<br />
GAE的Encoder借助GNN来处理节点的初始化向量和图的结构信息，从而得到节点的表示。比如使用GCN作为Encoder时，有： <span class="math display">\[
\mathbf{Z}=\operatorname{GCN}(\mathbf{X}, \mathbf{A})=\widehat{\mathbf{A}} \operatorname{ReLU}\left(\widehat{\mathbf{A}} \mathbf{X} \mathbf{W}_{0}\right) \mathbf{W}_{1}
\]</span> 除了用GNN作为encoder之外，还有其他的embedding方法，比如最简单的shallow embedding直接根据节点的编号，以及DeepWalk、node2vec等方法。</p></li>
<li><p>Decoder<br />
Decoder用来根据<span class="math inline">\(Z\)</span>重建邻接矩阵 <span class="math display">\[
\mathbf{A}^{\prime}=l\left(\mathbf{Z Z}^{\top}\right)
\]</span> 其中<span class="math inline">\(l(.)\)</span>是logistic sigmoid function，也就是说<span class="math inline">\(A_{i, j}^{\prime}=l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)\)</span></p></li>
</ul>
<p>不过在图表示学习一书中给出了多种decoder <img src="/images/pd10/4.png" title="decoder" /></p>
<ul>
<li>Training<br />
使用交叉熵进行训练 <span class="math display">\[
\mathcal{L}=-\frac{1}{N^{2}} \sum_{i=1}^{N} \sum_{j=1}^{N} A_{i, j} \log l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)+\left(1-A_{i, j}\right) \log \left(1-l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)\right)
\]</span></li>
</ul>
<h1 id="vae">VAE</h1>
<p><img src="/images/pd10/2.png" title="VAE" /></p>
<p>在介绍变分图自编码器 (VGAE)之前，我们先简单介绍一下变分自编码器Variational Auto-encoders。[Auto-Encoding Variational Bayes]</p>
<p><img src="/images/pd10/5.png" title="思路" /> 上图中的实线就代表了生成模型<span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\)</span>，也就是根据隐变量z生成目标数据。而这一生成模型中，我们需要用<span class="math inline">\(q_{\phi}(\mathbf{z} \mid \mathbf{x})\)</span>来拟合无法得到的后验分布<span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\)</span>。而这里就涉及到变分推断VI的内容。</p>
<p>接下来我们具体展开来说。</p>
<p>给定一个真实样本 <span class="math inline">\(x_{k}\)</span>, 我们假设存在一个后验分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span> 并假设这个分布是正态分布。后续我们要训练一个生成器<span class="math inline">\(x=g(z)\)</span>, 使其能从分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span> 采样出来的一个 <span class="math inline">\(\hat x_{k}\)</span> 还原为<span class="math inline">\(x_{k}\)</span>。<br />
对于正态分布的两个参数：均值 <span class="math inline">\(\mu\)</span> 和方差 <span class="math inline">\(\sigma^{2}\)</span>，我们用神经网络进行拟合<span class="math inline">\(\mu_{k}=f_{1}\left(x_{k}\right), \log \sigma_{k}^{2}=f_{2}\left(x_{k}\right)\)</span>。再借助重参数技巧从<span class="math inline">\(z_k\)</span>的分布中采样得到<span class="math inline">\(z_k\)</span>。（对于重参数技巧，就是使采样这一步骤变为可导的一种方法）。</p>
<p>由于VAE最开始假设了隐变量服从正态分布，这就需要神经网络拟合的分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span>向标准正态分布看起，因为 <span class="math display">\[
p(z)=\sum_{x} p(z \mid x) p(x)=\sum_{x} \mathcal{N}(0, I) p(x)=\mathcal{N}(0, I) \sum_{x} p(x)=\mathcal{N}(0, I)
\]</span> 使分布与标准正态看齐这一过程借助在loss增加一个额外的loss（生成分布与标准正态分布的KL散度）来实现 <span class="math display">\[
\mathcal{L}_{\mu, \sigma^{2}}=\frac{1}{2} \sum_{i=1}^{d}\left(\mu_{(i)}^{2}+\sigma_{(i)}^{2}-\log \sigma_{(i)}^{2}-1\right)
\]</span></p>
<p>上述的过程介绍从Auto-Encoder的角度来介绍VAE，事实上如果阅读原论文会发现这种介绍VAE的思路是很令人费解的。回到最基础的贝叶斯学习，我们需要<span class="math inline">\(q_{\phi}\left(\mathrm{z} \mid \mathrm{x}^{(i)}\right)\)</span> 去逼近真实的后验概率 <span class="math inline">\(p_{\theta}\left(\mathrm{z} \mid \mathrm{x}^{(i)}\right)\)</span>，很自然的我们选择用KL散度作为loss，而后经过变分推断的推到转换为优化变分下界 <span class="math display">\[
\tilde{\mathcal{L}}\left(\theta, \phi ; \mathrm{x}^{(i)}\right)=\frac{1}{L} \sum_{l=1}^{L}\left[\log p_{\theta}\left(\mathrm{x}^{(i)}, \mathrm{z}^{(i, l)}\right)-\log q_{\phi}\left(\mathrm{z}^{(i, l)} \mid \mathrm{x}^{(i)}\right)\right]
\]</span> 其中, <span class="math inline">\(\mathrm{z}^{(i, l)}=g_{\phi}\left(\epsilon^{(i, l)}, \mathrm{x}^{(i)}\right), \quad \epsilon^{(i, l)} \sim p(\epsilon)\)</span> 。</p>
<p>而VAE正是给定上述结果中<span class="math inline">\(\epsilon, p_{\theta}(\mathrm{x} \mid \mathrm{z}), q_{\phi}(\mathrm{z} \mid \mathrm{x}), p_{\theta}(\mathrm{z})\)</span> 分布具体形式（正态分布）后的特例。</p>
<h1 id="vgae">VGAE</h1>
<p><img src="/images/pd10/3.png" title="VGAE" /></p>
<p>对于变分图自编码器，简单来看就是输入变为节点特征和邻接矩阵，输出为生成的邻接矩阵。</p>
<p><span class="math inline">\(p_{\theta}(\mathbf{A} \mid \mathbf{X})\)</span>为节点特征<span class="math inline">\(X\)</span>与邻接矩阵A的条件概率分布 <span class="math display">\[
p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{X})=\int p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X}) p(\mathbf{Z} \mid \mathbf{X}) d \mathbf{Z}
\]</span> 其中隐变量先验分布独立于特征向量X，只和节点自身有关<span class="math inline">\(p(\mathbf{Z} \mid \mathbf{X})=\prod_{i=1}^{N} p\left(\mathbf{z}_{i}\right)\)</span>。更具体的说，我们令<span class="math inline">\(p\left(\mathbf{z}_{i}\right)=\mathcal{N}\left(\mathbf{z}_{i} ; \mathbf{0}, \mathbf{I}\right)\)</span>。我们的目标是得到最优的参数<span class="math inline">\(\theta\)</span>。</p>
<p>根据变分推断的框架，我们引入inference model： <span class="math display">\[
q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A})=\prod_{i=1}^{N} q_{\phi}\left(\mathbf{z}_{i} \mid \mathbf{X}, \mathbf{A}\right)
\]</span> 其中 <span class="math inline">\(q_{\phi}\left(\mathbf{z}_{i} \mid \mathbf{X}, \mathbf{A}\right)=\mathcal{N}\left(\mathbf{z}_{i} ; \boldsymbol{\mu}_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right)\)</span></p>
<p>具体模型中，我们用两个GCN作为学习inference model的参数： <span class="math display">\[
\mu_{i}=\left[\mathrm{GCN}^{(1)}(\mathbf{X}, \mathbf{A})\right]_{i}
\]</span> <span class="math display">\[
\log \sigma_{i}=\left[\mathrm{GCN}^{(2)}(\mathbf{X}, \mathbf{A})\right]_{i}
\]</span></p>
<p>接下来的generative model，我们假设它与初始输入的节点特征无关，只与隐变量有关， <span class="math display">\[
p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X})=\prod_{i=1}^{N} \prod_{j=1}^{N} p_{\boldsymbol{\theta}}\left(A_{i, j} \mid \mathbf{z}_{i}, \mathbf{z}_{j}\right)
\]</span></p>
<p>模型所要优化的KL散度与变分推断的过程相似，可以变为优化 <span class="math display">\[
\operatorname{ELBO}=\mathbb{E}_{q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A})}\left[\log p_{\theta}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X})\right]-\operatorname{KL}\left[q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A}) \| p(\mathbf{Z})\right].
\]</span></p>
<p>我们可以将上述过程写的通俗一点，其中编码过程为： <span class="math display">\[
q\left(z_{i} \mid X, A\right)=N\left(z_{i} \mid \mu_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right).
\]</span> 解码（以内积为例）的过程为： <span class="math display">\[
p\left(A_{i j}=1 \mid z_{i}, z_{j}\right)=\sigma\left(z_{i}^{T} z_{j}\right)_{\circ}
\]</span> 损失函数为： <span class="math display">\[
L=E_{q(Z \mid X, A)}[\log p(A \mid Z)]-K L[q(Z \mid X, A) \| p(Z)]
\]</span></p>
<p>作者在边预测任务上测试了VGAE的表现。不过，在Decoder时不考虑节点的特征<span class="math inline">\(X\)</span>仅仅是为了将模型简化，作者发现这不影响link prediction的准确率。 <img src="/images/pd10/6.png" /></p>
<p>值得推敲的是，在论文的前面有关于图卷积GCN在这几个数据集上的表现。而作者却没有用统一的评价指标（精度和准确率）来对比这两个模型的表现。</p>
<p><img src="/images/pd10/7.png" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/21/pd9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/12/21/pd9/" itemprop="url">QUINE'S EMPIRICAL ASSUMPTIONS [1968]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-12-21T20:09:17+08:00">
                2021-12-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言">前言</h1>
<h2 id="絮絮叨叨">絮絮叨叨</h2>
<p>为什么会突然看一篇60年代的文章？要从几天前我从学校图书馆借了一本叫《数学之美》的书说起。首先安利一下这本书，这书薄薄的似乎只有100页，但是书的作者的文学素养和专业知识让AI问题及其背后的数学充满了浪漫主义色彩。然后在书里刚开始关于自然语言处理的相关介绍中，我看到了一个名字-Noam Chomsky （乔姆斯基）。当时我并没有听说过他，于是就去搜索了一下，搜索结果几个关键词抓住了我的眼球：xxxx创始人、最伟大的学者之一。so, who is he?</p>
<h2 id="从规则到统计">从“规则”到“统计”</h2>
<p>自然语言处理作为人工智能下的子领域，自然也陪伴着人工智能走过几个“春季”与“冬季”。具体而言，NLP从早起的基于规则到如今的基于统计，背后不仅是深度学习技术的发展，也是其从“理想主义”为主流到“经验主义”占主导的转变过程。</p>
<blockquote>
<p>基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处理是哲学中的理性主义。在哲学领域中经验主义与理性主义的斗争一直是此消彼长，这种矛盾与斗争也反映在具体科学上，如自然语言处理。 早期的自然语言处理具有鲜明的经验主义色彩。如1913年马尔科夫提出马尔科夫随机过程与马尔科夫模型的基础就是“手工查频”，具体说就是统计了《欧根·奥涅金》长诗中元音与辅音出现频度；1948年香农把离散马尔科夫的概率模型应用于语言的自动机，同时采用手工方法统计英语字母的频率。<br />
然而这种经验主义到了乔姆斯基时出现了转变。<br />
1956年乔姆斯基借鉴香农的工作，把有限状态机作用刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型。这些工作非常伟大，为自然语言和形式语言找到了一种统一的数学描述理论，一个叫做“形式语言理论”的新领域诞生了。但乔老爷子干完这一票之后，挥一挥衣袖，说了一句“有限状态模型不适合用来描述自然语言”。 随后老爷子又补了一刀“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”。 -------《统计自然语言处理》</p>
</blockquote>
<p>从如今language model在NLP中的表现来看这句“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”是非常的离谱，而网上的几篇博客（内容几乎一模一样😅）说这句如此绝对的话来源于<em>QUINE'S EMPIRICAL ASSUMPTIONS</em>这篇文章。因此本着对伟人的尊重和求真务实的态度，我就决定去读一下这篇文章。另外这篇文章是在springer出版的书籍。不过在说这篇文章的内容之前还需要补充介绍几个名词</p>
<h2 id="willard-van-orman-quine-avram-noam-chomsky">Willard Van Orman Quine &amp; Avram Noam Chomsky</h2>
<p>这里我们首先简单的介绍一下两个人。<br />
WVO Quine就是文章标题中的这个Quine，文章中从他的作品<em>word and object</em>开始谈起。直接检索Willard Van Orman Quine会发现百度百科并没有收录这个词条，不过wikipedia倒是有。简单来说，Quine是美国一位著名的哲学家，主张经验主义，另外他倡导Semantic holism-语义整体论。语义整体论可以简单理解为语言的某个部分，无论是术语还是完整的句子，只能通过它与更大语言部分的关系来理解。</p>
<p>Avram Noam Chomsky也就是乔姆斯基（任在世），是美国哲学家，麻省理工学院语言学的荣誉退休教授。乔姆斯基的《句法结构》被认为是20世纪理论语言学研究上最伟大的贡献。<br />
《句法结构》（Syntactic Structures）是乔姆斯基介绍转换生成语法的《语言学理论的逻辑结构》一书的精华版。这一理论认为说话的方式（词序）遵循一定的句法，这种句法是以形式的语法为特征的，具体而言就是一种不受语境影响并带有转换生成规则的语法。儿童被假定为天生具有适用于所有人类语言的基本语法结构的知识。这种与生俱来的知识通常被称作普遍语法理论。</p>
<h2 id="humean-theory">Humean Theory</h2>
<p>休谟（David Hume）是苏格兰不可知论哲学家。他认为人的认知是有局限的。在休谟看来，我们所能认知的“自我”，其实只是感知，人的感知受人的感官局限。休谟认为，因果关系是人的理念，我们倾向于把某种序列中理念间的必然联系归于这种因果关系的本质。也就是说，因果只是我们头脑中的理念而已，两个客体造成恒定感知，比如每天我们看到太阳升起——天就亮了，我们就会把二者视为因果关系。但并不是自然界真的存在因果关系。</p>
<h1 id="正文">正文</h1>
<p>这篇文章是对奎恩经验主义假设的解读，不过这篇文章阅读下来非常晦涩，因为作者在叙述Quine的理论时会通过各种逗号断句或小括号来表达自己的观点，所以文章的内容没有结构化的组织（就这还语言学家呢，就这就这），另外这一篇十多页的文章居然一个小标题都没有。</p>
<p>首先quine的理论来源于Humean theory of language acquisition，他认为人们对于语言的知识可以被表示为a network of linguistic，这也意味着人类的theory，比如chemistry这种二级学科或者基础的学科都可以被表示为a fabric of sentences variously associated to one another。进而人类所有的知识都可以用这些结构来描述。quine的理论中提到了“language”和“theory”。乔姆斯基指出理论与语言是相互渗透的，另外理论还涵盖了common-sense和belief。</p>
<blockquote>
<p>Beneath the uniformity that unites us in communication there is a chaotic <strong>personal diversity of connections</strong>, and, for each of us, <strong>the connections continue to evolve</strong>. No two of us learn our language alike, nor, in a sense, does any finish learning it while he lives.</p>
</blockquote>
<p>奎因表示如果语言是通过<strong>条件反应的机制相互关联</strong>并<strong>与外部刺激相关联</strong>的句子网络，那么一个人对言语行为的倾向可以根据这种网络来表征。按照这种语言的抽象形式，我们如何从语言中获取知识？奎恩提出了一个prelinguistic quality space，其中定义了距离度量（意味着可以度量相似度）。简单来说，在这个空间的某个维度上来看red ball， yellow ball之间的距离比red kerchief要近。这一想法似乎是背离经验主义的，因为这种质量空间可以想象和定义得到的，而非学习得到的。</p>
<p>然而，奎因在他关于语言是如何学习的假说中回到了经典的经验主义概念。与他认为语言是一个句子网络的观点相一致，他列举了学习句子的三种可能机制。首先，句子可以通过“直接条件反射”到“适当的非语言刺激”来学习，也就是说，通过在适当的条件下重复配对句子和刺激；第二，通过句子与句子的关联；第三，新句子可以通过“类比合成”产生，不过这种类比指的并不是类似英语语法规则的东西，而是在固定的上下文中用一个词代替一个类似的词（“手”、“脚”）。他认为一种语言是相关句子的有限网络，有些也与刺激相关，因为这只是两个假定的机制所产生的结构，具有实质性内容的语言学习。</p>
<p>但是乔姆斯基认为语言是句子的无限集合构成的。由假定的机制推导出的网络必定是有限的（对应上文的学习句子的机制），它只会包含人们曾经接触过的句子。</p>
<blockquote>
<p>Presumably, a complex of dispositions is a structure that can be represented as a set of probabilities for utterances in certain definable 'circumstances' or 'situations'. But it must be recognized that the notion 'probability of a sentence' is an entirely useless one, under any known interpreta- tion of this term.</p>
</blockquote>
<p>这里乔姆斯基给出了这句话——句子的概率是没有意义的。他举例说“birds fly”或者“Tuesday follows Monday”这两个英语下句子的概率对日语中产生这两个句子的概率没有意义。他认为probability relative to a situation没有任何意义。如果complex of dispositions是由根据经验观察确定的，那么只有少数传统的问候语、陈词滥调等才有可能与语言的倾向相关联，因为在技术意义上，在任何合理的语料库或数据集中，很少有其他句子可能具有非空的相对频率。且随着语料库的增加，任何给定句子的频率都会无限制地减少。 有人可能会设想用其他方法根据经验为句子分配概率，但乔姆斯基认为，没有一种方法可以避免这些困难。因此，如果一种语言被理解为在正常情况下作出反应的复杂倾向（奎恩的经验主义假说），那么它不仅是有限的、而且非常“小”。</p>
<p>Quine在提出“言语倾向”时指出了翻译的不确定性问题，简单来说可以理解为每个人的说话习惯几乎没有相似之处，因此根本无法建立与这种倾向相一致的翻译手册。对于理论和语言的有限性假设带来的问题，乔姆斯基提出语言是人类头脑的先天属性所带来的，存在一种“普遍语法”。</p>
<p>到这里，我们简单的概括一下前文提到的大概内容，即Quine的理论和乔姆斯基的看法:</p>
<blockquote>
<p>We are left with the fact that Quine develops his explicit notion of 'language' and 'theory' within a narrowly conceived Humean framework (except for the possible intrusion of a rich system of innate ideas), and that he characterizes language learning (&quot;learning of sentences&quot;) in a way consistent with this narrow interpretation, although the conclusion that <strong>a language (or theory) is a finite fabric of sentences, constructed pairwise by training, or a set of sentences with empirically detectable probabilities of being produced</strong> (hence a nearly empty set) is incompatible with various truisms to which Quine would certainly agree.</p>
</blockquote>
<p>Quine依靠他关于知识获取和语言学习的经验主义假设来支持他的一些主要哲学结论。一个重要的例子可以说明这一点。知识的基础是从某些证据上做“分析假设”。对Quine来说，一个关键点是，在基本语言和“常识知识”的情况下，分析假设的正确性并不是“客观问题”，它可以是“对或错”。这些分析假设是超越了 “任何一个本地人的言语行为倾向所隐含的任何东西”。因此，当我们在翻译、学习一门语言时，我们自然而然地会使用这些分析性假设（知识）与母语进行类比。也就是说在Quine的经验主义观点建模下超越言语倾向的知识（分析假设）是一个主观的概念，而这就会带来“翻译的不确定性”。</p>
<p>另外，Quine对基于数据的分析假设的构建和基于数据的“观察句子的刺激意义”的假设进行了鲜明的区分。他指出，后者只涉及“正常感应”类型的不确定性。显然，包含真值功能连接词的句子的翻译（类似地，学习和理解）中涉及的归纳推理也是如此。在这些情况下，归纳法将我们引向“真正的假设”，这与“分析假设”截然不同（在讨论翻译的不确定性时提到的“分析假设”）。因此，Quine认为“<strong>正常归纳</strong>”与“<strong>假设形成或理论建构</strong>”之间存在区别，前者不涉及严重的认识论问题，后者确实涉及此类问题。毫无疑问，这种区别是可以区分的；然而，Quine没有具体说明“正常归纳”所基于的先验属性。这里，乔姆斯基认为大脑天生具有允许从“正常归纳”到“真实假设”的属性，但不允许“理论建构”和一些可能受到狭隘限制的“分析假设”。也就是说，他认为在经验主义下根据数据进行归纳而后得到一个假设的真值（对或错）这个过程是合理，但是直接归纳知识这一过程是不合理的。<br />
因此，一般来说关于语言不可能有一套固定的“分析假设”。我们需要为每种语言（更准确地说，为每种语言的每一个说话者）建立一套新的分析，因为语言的形式没有任何普遍性。</p>
<p>这里还是强调了乔姆斯基对于统计自然语言处理的观点，他认为每种语言，每个说话者的说话倾向会导致无法建立一套普遍的“分析假设”。因此乔姆斯基认为，当我们学习一门语言时，我们并不是在“学习句子”或通过训练获得“行为技能”。相反，我们以某种方式发展了某些原则（当然是无意识的），这些原则决定了许多句子的形式和意义。</p>
<h1 id="转换生成语法">转换生成语法</h1>
<p>在乔姆斯基的《句法结构》一书中，他提出了转换生成语法理论，他认为语言是人类特有的一种先天机制，不仅应该研究语言行为，而且应该研究语言能力，转换-生成语法就是关于语言能力的理论。具体而言，乔姆斯基认为语法主要包括基础和转换两个部分，基础部分生成深层结构，深层结构通过转换得到表层结构，语义部分属于深层结构，它为深层结构作出语义解释。语音部分属于表层结构并为表层结构作出语音解释。强调从认知学的角度对人类语言共性的解释，区分先天的语言能力和后天的语言知识，认为语言有生成能力，是有限规则的无限使用，转换则是生成的重要手段。</p>
<p>他的思想对当时主流的结构主义语言学产生了重要的影响。他的理论包含了几个关键的思想，首先是语义学是独立于语法学之外的，合乎语法的并不一定有意义。另外，他认为语言能力就像行走一样，是人与生俱来的理解语言及遣词造句的能力。</p>
<p>转换生成语法自创立以来, 就以对语言现象的解释充分性为目标, 试图建立一套能像数理公式般进行形式运算推理的规则来解释自然语言。期间虽经反复的修改否定再修改, 每一次都会有新的理论突破, 但其研究的对象、方法和原则却始终如一, 从而极大的推动了当代语言学的发展, 并为语言研究开辟了一条新的道路, 展现了一个全新的发展方向。<br />
比如说，基于规则的句法剖析主要是使用Chomsky的上下文无关语法。在上下文无关语法的基础上, 学者们提出了自顶向下分析法、自底向上分析法、左角分析法、CYK算法、Earley 算法、线图分析法等行之有效的剖析技术。</p>
<p>关于基于规则的自然语言处理在工业界中的应用，可以参考这个链接 https://www.zhihu.com/question/30748126</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/20/candies/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/12/20/candies/" itemprop="url">Gel Candy Evaluation 凝胶糖果测评</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-12-20T14:25:28+08:00">
                2021-12-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/" itemprop="url" rel="index">
                    <span itemprop="name">杂七杂八</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="abstract">Abstract</h1>
<p>Todo</p>
<h1 id="introduction">Introduction</h1>
<p>改革开放以来，人民生活水平日益提升，人们的物质文化生活也越来越丰富多彩。糖果作为小孩子最爱的零食，种类也越来越丰富。在跟王同学一起在研讨间学习的这段时间，在去往图书馆的路上，我偶尔会从天猫超市[1]或者苏果超市[2]买一包软糖给王同学，希望糖果的甜能为她备战考研的枯燥时光中带来些许快乐。由于笔者在这段时间中有点咳嗽，医生叮嘱不可以吃甜食，因此在测评过程中每包糖果我也只吃了几颗（有时一转眼整包就被某人吃完了），但是保证了每一类糖果中包含的每种味道都吃过。下面将对两个超市中售卖的几种凝胶糖果进行测评和打分。对于糖果的评价指标，由于笔者才疏学浅，只能使用个人主观评价以及王同学对部分糖果的一些评价进行评估。</p>
<h1 id="method">Method</h1>
<h2 id="gel-candy">Gel Candy</h2>
<h1 id="experiment">Experiment</h1>
<h2 id="evaluation">Evaluation</h2>
<p>笔者对于这些糖果的味道和口感按照1～5🌟进行打分，另外附带一些文字评价。另外王同学的评价一般包括：就这，还行，味太大，一般。</p>
<h2 id="alpenliebe">Alpenliebe</h2>
<ul>
<li>乐嚼Q动物果果</li>
<li>乐嚼Q虫虫派对</li>
<li>乐嚼Q乳酸果果</li>
<li>乐嚼Q熊猫小队</li>
</ul>
<h2 id="skittles">Skittles</h2>
<h2 id="uha">UHA</h2>
<h1 id="reference">Reference</h1>
<p>【1】东南大学九龙湖校区梅园天猫超市<br />
【2】东南大学九龙湖校区桃园苏果超市</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/08/pd8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/12/08/pd8/" itemprop="url">Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks [ACL2020]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-12-08T13:49:22+08:00">
                2021-12-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文介绍的内容包含多篇基于图神经网络进行文本分类的论文，从提出GCN后的简单应用到去年ACL上的TextING以及今年的BertGCN，GNN在文本分类上取得了非常好的效果。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.05679">Graph Convolutional Networks for Text Classification</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02356v2">Text Level Graph Neural Network for Text Classification</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.13826v1">Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.05727">BertGCN: Transductive Text Classification by Combining GCN and BERT</a></p>
<h1 id="text-classification">Text Classification</h1>
<p>文本分类的应用非常广泛，包括Sentiment Analysis、News Categorization、Topic Analysis甚至是Question Answering等。而automatic text classification方法大致可以被归为两类：基于规则的^rule-based以及基于机器学习的^数据驱动的。</p>
<p>基于规则的文本分类需要先验知识，包括pre-defined rules、domain knowledge等。而基于机器学习的方法则是借助标注数据集。通常我们可以把机器学习文本分类的步骤归为两步，首先是文本特征提取，而后将特征输入分类器进行分类。而发展到现在，基于神经网络的文本分类模型大致也随着深度学习的发展从前馈神经网络到CNN、RNN以及attention、transformer到如今pre-trained model如BERT等。</p>
<p>前馈神经网络进行文本分类通常将文本作为bag of words；RNN则把文本作为词的序列；CNN用于训练提取文本中的关键短语、词组等进行匹配分类；attention机制能识别文本中相互关联的词，可以嵌入到其他深度学习模型中；至于transformer以及BERT这类大规模预训练模型，则是“大力出奇迹”。</p>
<p>而本文的重点，则是基于GNN-图神经网络的文本分类方法。借助图神经网络进行文本中句法、语义解析树之类的图结构信息挖掘，进而进行文本分类。另外经过下文的介绍我们还能发现，基于GNN的模型能与其他深度神经网络进行级联并进行联合训练，进而有效提升分类准确率。<br />
基于图神经网络的文本分类模型的差异大致体现在三个方面：图的构建、节点嵌入的初始化、图神经网络。</p>
<h1 id="textgcn">TextGCN</h1>
<p>Graph Convolutional Networks for Text Classification<br />
<img src="/images/gcn/2.png" title="Framework of TextGCN" /> TextGCN为AAAI2018的论文，现在很多人看到这篇文章的时候可能会感叹“这也能发？”，但事实上这篇论文是最先构建了transductiive的基于GNN进行文本分类的框架，并取得了非常好的表现。</p>
<p>模型整体的框架如上图所示，包括图的构建和图神经网络两个模块。其中图神经网络由简单的两层卷积层构成。另一方面，图节点的特征初始化用one-hot vector，输入的信息仅为边信息和节点的结构信息，而其在分类结果上取得的准确率也反映了GNN应用文本分类的合理性。 <span class="math display">\[
Z=\operatorname{softmax}\left(\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W_{0}\right) W_{1}\right)
\]</span></p>
<p>对于构图方面，模型基于整个语料库构建一个异构图，图中的节点包括文档节点和词节点。而这两类节点之间的边，word-word、doc-word定义如下 <span class="math display">\[
A_{i j}=\left\{\begin{array}{ll}
\operatorname{PMI}(i, j) &amp; i, j \text { are words, } \operatorname{PMI}(i, j)&gt;0 \\
\operatorname{TF}-\operatorname{IDF}_{i j} &amp; i \text { is document, } j \text { is word } \\
1 &amp; i=j \\
0 &amp; \text { otherwise }
\end{array}\right.
\]</span> 其中词i与j之间的PMI (point-wise mutual information) 计算为 <span class="math display">\[
\begin{aligned}
\operatorname{PMI}(i, j) &amp;=\log \frac{p(i, j)}{p(i) p(j)} \\
p(i, j) &amp;=\frac{\# W(i, j)}{\# W} \\
p(i) &amp;=\frac{\# W(i)}{\# W}
\end{aligned}
\]</span> <span class="math inline">\(\#W(i,j)\)</span>代表滑窗中两个词共现次数。另外，图中的词节点会将语料库统计后的低频词过滤掉。至于为什么选择PMI以及TF-IDF这两个指标作为边权重，作者提到是从实验的结果出发作出的选择。 <img src="/images/pd8/2.png" title="Results: TextGCN" /></p>
<h1 id="text-level-gnn">Text-level GNN</h1>
<p>Text Level Graph Neural Network for Text Classification [ENMLP2019]<br />
TextGCN模型跟大部分直推式GNN模型一样，应用时存在明显的缺陷，即没有办法进行在线测试。当我们要输入的新文本进行分类时，需要将文本加入语料库后重新构建graph训练，这就会带来极大的开销。而这篇Text-level GNN则是构建基于文本级别的图，使得基于GNN的文本分类模型提供在线测试的功能，虽然模型依旧是Transductive。 <img src="/images/pd8/3.png" title="Text-level GNN" /> Text-level图的构建如上图所示，其中词与词之间连接的边权重以及词的embedding为整个语料库全局共享，保存在全局矩阵中（上图的两个矩阵），另外文档中的每个词不止和相邻的词存在边，而由一个超参数控制多跳邻居。</p>
<p>在图神经网络模块，虽然论文中介绍的是non-spectral message passing mechanism，但事实上与TextGCN本质上是一样的，不过边的权重会在训练过程中进行更新。 <span class="math display">\[
\begin{aligned}
\mathbf{M}_{\mathbf{n}} &amp;=\max _{a \in \mathcal{N}_{n}^{p}} e_{a n} \mathbf{r}_{\mathbf{a}} \\
\mathbf{r}_{\mathbf{n}}^{\prime} &amp;=\left(1-\eta_{n}\right) \mathbf{M}_{\mathbf{n}}+\eta_{n} \mathbf{r}_{\mathbf{n}}
\end{aligned}
\]</span> 上式中<span class="math inline">\(r\)</span>代表节点特征，<span class="math inline">\(\eta_{n}\)</span>为可训练的参数。<br />
最后，使用文本中的所有词的embedding进行类别的推断；而在TextGCN中则是直接使用文档节点的embedding进行分类。 <span class="math display">\[
y_{i}=\operatorname{softmax}\left(\operatorname{Relu}\left(\mathbf{W} \sum_{n \in N_{i}} \mathbf{r}_{\mathbf{n}}^{\prime}+\mathbf{b}\right)\right)
\]</span></p>
<p>在这里简单对Corpus-level GNN（TextGCN）和Text-level GNN进行简单的比较。首先两者在下游任务的准确率上有较小的差异，其中后者略胜一筹，不过后者使用了Glove词向量进行初始化，所以事实上将TextGCN使用一些小技巧后两者的准确率是非常接近的。不过Text-level GNN优越性体现在其能够提供在线测试上，当输入新文档进行分类时，它的计算开销会远小于TextGCN。而对于它使用的MPM神经网络而不是GCN，是因为MPM更适合它的构图模式，而不是MPM比常规的GCN更强的信息提取能力。Text-level使得全局的边权重必须成为可训练的参数，而MPM中的另一个可训练的参数<span class="math inline">\(\eta_{n}\)</span>实质上与GCN中结合<span class="math inline">\(I\)</span>的拉普拉斯矩阵<span class="math inline">\(L\)</span>是一致的。</p>
<h1 id="texting">TextING</h1>
<p>Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks<br />
上文的两个模型以及后续的BertGCN都是transductive，而本文的TextING则是inducive模型。论文发表在ACL2020上，也就是博客标题。 <img src="/images/pd8/4.png" title="TextING" /> 模型针对每篇文档构建一个图，以词共现作为边节点，借助滑窗（size 3）构建图。节点嵌入用Glove进行初始化。 模型的图神经网络模块使用了Gated Graph Neural Networks(GGNN)和图注意力(GAT)。 <span class="math display">\[
\begin{aligned}
\mathbf{a}^{t} &amp;=\mathbf{A h}^{t-1} \mathbf{W}_{a} \\
\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z} \mathbf{a}^{t}+\mathbf{U}_{z} \mathbf{h}^{t-1}+\mathbf{b}_{z}\right) \\
\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r} \mathbf{a}^{t}+\mathbf{U}_{r} \mathbf{h}^{t-1}+\mathbf{b}_{r}\right) \\
\tilde{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W}_{h} \mathbf{a}^{t}+\mathbf{U}_{h}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)+\mathbf{b}_{h}\right) \\
\mathbf{h}^{t} &amp;=\tilde{\mathbf{h}}^{t} \odot \mathbf{z}^{t}+\mathbf{h}^{t-1} \odot\left(1-\mathbf{z}^{t}\right)
\end{aligned}
\]</span></p>
<p>上式中<span class="math inline">\(h\)</span>表示节点embedding，<span class="math inline">\(a\)</span>代表接受的信息，<span class="math inline">\(z\)</span>和<span class="math inline">\(r\)</span>分别代表更新和遗忘。而后将节点借助readout模块输出为graph-level的embedding： <span class="math display">\[
\begin{array}{l}
\mathbf{h}_{v}=\sigma\left(f_{1}\left(\mathbf{h}_{v}^{t}\right)\right) \odot \tanh \left(f_{2}\left(\mathbf{h}_{v}^{t}\right)\right) \\
\mathbf{h}_{\mathcal{G}}=\frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \mathbf{h}_{v}+\text { Maxpooling }\left(\mathbf{h}_{1} \ldots \mathbf{h}_{\mathcal{V}}\right)
\end{array}
\]</span></p>
<p>上文提到的两个模型中GNN并没有attention模块，这是由于TextGCN的PMI、TF-IDF信息会损失，另一方面Text-level GNN全局的边权重也不应该引入文本图中的attention机制进行更新。</p>
<p>将上文的三个模型进行简单的对比，可以隐约感到存在一个图的构建与图神经网络之间的trade-off。前两个模型的图构建过程都嵌入了大量的信息（先验信息、全局信息），而他们的图神经网络都非常简单。事实上我曾在TextGCN上做过一些实验，尝试把GAT融入到信息传播过程中，发现准确率会有明显下降。而TextING的构图过程中的信息仅是词节点的共现所包含的上下文信息和结构信息，因此它可以接受更复杂的信息传播和聚合过程。这也使得TextING可以面对新词和新输入的文本直接进行分类。</p>
<h1 id="bertgcn">BertGCN</h1>
<p>BertGCN: Transductive Text Classification by Combining GCN and BERT<br />
BertGCN是由香侬科技提出，发表在ACL2021上的文章，也是目前文本分类的SOTA模型。不过这篇文章的贡献主要是工程上的。另外，不妨排列组合一下将Bert与TextING结合，应该能取得更好的结果XD（虽然在R8上的结果已经非常接近100了）。<br />
回顾TextGCN，模型中图节点初始化用的是one-hot向量，而BertGCN则是用Bert进行embedding的初始化，另外将Bert与GNN两个模块进行联合训练，取得了很好的表现。</p>
<p>两者的结合存在两个问题，一是难收敛：BERT与GCN处理数据的方式不同、模型大小不同；二是GCN是在整个图上运算，而BERT过大的模型无法一次全部加载图中所有结点，这就给BertGCN的训练带来阻碍。 针对第一个问题，模型使用了Interpolating损失。 <span class="math display">\[
Z=\lambda Z_{\mathrm{GCN}}+(1-\lambda) Z_{\mathrm{BERT}}, Z_{\mathrm{BERT}}=\operatorname{softmax}(W X)
\]</span> 当<span class="math inline">\(\lambda=1\)</span>时，BERT模块没有更新；当<span class="math inline">\(\lambda=0\)</span>时，GCN模块没有更新；当<span class="math inline">\(\lambda \in (0,1)\)</span>时，两个模块都能得到更新，并且通过调节<span class="math inline">\(\lambda\)</span>实现BertGCN整体模块的快速收敛。<br />
对于无法整图训练这一问题，BertGNN提出了一个Memory Bank用于保存所有节点特征，每次从中加载batch进行训练并更新，其他保持不变。将整个语料库中的文档特征分批更新，为了防止异步更新带来的不一致性，模型在训练Bert模型时采用了小学习率。</p>
<p><img src="/images/pd8/1.png" title="Results: BertGCN" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/default/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/default/page/4/">4</a><a class="extend next" rel="next" href="/default/page/2/">&gt;</a>
  </nav>




          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.JPG"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Bbchen229" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:1109441357@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen jiayuan</span>

  
</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
