<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">



  <link rel="icon" type="image/png" sizes="16x16" href="/images/logo.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Chen's Homepage" type="application/atom+xml" />






<meta property="og:type" content="website">
<meta property="og:title" content="Chen&#39;s Homepage">
<meta property="og:url" content="http://example.com/default/index.html">
<meta property="og:site_name" content="Chen&#39;s Homepage">
<meta property="og:locale">
<meta property="article:author" content="Chen jiayuan">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/"/>





  <title>Chen's Homepage</title>
  








<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    
    <a target="_blank" rel="noopener" href="https://github.com/Bbchen229" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chen's Homepage</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Hello AI</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/21/pd9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/12/21/pd9/" itemprop="url">QUINE'S EMPIRICAL ASSUMPTIONS [1968]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-12-21T20:09:17+08:00">
                2021-12-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言">前言</h1>
<h2 id="絮絮叨叨">絮絮叨叨</h2>
<p>为什么会突然看一篇60年代的文章？要从几天前我从学校图书馆借了一本叫《数学之美》的书说起。首先安利一下这本书，这书薄薄的似乎只有100页，但是书的作者的文学素养和专业知识让AI问题及其背后的数学充满了浪漫主义色彩。然后在书里刚开始关于自然语言处理的相关介绍中，我看到了一个名字-Noam Chomsky （乔姆斯基）。当时我并没有听说过他，于是就去搜索了一下，搜索结果几个关键词抓住了我的眼球：xxxx创始人、最伟大的学者之一。so, who is he?</p>
<h2 id="从规则到统计">从“规则”到“统计”</h2>
<p>自然语言处理作为人工智能下的子领域，自然也陪伴着人工智能走过几个“春季”与“冬季”。具体而言，NLP从早起的基于规则到如今的基于统计，背后不仅是深度学习技术的发展，也是其从“理想主义”为主流到“经验主义”占主导的转变过程。</p>
<blockquote>
<p>基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处理是哲学中的理性主义。在哲学领域中经验主义与理性主义的斗争一直是此消彼长，这种矛盾与斗争也反映在具体科学上，如自然语言处理。 早期的自然语言处理具有鲜明的经验主义色彩。如1913年马尔科夫提出马尔科夫随机过程与马尔科夫模型的基础就是“手工查频”，具体说就是统计了《欧根·奥涅金》长诗中元音与辅音出现频度；1948年香农把离散马尔科夫的概率模型应用于语言的自动机，同时采用手工方法统计英语字母的频率。<br />
然而这种经验主义到了乔姆斯基时出现了转变。<br />
1956年乔姆斯基借鉴香农的工作，把有限状态机作用刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型。这些工作非常伟大，为自然语言和形式语言找到了一种统一的数学描述理论，一个叫做“形式语言理论”的新领域诞生了。但乔老爷子干完这一票之后，挥一挥衣袖，说了一句“有限状态模型不适合用来描述自然语言”。 随后老爷子又补了一刀“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”。 -------《统计自然语言处理》</p>
</blockquote>
<p>从如今language model在NLP中的表现来看这句“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”是非常的离谱，而网上的几篇博客（内容几乎一模一样😅）说这句如此绝对的话来源于<em>QUINE'S EMPIRICAL ASSUMPTIONS</em>这篇文章。因此本着对伟人的尊重和求真务实的态度，我就决定去读一下这篇文章。另外这篇文章是在springer出版的书籍。不过在说这篇文章的内容之前还需要补充介绍几个名词</p>
<h2 id="willard-van-orman-quine-avram-noam-chomsky">Willard Van Orman Quine &amp; Avram Noam Chomsky</h2>
<p>这里我们首先简单的介绍一下两个人。<br />
WVO Quine就是文章标题中的这个Quine，文章中从他的作品<em>word and object</em>开始谈起。直接检索Willard Van Orman Quine会发现百度百科并没有收录这个词条，不过wikipedia倒是有。简单来说，Quine是美国一位著名的哲学家，主张经验主义，另外他倡导Semantic holism-语义整体论。语义整体论可以简单理解为语言的某个部分，无论是术语还是完整的句子，只能通过它与更大语言部分的关系来理解。</p>
<p>Avram Noam Chomsky也就是乔姆斯基（任在世），是美国哲学家，麻省理工学院语言学的荣誉退休教授。乔姆斯基的《句法结构》被认为是20世纪理论语言学研究上最伟大的贡献。<br />
《句法结构》（Syntactic Structures）是乔姆斯基介绍转换生成语法的《语言学理论的逻辑结构》一书的精华版。这一理论认为说话的方式（词序）遵循一定的句法，这种句法是以形式的语法为特征的，具体而言就是一种不受语境影响并带有转换生成规则的语法。儿童被假定为天生具有适用于所有人类语言的基本语法结构的知识。这种与生俱来的知识通常被称作普遍语法理论。</p>
<h2 id="humean-theory">Humean Theory</h2>
<p>休谟（David Hume）是苏格兰不可知论哲学家。他认为人的认知是有局限的。在休谟看来，我们所能认知的“自我”，其实只是感知，人的感知受人的感官局限。休谟认为，因果关系是人的理念，我们倾向于把某种序列中理念间的必然联系归于这种因果关系的本质。也就是说，因果只是我们头脑中的理念而已，两个客体造成恒定感知，比如每天我们看到太阳升起——天就亮了，我们就会把二者视为因果关系。但并不是自然界真的存在因果关系。</p>
<h1 id="正文">正文</h1>
<p>这篇文章是对奎恩经验主义假设的解读，不过这篇文章阅读下来非常晦涩，因为作者在叙述Quine的理论时会通过各种逗号断句或小括号来表达自己的观点，所以文章的内容没有结构化的组织（就这还语言学家呢，就这就这），另外这一篇十多页的文章居然一个小标题都没有。</p>
<p>首先quine的理论来源于Humean theory of language acquisition，他认为人们对于语言的知识可以被表示为a network of linguistic，这也意味着人类的theory，比如chemistry这种二级学科或者基础的学科都可以被表示为a fabric of sentences variously associated to one another。进而人类所有的知识都可以用这些结构来描述。quine的理论中提到了“language”和“theory”。乔姆斯基指出理论与语言是相互渗透的，另外理论还涵盖了common-sense和belief。</p>
<blockquote>
<p>Beneath the uniformity that unites us in communication there is a chaotic <strong>personal diversity of connections</strong>, and, for each of us, <strong>the connections continue to evolve</strong>. No two of us learn our language alike, nor, in a sense, does any finish learning it while he lives.</p>
</blockquote>
<p>奎因表示如果语言是通过<strong>条件反应的机制相互关联</strong>并<strong>与外部刺激相关联</strong>的句子网络，那么一个人对言语行为的倾向可以根据这种网络来表征。按照这种语言的抽象形式，我们如何从语言中获取知识？奎恩提出了一个prelinguistic quality space，其中定义了距离度量（意味着可以度量相似度）。简单来说，在这个空间的某个维度上来看red ball， yellow ball之间的距离比red kerchief要近。这一想法似乎是背离经验主义的，因为这种质量空间可以想象和定义得到的，而非学习得到的。</p>
<p>然而，奎因在他关于语言是如何学习的假说中回到了经典的经验主义概念。与他认为语言是一个句子网络的观点相一致，他列举了学习句子的三种可能机制。首先，句子可以通过“直接条件反射”到“适当的非语言刺激”来学习，也就是说，通过在适当的条件下重复配对句子和刺激；第二，通过句子与句子的关联；第三，新句子可以通过“类比合成”产生，不过这种类比指的并不是类似英语语法规则的东西，而是在固定的上下文中用一个词代替一个类似的词（“手”、“脚”）。他认为一种语言是相关句子的有限网络，有些也与刺激相关，因为这只是两个假定的机制所产生的结构，具有实质性内容的语言学习。</p>
<p>但是乔姆斯基认为语言是句子的无限集合构成的。由假定的机制推导出的网络必定是有限的（对应上文的学习句子的机制），它只会包含人们曾经接触过的句子。</p>
<blockquote>
<p>Presumably, a complex of dispositions is a structure that can be represented as a set of probabilities for utterances in certain definable 'circumstances' or 'situations'. But it must be recognized that the notion 'probability of a sentence' is an entirely useless one, under any known interpreta- tion of this term.</p>
</blockquote>
<p>这里乔姆斯基给出了这句话——句子的概率是没有意义的。他举例说“birds fly”或者“Tuesday follows Monday”这两个英语下句子的概率对日语中产生这两个句子的概率没有意义。他认为probability relative to a situation没有任何意义。如果complex of dispositions是由根据经验观察确定的，那么只有少数传统的问候语、陈词滥调等才有可能与语言的倾向相关联，因为在技术意义上，在任何合理的语料库或数据集中，很少有其他句子可能具有非空的相对频率。且随着语料库的增加，任何给定句子的频率都会无限制地减少。 有人可能会设想用其他方法根据经验为句子分配概率，但乔姆斯基认为，没有一种方法可以避免这些困难。因此，如果一种语言被理解为在正常情况下作出反应的复杂倾向（奎恩的经验主义假说），那么它不仅是有限的、而且非常“小”。</p>
<p>Quine在提出“言语倾向”时指出了翻译的不确定性问题，简单来说可以理解为每个人的说话习惯几乎没有相似之处，因此根本无法建立与这种倾向相一致的翻译手册。对于理论和语言的有限性假设带来的问题，乔姆斯基提出语言是人类头脑的先天属性所带来的，存在一种“普遍语法”。</p>
<p>到这里，我们简单的概括一下前文提到的大概内容，即Quine的理论和乔姆斯基的看法:</p>
<blockquote>
<p>We are left with the fact that Quine develops his explicit notion of 'language' and 'theory' within a narrowly conceived Humean framework (except for the possible intrusion of a rich system of innate ideas), and that he characterizes language learning (&quot;learning of sentences&quot;) in a way consistent with this narrow interpretation, although the conclusion that <strong>a language (or theory) is a finite fabric of sentences, constructed pairwise by training, or a set of sentences with empirically detectable probabilities of being produced</strong> (hence a nearly empty set) is incompatible with various truisms to which Quine would certainly agree.</p>
</blockquote>
<p>Quine依靠他关于知识获取和语言学习的经验主义假设来支持他的一些主要哲学结论。一个重要的例子可以说明这一点。知识的基础是从某些证据上做“分析假设”。对Quine来说，一个关键点是，在基本语言和“常识知识”的情况下，分析假设的正确性并不是“客观问题”，它可以是“对或错”。这些分析假设是超越了 “任何一个本地人的言语行为倾向所隐含的任何东西”。因此，当我们在翻译、学习一门语言时，我们自然而然地会使用这些分析性假设（知识）与母语进行类比。也就是说在Quine的经验主义观点建模下超越言语倾向的知识（分析假设）是一个主观的概念，而这就会带来“翻译的不确定性”。</p>
<p>另外，Quine对基于数据的分析假设的构建和基于数据的“观察句子的刺激意义”的假设进行了鲜明的区分。他指出，后者只涉及“正常感应”类型的不确定性。显然，包含真值功能连接词的句子的翻译（类似地，学习和理解）中涉及的归纳推理也是如此。在这些情况下，归纳法将我们引向“真正的假设”，这与“分析假设”截然不同（在讨论翻译的不确定性时提到的“分析假设”）。因此，Quine认为“<strong>正常归纳</strong>”与“<strong>假设形成或理论建构</strong>”之间存在区别，前者不涉及严重的认识论问题，后者确实涉及此类问题。毫无疑问，这种区别是可以区分的；然而，Quine没有具体说明“正常归纳”所基于的先验属性。这里，乔姆斯基认为大脑天生具有允许从“正常归纳”到“真实假设”的属性，但不允许“理论建构”和一些可能受到狭隘限制的“分析假设”。也就是说，他认为在经验主义下根据数据进行归纳而后得到一个假设的真值（对或错）这个过程是合理，但是直接归纳知识这一过程是不合理的。<br />
因此，一般来说关于语言不可能有一套固定的“分析假设”。我们需要为每种语言（更准确地说，为每种语言的每一个说话者）建立一套新的分析，因为语言的形式没有任何普遍性。</p>
<p>这里还是强调了乔姆斯基对于统计自然语言处理的观点，他认为每种语言，每个说话者的说话倾向会导致无法建立一套普遍的“分析假设”。因此乔姆斯基认为，当我们学习一门语言时，我们并不是在“学习句子”或通过训练获得“行为技能”。相反，我们以某种方式发展了某些原则（当然是无意识的），这些原则决定了许多句子的形式和意义。</p>
<h1 id="转换生成语法">转换生成语法</h1>
<p>在乔姆斯基的《句法结构》一书中，他提出了转换生成语法理论，他认为语言是人类特有的一种先天机制，不仅应该研究语言行为，而且应该研究语言能力，转换-生成语法就是关于语言能力的理论。具体而言，乔姆斯基认为语法主要包括基础和转换两个部分，基础部分生成深层结构，深层结构通过转换得到表层结构，语义部分属于深层结构，它为深层结构作出语义解释。语音部分属于表层结构并为表层结构作出语音解释。强调从认知学的角度对人类语言共性的解释，区分先天的语言能力和后天的语言知识，认为语言有生成能力，是有限规则的无限使用，转换则是生成的重要手段。</p>
<p>他的思想对当时主流的结构主义语言学产生了重要的影响。他的理论包含了几个关键的思想，首先是语义学是独立于语法学之外的，合乎语法的并不一定有意义。另外，他认为语言能力就像行走一样，是人与生俱来的理解语言及遣词造句的能力。</p>
<p>转换生成语法自创立以来, 就以对语言现象的解释充分性为目标, 试图建立一套能像数理公式般进行形式运算推理的规则来解释自然语言。期间虽经反复的修改否定再修改, 每一次都会有新的理论突破, 但其研究的对象、方法和原则却始终如一, 从而极大的推动了当代语言学的发展, 并为语言研究开辟了一条新的道路, 展现了一个全新的发展方向。<br />
比如说，基于规则的句法剖析主要是使用Chomsky的上下文无关语法。在上下文无关语法的基础上, 学者们提出了自顶向下分析法、自底向上分析法、左角分析法、CYK算法、Earley 算法、线图分析法等行之有效的剖析技术。</p>
<p>关于基于规则的自然语言处理在工业界中的应用，可以参考这个链接 https://www.zhihu.com/question/30748126</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/12/08/pd8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/12/08/pd8/" itemprop="url">Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks [ACL2020]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-12-08T13:49:22+08:00">
                2021-12-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>本文介绍的内容包含多篇基于图神经网络进行文本分类的论文，从提出GCN后的简单应用到去年ACL上的TextING以及今年的BertGCN，GNN在文本分类上取得了非常好的效果。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.05679">Graph Convolutional Networks for Text Classification</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02356v2">Text Level Graph Neural Network for Text Classification</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.13826v1">Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.05727">BertGCN: Transductive Text Classification by Combining GCN and BERT</a></p>
<h1 id="text-classification">Text Classification</h1>
<p>文本分类的应用非常广泛，包括Sentiment Analysis、News Categorization、Topic Analysis甚至是Question Answering等。而automatic text classification方法大致可以被归为两类：基于规则的^rule-based以及基于机器学习的^数据驱动的。</p>
<p>基于规则的文本分类需要先验知识，包括pre-defined rules、domain knowledge等。而基于机器学习的方法则是借助标注数据集。通常我们可以把机器学习文本分类的步骤归为两步，首先是文本特征提取，而后将特征输入分类器进行分类。而发展到现在，基于神经网络的文本分类模型大致也随着深度学习的发展从前馈神经网络到CNN、RNN以及attention、transformer到如今pre-trained model如BERT等。</p>
<p>前馈神经网络进行文本分类通常将文本作为bag of words；RNN则把文本作为词的序列；CNN用于训练提取文本中的关键短语、词组等进行匹配分类；attention机制能识别文本中相互关联的词，可以嵌入到其他深度学习模型中；至于transformer以及BERT这类大规模预训练模型，则是“大力出奇迹”。</p>
<p>而本文的重点，则是基于GNN-图神经网络的文本分类方法。借助图神经网络进行文本中句法、语义解析树之类的图结构信息挖掘，进而进行文本分类。另外经过下文的介绍我们还能发现，基于GNN的模型能与其他深度神经网络进行级联并进行联合训练，进而有效提升分类准确率。<br />
基于图神经网络的文本分类模型的差异大致体现在三个方面：图的构建、节点嵌入的初始化、图神经网络。</p>
<h1 id="textgcn">TextGCN</h1>
<p>Graph Convolutional Networks for Text Classification<br />
<img src="/images/gcn/2.png" title="Framework of TextGCN" /> TextGCN为AAAI2018的论文，现在很多人看到这篇文章的时候可能会感叹“这也能发？”，但事实上这篇论文是最先构建了transductiive的基于GNN进行文本分类的框架，并取得了非常好的表现。</p>
<p>模型整体的框架如上图所示，包括图的构建和图神经网络两个模块。其中图神经网络由简单的两层卷积层构成。另一方面，图节点的特征初始化用one-hot vector，输入的信息仅为边信息和节点的结构信息，而其在分类结果上取得的准确率也反映了GNN应用文本分类的合理性。 <span class="math display">\[
Z=\operatorname{softmax}\left(\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W_{0}\right) W_{1}\right)
\]</span></p>
<p>对于构图方面，模型基于整个语料库构建一个异构图，图中的节点包括文档节点和词节点。而这两类节点之间的边，word-word、doc-word定义如下 <span class="math display">\[
A_{i j}=\left\{\begin{array}{ll}
\operatorname{PMI}(i, j) &amp; i, j \text { are words, } \operatorname{PMI}(i, j)&gt;0 \\
\operatorname{TF}-\operatorname{IDF}_{i j} &amp; i \text { is document, } j \text { is word } \\
1 &amp; i=j \\
0 &amp; \text { otherwise }
\end{array}\right.
\]</span> 其中词i与j之间的PMI (point-wise mutual information) 计算为 <span class="math display">\[
\begin{aligned}
\operatorname{PMI}(i, j) &amp;=\log \frac{p(i, j)}{p(i) p(j)} \\
p(i, j) &amp;=\frac{\# W(i, j)}{\# W} \\
p(i) &amp;=\frac{\# W(i)}{\# W}
\end{aligned}
\]</span> <span class="math inline">\(\#W(i,j)\)</span>代表滑窗中两个词共现次数。另外，图中的词节点会将语料库统计后的低频词过滤掉。至于为什么选择PMI以及TF-IDF这两个指标作为边权重，作者提到是从实验的结果出发作出的选择。 <img src="/images/pd8/2.png" title="Results: TextGCN" /></p>
<h1 id="text-level-gnn">Text-level GNN</h1>
<p>Text Level Graph Neural Network for Text Classification [ENMLP2019]<br />
TextGCN模型跟大部分直推式GNN模型一样，应用时存在明显的缺陷，即没有办法进行在线测试。当我们要输入的新文本进行分类时，需要将文本加入语料库后重新构建graph训练，这就会带来极大的开销。而这篇Text-level GNN则是构建基于文本级别的图，使得基于GNN的文本分类模型提供在线测试的功能，虽然模型依旧是Transductive。 <img src="/images/pd8/3.png" title="Text-level GNN" /> Text-level图的构建如上图所示，其中词与词之间连接的边权重以及词的embedding为整个语料库全局共享，保存在全局矩阵中（上图的两个矩阵），另外文档中的每个词不止和相邻的词存在边，而由一个超参数控制多跳邻居。</p>
<p>在图神经网络模块，虽然论文中介绍的是non-spectral message passing mechanism，但事实上与TextGCN本质上是一样的，不过边的权重会在训练过程中进行更新。 <span class="math display">\[
\begin{aligned}
\mathbf{M}_{\mathbf{n}} &amp;=\max _{a \in \mathcal{N}_{n}^{p}} e_{a n} \mathbf{r}_{\mathbf{a}} \\
\mathbf{r}_{\mathbf{n}}^{\prime} &amp;=\left(1-\eta_{n}\right) \mathbf{M}_{\mathbf{n}}+\eta_{n} \mathbf{r}_{\mathbf{n}}
\end{aligned}
\]</span> 上式中<span class="math inline">\(r\)</span>代表节点特征，<span class="math inline">\(\eta_{n}\)</span>为可训练的参数。<br />
最后，使用文本中的所有词的embedding进行类别的推断；而在TextGCN中则是直接使用文档节点的embedding进行分类。 <span class="math display">\[
y_{i}=\operatorname{softmax}\left(\operatorname{Relu}\left(\mathbf{W} \sum_{n \in N_{i}} \mathbf{r}_{\mathbf{n}}^{\prime}+\mathbf{b}\right)\right)
\]</span></p>
<p>在这里简单对Corpus-level GNN（TextGCN）和Text-level GNN进行简单的比较。首先两者在下游任务的准确率上有较小的差异，其中后者略胜一筹，不过后者使用了Glove词向量进行初始化，所以事实上将TextGCN使用一些小技巧后两者的准确率是非常接近的。不过Text-level GNN优越性体现在其能够提供在线测试上，当输入新文档进行分类时，它的计算开销会远小于TextGCN。而对于它使用的MPM神经网络而不是GCN，是因为MPM更适合它的构图模式，而不是MPM比常规的GCN更强的信息提取能力。Text-level使得全局的边权重必须成为可训练的参数，而MPM中的另一个可训练的参数<span class="math inline">\(\eta_{n}\)</span>实质上与GCN中结合<span class="math inline">\(I\)</span>的拉普拉斯矩阵<span class="math inline">\(L\)</span>是一致的。</p>
<h1 id="texting">TextING</h1>
<p>Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks<br />
上文的两个模型以及后续的BertGCN都是transductive，而本文的TextING则是inducive模型。论文发表在ACL2020上，也就是博客标题。 <img src="/images/pd8/4.png" title="TextING" /> 模型针对每篇文档构建一个图，以词共现作为边节点，借助滑窗（size 3）构建图。节点嵌入用Glove进行初始化。 模型的图神经网络模块使用了Gated Graph Neural Networks(GGNN)和图注意力(GAT)。 <span class="math display">\[
\begin{aligned}
\mathbf{a}^{t} &amp;=\mathbf{A h}^{t-1} \mathbf{W}_{a} \\
\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z} \mathbf{a}^{t}+\mathbf{U}_{z} \mathbf{h}^{t-1}+\mathbf{b}_{z}\right) \\
\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r} \mathbf{a}^{t}+\mathbf{U}_{r} \mathbf{h}^{t-1}+\mathbf{b}_{r}\right) \\
\tilde{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W}_{h} \mathbf{a}^{t}+\mathbf{U}_{h}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)+\mathbf{b}_{h}\right) \\
\mathbf{h}^{t} &amp;=\tilde{\mathbf{h}}^{t} \odot \mathbf{z}^{t}+\mathbf{h}^{t-1} \odot\left(1-\mathbf{z}^{t}\right)
\end{aligned}
\]</span></p>
<p>上式中<span class="math inline">\(h\)</span>表示节点embedding，<span class="math inline">\(a\)</span>代表接受的信息，<span class="math inline">\(z\)</span>和<span class="math inline">\(r\)</span>分别代表更新和遗忘。而后将节点借助readout模块输出为graph-level的embedding： <span class="math display">\[
\begin{array}{l}
\mathbf{h}_{v}=\sigma\left(f_{1}\left(\mathbf{h}_{v}^{t}\right)\right) \odot \tanh \left(f_{2}\left(\mathbf{h}_{v}^{t}\right)\right) \\
\mathbf{h}_{\mathcal{G}}=\frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \mathbf{h}_{v}+\text { Maxpooling }\left(\mathbf{h}_{1} \ldots \mathbf{h}_{\mathcal{V}}\right)
\end{array}
\]</span></p>
<p>上文提到的两个模型中GNN并没有attention模块，这是由于TextGCN的PMI、TF-IDF信息会损失，另一方面Text-level GNN全局的边权重也不应该引入文本图中的attention机制进行更新。</p>
<p>将上文的三个模型进行简单的对比，可以隐约感到存在一个图的构建与图神经网络之间的trade-off。前两个模型的图构建过程都嵌入了大量的信息（先验信息、全局信息），而他们的图神经网络都非常简单。事实上我曾在TextGCN上做过一些实验，尝试把GAT融入到信息传播过程中，发现准确率会有明显下降。而TextING的构图过程中的信息仅是词节点的共现所包含的上下文信息和结构信息，因此它可以接受更复杂的信息传播和聚合过程。这也使得TextING可以面对新词和新输入的文本直接进行分类。</p>
<h1 id="bertgcn">BertGCN</h1>
<p>BertGCN: Transductive Text Classification by Combining GCN and BERT<br />
BertGCN是由香侬科技提出，发表在ACL2021上的文章，也是目前文本分类的SOTA模型。不过这篇文章的贡献主要是工程上的。另外，不妨排列组合一下将Bert与TextING结合，应该能取得更好的结果XD（虽然在R8上的结果已经非常接近100了）。<br />
回顾TextGCN，模型中图节点初始化用的是one-hot向量，而BertGCN则是用Bert进行embedding的初始化，另外将Bert与GNN两个模块进行联合训练，取得了很好的表现。</p>
<p>两者的结合存在两个问题，一是难收敛：BERT与GCN处理数据的方式不同、模型大小不同；二是GCN是在整个图上运算，而BERT过大的模型无法一次全部加载图中所有结点，这就给BertGCN的训练带来阻碍。 针对第一个问题，模型使用了Interpolating损失。 <span class="math display">\[
Z=\lambda Z_{\mathrm{GCN}}+(1-\lambda) Z_{\mathrm{BERT}}, Z_{\mathrm{BERT}}=\operatorname{softmax}(W X)
\]</span> 当<span class="math inline">\(\lambda=1\)</span>时，BERT模块没有更新；当<span class="math inline">\(\lambda=0\)</span>时，GCN模块没有更新；当<span class="math inline">\(\lambda \in (0,1)\)</span>时，两个模块都能得到更新，并且通过调节<span class="math inline">\(\lambda\)</span>实现BertGCN整体模块的快速收敛。<br />
对于无法整图训练这一问题，BertGNN提出了一个Memory Bank用于保存所有节点特征，每次从中加载batch进行训练并更新，其他保持不变。将整个语料库中的文档特征分批更新，为了防止异步更新带来的不一致性，模型在训练Bert模型时采用了小学习率。</p>
<p><img src="/images/pd8/1.png" title="Results: BertGCN" /></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/28/gcn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/28/gcn/" itemprop="url">就叫“深入浅出图卷积(GCN)”吧</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-28T18:35:25+08:00">
                2021-11-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Preliminary-AI/" itemprop="url" rel="index">
                    <span itemprop="name">Preliminary AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>图卷积网络作为图神经网络之一，具有非常广泛的应用。因此网上也有非常多的关于GCN的介绍，但是各种博客看的多了搞得我脑壳嗡嗡的，而刚好最近的一个工作涉及到GCN的内核，因此借助这篇博客整理对GCN进行整理。</p>
<p>在介绍GCN先介绍几篇文献：<br />
Semi-Supervised Classification with Graph Convolutional Networks-首次提出GCN的论文（ICLR2017)<br />
Data Analytics on Graphs-图机器学习的教材<br />
<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/54504471" class="uri">https://www.zhihu.com/question/54504471</a>-介绍GCN的博客</p>
<h2 id="gnn-message-passing">GNN &amp; Message Passing</h2>
<p>GNN作为一种graph embedding的手段，可以借助节点特征的message passing提取图结构信息 <span class="math display">\[\begin{aligned} \mathbf{h}_{u}^{(k+1)} &amp;=\operatorname{UPDATE}^{(k)}\left(\mathbf{h}_{u}^{(k)}, \text { AGGREGATE }^{(k)}\left(\left\{\mathbf{h}_{v}^{(k)}, \forall v \in \mathcal{N}(u)\right\}\right)\right) \\ &amp;=\operatorname{UPDATE}^{(k)}\left(\mathbf{h}_{u}^{(k)}, \mathbf{m}_{\mathcal{N}(u)}^{(k)}\right) \end{aligned}\]</span></p>
<p><img src="/images/gcn/1.png" title="GNN Framework" /> GNN进行k轮迭代，每轮包括一个聚合（aggregate）和更新（update）操作。聚合来获取邻节点的信息，而后更新节点的自身特征。这种多轮message passaging的机制使得图的结构信息以及邻节点特征被提取到节点的特征中。广义上来说，GCN也是GNN中的一种。GCN的message passaging非常的简单。从下式（最基础形式的GCN）可以看出，GCN借助边信息对节点信息进行聚合。 <span class="math display">\[
f\left(H^{(l)}, A\right)=\sigma\left(A H^{(l)} W^{(l)}\right)
\]</span></p>
<h2 id="gnn-cnn">GNN &amp; CNN</h2>
<p>对于图像来说，nxn的卷积核可以作为图像中的特征提取器（为了防止图和图像这两个词的太过接近导致可能出现的问题，下文提到图时将用graph）。但是这种卷积操作无法直接用在Graph上，为什么？<br />
由于图像与graph的数据特性和卷积操作的特性。首先，图像具有局部平移不变性(local translational invariance)，使得卷积核能够对图像矩阵进行扫描卷积；而graph作为非欧空间的数据 (Non Euclidean Structure)，每个节点邻接点的各异导致传统CNN操作无法应用。第二，卷积核是参数共享的，且可以实现层次化特征提取-卷积层可以在前一层的基础上提取更高阶的特征；而graph的层数加深则是使节点获取更广的感受野。</p>
<h2 id="spectrum">Spectrum</h2>
<p>参考<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/120311352" class="uri">https://zhuanlan.zhihu.com/p/120311352</a><br />
在空间域上的图卷积碰壁并不意味着在图上没法进行操作，我们可以从频域中进行分析。</p>
<h3 id="图的拉普拉斯矩阵">图的拉普拉斯矩阵</h3>
<p>首先定义几个概念：<br />
在图上最基本的拉普拉斯矩阵Laplacian matrix为： <span class="math display">\[
\mathbf{L}=\mathbf{D}-\mathbf{A}
\]</span> 其中<span class="math inline">\(\mathbf{D}\)</span>为度矩阵，<span class="math inline">\(\mathbf{A}\)</span>为邻接矩阵。拉普拉斯矩阵有一些基本的性质：对称 (<span class="math inline">\(\mathbf{L}^{T}=\mathbf{L}\)</span>)；半正定 (<span class="math inline">\(\mathbf{x}^{\top} \mathbf{L} \mathbf{x} \geq 0, \forall \mathbf{x} \in \mathbb{R}^{|\mathcal{V}|}\)</span>)，这也意味着拉普拉斯矩阵的特征值都是非负的：<span class="math inline">\(0=\lambda_{|\mathcal{V}|} \leq \lambda_{|\mathcal{V}|-1} \leq \ldots \leq \lambda_{1}\)</span>。 <span class="math display">\[
\begin{aligned}
\mathbf{x}^{\top} \mathbf{L} \mathbf{x} &amp;=\frac{1}{2} \sum_{u \in \mathcal{V}} \sum_{v \in \mathcal{V}} \mathbf{A}[u, v](\mathbf{x}[u]-\mathbf{x}[v])^{2} \\
&amp;=\sum_{(u, v) \in \mathcal{E}}(\mathbf{x}[u]-\mathbf{x}[v])^{2}
\end{aligned}
\]</span> 另外，对称规范化拉普拉斯矩阵symmetric normalized Laplacian定义如下，这是GCN相关工作中比较常用的。 <span class="math display">\[
\mathbf{L}_{\mathrm{sym}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}}
\]</span></p>
<h3 id="拉普拉斯算子">拉普拉斯算子</h3>
<p>接下来我们来一步步理解为什么要这样定义图的拉普拉斯矩阵。对于空间中的任意函数<span class="math inline">\(f\)</span>来说， <span class="math display">\[
\Delta f=\nabla^{2} f=\nabla \cdot \nabla f =\sum_{i=1}^{n} \frac{\partial^{2} f}{\partial x_{i}^{2}}
\]</span> 拉普拉斯算子 (Laplacian)是欧式空间中的函数梯度的散度 (Divergence)对应的微分算子。在n维空间中计算的是函数各个维度二阶偏导的和。在二维空间中，可以<strong>近似</strong>为差分的计算 <span class="math display">\[
\begin{aligned}
\Delta f(x, y) &amp;=\frac{\partial^{2} f}{\partial x^{2}}+\frac{\partial^{2} f}{\partial y^{2}} \\
&amp;=[f(x+1, y)+f(x-1, y))-2 f(x, y)]+[f(x, y+1)+f(x, y-1))-2 f(x, y)] \\
&amp;=f(x+1, y)+f(x-1, y))+f(x, y+1)+f(x, y-1))-4 f(x, y)
\end{aligned}
\]</span> 上式事实上就是在图像上作用拉普拉斯卷积核 <span class="math display">\[
\begin{array}{|r|r|r|}
\hline 0 &amp; 1 &amp; 0 \\
\hline 1 &amp; -4 &amp; 1 \\
\hline 0 &amp; 1 &amp; 0 \\
\hline
\end{array}
\]</span> 因此拉普拉斯算子可以理解为——在所有自由度上进行微小变化后所获得的增益。<br />
而将其推广到有N节点的graph上时，<span class="math inline">\(f\)</span>维度最高为N，<span class="math inline">\(f=\left(f_{1}, \ldots, f_{N}\right)\)</span>。其中<span class="math inline">\(f_{i}\)</span> 表示函数<span class="math inline">\(f\)</span>在网络图中节点i处的函数值, 类比<span class="math inline">\(f(x, y)\)</span>为函数<span class="math inline">\(f\)</span>在 <span class="math inline">\((\mathrm{x}, \mathrm{y})\)</span>的函数值。<br />
因此当拉普拉斯算子作用在加权graph（<strong>边权重为</strong><span class="math inline">\(w_{i j}\)</span>）上时，借助差分近似后有： <span class="math display">\[
\begin{aligned}
\Delta \boldsymbol{f}_{i} &amp;=\sum_{j \in N_{i}} \frac{\partial f_{i}}{\partial j^{2}} \\
&amp; \approx \sum_{j} w_{i j}\left(f_{i}-f_{j}\right) \\
&amp;=\sum_{j} w_{i j}\left(f_{i}-f_{j}\right) \\
&amp;=\left(\sum_{j} w_{i j}\right) f_{i}-\sum_{j} w_{i j} f_{j} \\
&amp;=d_{i} f_{i}-w_{i:} f
\end{aligned}
\]</span> 对于任意<span class="math inline">\(i \in N\)</span>都成立，所以就得到了： <span class="math display">\[
\begin{aligned}
\Delta f=\left(\begin{array}{c}
\Delta f_{1} \\
\vdots \\
\Delta f_{N}
\end{array}\right) &amp;=\left(\begin{array}{cc}
d_{1} f_{1}-w_{1:} f \\
\vdots \\
d_{N} f_{N}-w_{N:} f
\end{array}\right) \\
&amp;=\left(\begin{array}{ccc}
d_{1} &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; d_{N}
\end{array}\right) f-\left(\begin{array}{c}
w_{1:} \\
\vdots \\
w_{N:}
\end{array}\right) f \\
&amp;=\operatorname{diag}\left(d_{i}\right) f-\mathbf{W} f \\
&amp;=(\mathbf{D}-\mathbf{W}) f \\
&amp;=\mathbf{L} f
\end{aligned}
\]</span> 这就意味着，对由图节点特征构成的向量<span class="math inline">\(f\)</span>做拉普拉斯等价于图拉普拉斯矩阵与向量<span class="math inline">\(f\)</span>进行点积。</p>
<h3 id="graph-fourier-transformer">Graph Fourier Transformer</h3>
<p>拉普拉斯矩阵的特征分解 <span class="math display">\[
\mathbf{L} \mathbf{u}_{\mathbf{k}}=\lambda_{k} \mathbf{u}_{\mathbf{k}}
\]</span> 继而进行正交相似对角化后就得到 <span class="math display">\[
\mathbf{L}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{-1}=\mathbf{U}\left(\begin{array}{ccc}
\lambda_{1} &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp; \\
&amp; &amp; \lambda_{n}
\end{array}\right) \mathbf{U^{-1}}=\mathbf{U}\boldsymbol{\Lambda} \mathbf{U}^{T}
\]</span> 其中<span class="math inline">\(\boldsymbol{\Lambda}\)</span> 为特征值构成对角矩阵, <span class="math inline">\(\mathbf{U}\)</span> 为特征向量构成的正交矩阵。</p>
<p>在这里补充一条性质 <span class="math display">\[\Delta e^{-i \omega t} =\frac{\partial^{2} e^{-i \omega t}}{\partial t^{2}}= -\omega^{2} e^{-i \omega t}\]</span> 从广义上来看，这符合特征方程<span class="math inline">\(AV=\lambda V\)</span>的定义，也就是说<span class="math inline">\(e^{-i \omega t}\)</span>是拉普拉斯算子的特征函数</p>
<p>把传统的傅里叶变换以及卷积迁移到Graph上来, 核心工作其实就是把<strong>拉普拉斯算子的特征函数<span class="math inline">\(e^{-i \omega t}\)</span></strong> 变为Graph对应的<strong>拉普拉斯矩阵的特征向量</strong>。 傅立叶变化是信号函数<span class="math inline">\(f(t)\)</span>与基函数<span class="math inline">\(e^{-i \omega t}\)</span>的内积 <span class="math display">\[
\mathcal{F}_{T}(\omega)=\int_{-\infty}^{+\infty} f(t) e^{-i \omega t} d t
\]</span> 因此对于Graph我们就可以定义 <span class="math display">\[
F\left(\lambda_{l}\right)=\hat{f}\left(\lambda_{l}\right)=\sum_{i=1}^{N} f(i) u_{l}^{*}(i)
\]</span> 其中<span class="math inline">\(f\)</span>是graph上的N维向量，<span class="math inline">\(f(i)\)</span>对应于graph上的第i个顶点，<span class="math inline">\(u_{l}(i)\)</span>表示第l个特征向量的第i个分量。特征值<span class="math inline">\(\lambda_l\)</span>（频率）下的<span class="math inline">\(f\)</span>的graph傅立叶变换就是与<span class="math inline">\(\lambda_l\)</span>对应的特征向量<span class="math inline">\(u_{l}\)</span>进行内积运算。将上式推广到矩阵形式，就有 <span class="math display">\[
\left(\begin{array}{c}
\hat{f}\left(\lambda_{1}\right) \\
\hat{f}\left(\lambda_{2}\right) \\
\vdots \\
\hat{f}\left(\lambda_{N}\right)
\end{array}\right)=\left(\begin{array}{cccc}
u_{1}(1) &amp; u_{1}(2) &amp; \ldots &amp; u_{1}(N) \\
u_{2}(1) &amp; u_{2}(2) &amp; \ldots &amp; u_{2}(N) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
u_{N}(1) &amp; u_{N}(2) &amp; \ldots &amp; u_{N}(N)
\end{array}\right)\left(\begin{array}{c}
f(1) \\
f(2) \\
\vdots \\
f(N)
\end{array}\right)
\]</span> <span class="math display">\[
\hat{f} = U^T f
\]</span></p>
<p>图上的傅立叶逆变换类似于传统傅立叶逆变换的对频率求积分： <span class="math display">\[
f(i)=\sum_{l=1}^{N} \hat{f}\left(\lambda_{l}\right) u_{l}(i)
\]</span> <span class="math display">\[f=U \hat{f}\]</span></p>
<h2 id="gcn">GCN</h2>
<p>上文从百草园讲到三味书屋，终于要讲到了本文的主角-GCN。在上面graph傅立叶变换的基础上，我们可以将卷积推广到graph上。 <span class="math display">\[
f * h=\mathcal{F}^{-1}[\hat{f}(\omega) \hat{h}(\omega)]=\frac{1}{2 \Pi} \int \hat{f}(\omega) \hat{h}(\omega) e^{i \omega t} d \omega
\]</span> 类比到graph上，函数<span class="math inline">\(f\)</span>与卷积核<span class="math inline">\(h\)</span>在graph上的卷积 <span class="math display">\[
(f * h)_{G}=U\left(\begin{array}{lll}
\hat{h}\left(\lambda_{1}\right) &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp; \hat{h}\left(\lambda_{n}\right)
\end{array}\right) U^{T} f
\]</span> 式中<span class="math inline">\(\hat{h}\left(\lambda_{l}\right)=\sum_{i=1}^{N} h(i) u_{l}^{*}(i)\)</span>是根据需要设计的卷积核<span class="math inline">\(h\)</span>在graph上的傅立叶变换。 图表示学习中用的定义为 <span class="math display">\[
(f * h)_{G}=U\left(\left(U^{T} h\right) \odot\left(U^{T} f\right)\right)
\]</span> <span class="math inline">\(\odot\)</span> 表示Hadamard product，对两个维度相同的向量、矩阵进行对应位置的逐元素乘积。</p>
<p>将神经网络与graph卷积结合，只需令卷积核变为可学习的参数，也就得到 <span class="math display">\[
y_{\text {output }}=\sigma\left(U \left(\begin{array}{lll}
\theta_{1} &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp; \theta_{n}
\end{array}\right) U^{T} x\right)
\]</span> 我们将卷积核记为<span class="math inline">\(g(\Lambda)\)</span> (<span class="math inline">\(\Lambda\)</span>就是大写的<span class="math inline">\(\lambda\)</span>)。<br />
这种图卷积被称为<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6203">第一代图卷积</a>，但这类图卷积计算开销非常大，且卷积核有n个参数，这种图卷积很难处理工业级的数据。 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1606.09375.pdf">第二代图卷积</a>使用了polynomial filter <span class="math display">\[
g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} \Lambda^{k}=\left(\begin{array}{ccc}
\sum_{j=0}^{K} \alpha_{j} \lambda_{1}^{j} &amp; &amp; \\
&amp; \ddots &amp; \\
&amp; &amp; \\
&amp; &amp; &amp; \sum_{j=0}^{K} \alpha_{j} \lambda_{n}^{j}
\end{array}\right)
\]</span> 其中<span class="math inline">\(\theta \in \mathbb{R}^{K}\)</span>是多项式系数向量。进而可以推出 <span class="math display">\[
U \sum_{j=0}^{K} \alpha_{j} \Lambda^{j} U^{T}=\sum_{j=0}^{K} \alpha_{j} U \Lambda^{j} U^{T}=\sum_{j=0}^{K} \alpha_{j} L^{j}
\]</span> <span class="math display">\[y_{\text {output }}=\sigma\left(\sum_{j=0}^{K-1} \alpha_{j} L^{j} x\right)\]</span> 此时，我们计算图卷积运算就不需要再乘上特征向量矩阵<span class="math inline">\(U\)</span>，而是直接使用拉普拉斯矩阵<span class="math inline">\(L\)</span>的k 次方（K远小于n），这样就避免了进行特征分解。而我们可以事先计算好<span class="math inline">\(L^K\)</span> ，这样就只需要计算矩阵相乘。 此外，高阶拉普拉斯可以用切比雪夫展开来近似，因此卷积核还可以用切比雪夫多项式来表示 <span class="math display">\[
g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} T_{k}(\tilde{\Lambda})
\]</span> 而将切比雪夫多项式的阶数限制为2的时候就得到在下游任务中常用的图卷积公式： <span class="math display">\[
H^{(l+1)}=\sigma\left(\widetilde{D}^{-\frac{1}{2}} \widetilde{A} \widetilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)
\]</span> 上式中，<span class="math inline">\(\widetilde{A}=A+I, \mathrm{~A}\)</span>为邻接矩阵, <span class="math inline">\(I\)</span>为单位矩阵, 所以<span class="math inline">\(\widetilde{A}\)</span> 为添加自连接的邻接矩阵;<span class="math inline">\(W^{(l)}\)</span> 为神经网络第<span class="math inline">\(l\)</span>层的权重矩阵; <span class="math inline">\(\sigma(\cdot)\)</span>是激活函数</p>
<h2 id="gcn-self-attention">GCN &amp; Self-attention</h2>
<p>上面从频域分析graph convolution，当我们从空间域上分析得到的卷积公式时，可以看出它仍是一种message passing机制。 <span class="math display">\[
A  H^{(l)}  W^{(l)}
\]</span> 上式可以分为两个步骤，首先是<span class="math inline">\(H\)</span>与参数矩阵<span class="math inline">\(W\)</span>做一个线性映射，而后与邻节点及其边信息进行聚合汇总。这一框架在某种意义上说与self- attention是非常类似的。</p>
<p>self- attention包含query、key、value；其中输入的query与每个key计算相似度，而后得到一个注意力系数<span class="math inline">\(\alpha\)</span>，再由注意力系数对value进行加权求和输出最终的结果。抛开邻节点后，这两者的计算机制可以说非常类似，都可以被囊括在message passing这一框架下，而self-attention也可以理解为在完成图（所有节点都相连）上的GCN。而Transformer以及GNN在NLP中的广泛应用也从某种意义上说明了两者之间存在某种相似性。</p>
<h2 id="gcn应用">GCN应用</h2>
<h3 id="textgcn">TextGCN</h3>
<p><img src="/images/gcn/2.png" title="TextGCN" /> 论文<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.05679v3">Graph Convolutional Networks for Text Classification</a>所构建的TextGCN将GCN用于文本分类中，在电影评价、新闻等数据集上都取得了不错的表现。<br />
如上图所示，模型将语料库中的每一篇文档和语料库中词作为节点，联合构建了一个异构图，再借助GCN进行特征传播，得到每个文档节点的embedding后进行softmax分类。<br />
文章中节点都使用one-hot vector进行初始化，而文档-词边、词-词边分别用TF-IDF、PMI赋以不同的权重，而最终得到的分类准确率比传统的CNN、LSTM等网络效果要高，足以证明GCN在NLP任务中的潜力。此外，后续用BERT进行节点初始化的BERTGCN也是目前的文本分类的SOTA模型。</p>
<h3 id="st-gcn">ST-GCN</h3>
<p><img src="/images/gcn/4.png" title="ST-GCN" /> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1801.07455.pdf">ST-GCN</a>可以说是GCN在骨骼行为识别里面的开山之作。</p>
<h3 id="sgc">SGC</h3>
<p><img src="/images/gcn/3.png" title="Simplifying Graph Convolutional Networks (SGC)" /> 作者将图卷积层中的激活函数去掉，得到了SGC在许多NLP任务上更优的结果，且模型速度有了极大的提升。</p>
<h2 id="再回首">再回首</h2>
<p>回顾一下上文的内容，首先GCN是GNN的一种，从公式上看聚合函数采用的是图的拉普拉斯矩阵。当我们从频域上分析图的拉普拉斯矩阵及其特征分解之后可以发现，拉普拉斯矩阵的特征向量可以作为傅里叶变换的基、特征值表示频率，从而就可以定义图上的傅立叶变换，进而扩展到卷积操作。而将图卷积与神经网络结合后，借助多项式优化后就得到了现在常用的卷积公式。而从空间域中看，图卷积本质上也就是一种信息传播机制，借助边的权重信息对邻节点的特征做限制后传播、聚合、更新节点原本的特征。</p>
<p>在频域分析过程中我们可以得到，在由Graph确定的<span class="math inline">\(n\)</span>维空间中，越小的特征值 <span class="math inline">\(\lambda_{l}\)</span> 表明：拉普拉斯矩阵 <span class="math inline">\(L\)</span> 其所对应的基 <span class="math inline">\(u_{l}\)</span> 上的分量、&quot;信息&quot;越少、高频部分。所以图卷积有时候也被认知为是图上的高斯平滑，一种滤波的过程，这也导出了图卷积中的一大问题：over-smooshing。当图卷积层数加深时，图上节点自身的特征会因为不断的传播后导致自身的特征消失，所有节点的特征会越来越接近，进而使得下游任务准确率下降。</p>
<h3 id="gcn-vs-cnn">GCN vs CNN</h3>
<p>GCN可以退化为CNN...TODO</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/23/pd7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/23/pd7/" itemprop="url">Two are Better than One — Joint Entity and Relation Extraction with Table-Sequence Encoders [EMNLP2020]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-23T22:09:00+08:00">
                2021-11-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="前言">前言</h1>
<p>最近似乎吃到了一个瓜</p>
<blockquote>
<p>如何看待EMNLP'21的文章涉嫌抄袭EMNLP'20上的文章?<br />
本人在阅读EMNLP2021的文章时，偶然发现一篇名为Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model的文章，这篇文章的内容与EMNLP2020上的一篇文章Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders高度相似。</p>
</blockquote>
<p>先不谈到底有没有抄袭事件，我们来看一看20年和21这两篇文章的所做的工作</p>
<h1 id="emnlp20">EMNLP'20</h1>
<h2 id="intro">Intro</h2>
<h1 id="emnlp21">EMNLP'21</h1>
<h1 id="p.s.">p.s.</h1>
<p>最近尝试复现旷视的<a href="/2021/10/26/pd5/" title="ML-GCN">ML-GCN</a>也遇到了大无语事件，其一是用了好几个版本的Pytorch都无法较好的收敛，看网友的一些讨论说居然要用19年特定的0.3.1版本；其二就是ML-GCN中最主要的模块GCN对模型的准确率没有任何贡献，去掉该模块反而有更好的效果。😅<br />
怪不得会有人用“魔改”这种略带侮辱性的词来形容AI的科研。希望能有越来越多的人做有意义的科研，也希望“独角兽”们和“master”们把挣钱和科研分开。“挣钱嘛，不寒碜“ （让子弹飞赶紧申遗），但既然想挣钱就别往科研里钻，整些无意义灌水甚至是剽窃他人的成果。<br />
当然，抛开环境现状不谈来批判某些个体的行为似乎有些耍流氓。还记得大二时候导师跟我和zb说要做一个一身正气的科研工作者。（虽然现在也说不准以后是进学术界还是工业界）国家快速发展总会留下一些弊病，希望能从高校老师和学生开始，逐渐构建一片净土，让热爱学术的人能够坚定不移的走下去。至少以后学术造假等事件曝光时，我们看到的主角不应该是中国人的名字。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/18/svgd-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/18/svgd-2/" itemprop="url">SVGD补充</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-18T10:03:58+08:00">
                2021-11-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>关于<a href="/2021/11/09/svgd/" title="SVGD">SVGD</a>的内容，这一篇博客中大致做了简单的介绍，包括从上到下的数学推导和简单的逻辑链。但整个贝叶斯推断背后还有许多思想，且SVGD背后也隐藏的一些思维过程。这篇Blog将作为SVGD相关内容的补充以及涉及到的知识体系的简要归纳。</p>
<h1 id="svgd实验">SVGD实验</h1>
<h2 id="拟合高斯分布">拟合高斯分布</h2>
<h2 id="贝叶斯神经网络">贝叶斯神经网络</h2>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/16/kernel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/16/kernel/" itemprop="url">Kernels and Hilbert Space</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-16T19:43:16+08:00">
                2021-11-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Preliminary-AI/" itemprop="url" rel="index">
                    <span itemprop="name">Preliminary AI</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Reference：<br />
‘Introduction to Hilbert Spaces with Application.’<br />
‘Introduction to RKHS, and some simple kernel algorithms.’</p>
<p>Since Kernel trick is one of the core methods in SVM and SVGD also involves expertise related to RKHS. I looked up several books on Kernel method, trying to get a systematic understanding of Kernel and Hilbert space. This blog can also be regarded as a summary and summary of the book ‘Introduction to Hilbert Spaces with Application ’.</p>
<h1 id="introduction">Introduction</h1>
<p><img src="/images/kernel/1.png" title="XOR example" /> <img src="/images/kernel/2.png" title="Document classification example" /></p>
<h2 id="kernel">Kernel</h2>
<p>Definition: Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[
k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}
\]</span></p>
<h1 id="normed-vector-spaces">Normed Vector Spaces</h1>
<p>First, the space defined in mathematics can be divided from simple to complex as:</p>
<ul>
<li><p>Vector Space<br />
a nonempty set <span class="math inline">\(E\)</span> with two operations: <em>addition</em> and <em>multiplication by scalars</em>.<br />
e.g. <span class="math inline">\(\mathbb{R}^{N}\)</span> <span class="math inline">\(\mathbb{C}^{N}\)</span></p></li>
<li><p>Normed Space<br />
norm is an abstract generalization of the length of a vector:<br />
function <span class="math inline">\(x \mapsto\|x\|\)</span> from a vector space <span class="math inline">\(E\)</span> into <span class="math inline">\(\mathbb{R}\)</span></p></li>
<li><p>Banach Space: complete normed space<br />
A normed space is complete if and only if every absolutely convergent series converges. (The contents of Cauchy sequence and Cauchy series are put in the appendix)<br />
Actually, Banach space introduces the concept of Limits</p></li>
<li><p>Inner Product Spaces<br />
The space that defines the <a href="#jump">inner product</a>.</p></li>
<li><p>Hilbert Spaces: A complete inner product space</p></li>
</ul>
<h1 id="hilbert-spaces">Hilbert Spaces</h1>
<h1 id="appendix">Appendix</h1>
<h2 id="cauchy-sequence-and-cauchy-series">Cauchy sequence and Cauchy series</h2>
<p>Definition of <strong><em>Cauchy sequence</em></strong>. A sequence <span class="math inline">\(\left\{f_{n}\right\}_{n=1}^{\infty}\)</span> of elements in a normed space <span class="math inline">\(\mathcal{H}\)</span> is said to be a Cauchy sequence if for every <span class="math inline">\(\epsilon&gt;0\)</span>, there exists <span class="math inline">\(N=N(\varepsilon) \in \mathbb{N}\)</span>, such that for all <span class="math inline">\(n, m \geq N,\left\|f_{n}-f_{m}\right\|_{\mathcal{H}}&lt;\epsilon\)</span></p>
<h2 id="inner-product">Inner product</h2>
<p><span id="jump"> </span> Definition of <strong><em>Inner product</em></strong>. Let <span class="math inline">\(\mathcal{H}\)</span> be a vector space over <span class="math inline">\(\mathbb{R}\)</span>. A function <span class="math inline">\(\langle\cdot, \cdot\rangle_{\mathcal{H}}: \mathcal{H} \times \mathcal{H} \rightarrow \mathbb{R}\)</span> is said to be an inner product on <span class="math inline">\(\mathcal{H}\)</span> if:</p>
<ul>
<li><p><span class="math inline">\(\left\langle\alpha_{1} f_{1}+\alpha_{2} f_{2}, g\right\rangle_{\mathcal{H}}=\alpha_{1}\left\langle f_{1}, g\right\rangle_{\mathcal{H}}+\alpha_{2}\left\langle f_{2}, g\right\rangle_{\mathcal{H}}\)</span></p></li>
<li><p><span class="math inline">\(\langle f, g\rangle_{\mathcal{H}}=\langle g, f\rangle_{\mathcal{H}}{ }^{1}\)</span></p></li>
<li><p><span class="math inline">\(\langle f, f\rangle_{\mathcal{H}} \geq 0\)</span> and <span class="math inline">\(\langle f, f\rangle_{\mathcal{H}}=0\)</span> if and only if <span class="math inline">\(f=0\)</span>.</p></li>
</ul>
<p>the inner product between matrices <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(B \in\)</span> <span class="math inline">\(\mathbb{R}^{m \times n}\)</span> is <span class="math display">\[
\langle A, B\rangle=\operatorname{trace}\left(A^{\top} B\right)
\]</span></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/11/09/svgd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/11/09/svgd/" itemprop="url">Stein variational gradient descent (NIPS2018)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-11-09T20:28:58+08:00">
                2021-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="intro">Intro</h1>
<p>这一工作是清华大学liu qiang老师提出的，相关论文从2016年开始也一直在更新，分别发表在NIPS、ICLR等顶会上。<br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.07520">Stein Variational Gradient Descent as Gradient Flow</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.11693">Stein Variational Gradient Descent as Moment Matching</a><br />
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1608.04471.pdf">Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a></p>
<p>从定义上来说，SVGD是一种确定性的采样算法，用一组粒子来近似给定的分布。基于这两点，它和MCMC以及VI都有共通之处。但SVGD即保证了在大量数据下的计算速度，也比变分推断具有更高的准确性。</p>
<p>从整体上来看，这一工作通过引入Stein discrepancy来度量两个分布之间的距离，再借助RKHS使其容易计算，最后借助gradient descent进行优化。因此下文也就从这三部分一一介绍。</p>
<h1 id="background">Background</h1>
<h2 id="steins-method">Stein's method</h2>
<p>首先我们需要引入几个定义：</p>
<ul>
<li><p><em>Stein score function</em> <span class="math display">\[
\boldsymbol{s}_{p}=\nabla_{x} \log p(x)=\frac{\nabla_{x} p(x)}{p(x)}
\]</span> 这一函数被称为<span class="math inline">\(q(x)\)</span>的Stein score function</p></li>
<li><p><em>Stein class</em><br />
当函数<span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}\)</span>满足下式时则称其在stein class中 <span class="math display">\[
\int_{x \in \mathcal{X}} \nabla_{x}(f(x) p(x)) d x=0
\]</span> 其中<span class="math inline">\(\mathcal{X}\)</span> 是<span class="math inline">\(\mathbb{R}^{d}\)</span>下的子集，而<span class="math inline">\(p(x)\)</span>则是在<span class="math inline">\(\mathcal{X}\)</span> 下连续可微的分布。</p></li>
<li><p><em>Stein's operator</em>：作用在<span class="math inline">\(p\)</span>上的线性操作 <span class="math display">\[
\mathcal{A}_{p} f(x)=\boldsymbol{s}_{p}(x) f(x)+\nabla_{x} f(x)
\]</span> 其中<span class="math inline">\(s_{p}\)</span>和<span class="math inline">\(\mathcal{A}_{p} f\)</span> 都是<span class="math inline">\(d \times 1\)</span> 函数(mapping from <span class="math inline">\(\mathcal{X}\)</span> to <span class="math inline">\(\mathbb{R}^{d}.\)</span>)</p></li>
</ul>
<p>有了以上三个定义后，我们可以尝试得到stein discrepancy。首先作为一个度量手段，必然需要满足一些条件。<br />
当且仅当 <span class="math display">\[
\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]=0   \qquad   (1) 
\]</span> <span class="math inline">\(p(x)\)</span> 和 <span class="math inline">\(q(x)\)</span>是相等的。而当两个分布<span class="math inline">\(p=q\)</span>时又被称为stein identity。<br />
借助 (1) 式，我们可以定义Stein discrepancy来度量两个分布<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>之间的差异： <span class="math display">\[
\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]\right)^{2}
\]</span> 借助之前定义的stein operator，也可以把上式写为 <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\mathcal{A}_{q} f(x)\right]\right)^{2}\]</span></p>
<p><span class="math inline">\(\mathcal{F}\)</span>是一系列连续可微的且满足<span class="math inline">\(\mathbb{S}(p, q)\)</span>不为0(<span class="math inline">\(p \neq q\)</span>时)函数集合。当<span class="math inline">\(p \neq q\)</span>时，<span class="math inline">\(\mathbb{S}(p,q)&gt;0\)</span>，而<span class="math inline">\(max\)</span>则是因为我们希望距离尽可能明显。</p>
<p><span class="math inline">\(\mathbb{S}(p, q)\)</span>并没有被广泛应用在机器学习中，因为其计算和优化的复杂性: <span class="math inline">\(q(x)=f(x) / Z\)</span> 而<span class="math inline">\(Z=\int f(x) d x\)</span>的计算往往设计高维积分。<br />
但是论文提出了将函数<span class="math inline">\(\mathcal{F}\)</span>用核函数代替时，会得到易于计算的Stein discrepancy <span class="math inline">\(\mathbb{S}(p, q)\)</span>。具体而言，我们令<span class="math inline">\(\mathcal{F}\)</span>来源于希尔伯特再生核空间的一个球 (reproducing kernel Hilbert space (RKHS))。</p>
<h2 id="kernelized-stein-discrepancy">Kernelized Stein Discrepancy</h2>
<p>对于映射后的函数，对应的正定核<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>，我们有 <span class="math display">\[
\mathbb{S}(p, q)=\mathbb{E}_{x, x^{\prime} \sim p}\left[u_{q}\left(x, x^{\prime}\right)\right]
\]</span> 其中<span class="math inline">\(x, x^{\prime}\)</span>是<span class="math inline">\(p\)</span>中独立同分布的两个变量，函数<span class="math inline">\(u_{q}\left(x, x^{\prime}\right)\)</span>由<span class="math inline">\(q\)</span>确定，如果展开的话实际上是： <span class="math display">\[u_{q}\left(x, x^{\prime}\right)= \boldsymbol{s}_{q}(x)^{\top} k\left(x, x^{\prime}\right) \boldsymbol{s}_{q}\left(x^{\prime}\right)+\boldsymbol{s}_{q}(x)^{\top} \nabla_{x^{\prime}} k\left(x, x^{\prime}\right)+\nabla_{x} k\left(x, x^{\prime}\right)^{\top} \boldsymbol{s}_{q}\left(x^{\prime}\right)+\operatorname{trace}\left(\nabla_{x, x^{\prime}} k\left(x, x^{\prime}\right)\right)\]</span></p>
<p>当我们从未知分布<span class="math inline">\(p(x)\)</span>采样出一个样本<span class="math inline">\({x_i}\)</span>时，我们可以进行近似计算 <span class="math display">\[\hat{\mathbb{S}}(p, q)=\frac{1}{n(n-1)} \sum_{i \neq j} u_{q}\left(x_{i}, x_{j}\right)\]</span></p>
<p>接下来我们详细介绍上述的过程。</p>
<h3 id="kernels-and-reproducing-kernel-hilbert-spaces">Kernels and Reproducing Kernel Hilbert Spaces</h3>
<a href="/2021/11/16/kernel/" title="Kernel and Hilbert Spaces介绍 (未完待续)">Kernel and Hilbert Spaces介绍 (未完待续)</a>
<p>令<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>为一个正定核，根据Mercer’s theorem我们对其进行谱分解： <span class="math display">\[
k\left(x, x^{\prime}\right)=\sum_{j} \lambda_{j} e_{j}(x) e_{j}\left(x^{\prime}\right)
\]</span> 其中<span class="math inline">\(\left\{e_{j}\right\},\left\{\lambda_{j}\right\}\)</span>分别是正交特征函数和正特征值，满足<span class="math inline">\(\int e_{i}(x) e_{j}(x) d x=\mathbb{I}[i=j]\)</span>, for <span class="math inline">\(\forall i, j\)</span></p>
<p>对于一个正定核，它可以分解为RKHS中特征函数的线性组合（空间中任何一个函数可以用这组基的线性组合来表示）。由一个特定的核函数能产生一个唯一的Hilbert空间，有性质 <span class="math display">\[
f(x)=\langle f, k(\cdot, x)\rangle_{\mathcal{H}}, \quad k\left(x, x^{\prime}\right)=\left\langle k(\cdot, x), k\left(\cdot, x^{\prime}\right)\right\rangle_{\mathcal{H}}
\]</span> 当我们定义 <span class="math inline">\(\mathcal{H}^{d}=\mathcal{H} \times \mathcal{H} \times \cdots \mathcal{H}\)</span> 为 <span class="math inline">\(d\)</span> 维向量函数 <span class="math inline">\(\mathbf{f}=\left\{f_{i}: f_{i} \in \mathcal{H} \quad i=1, \cdots, d\right\}\)</span> 组成的 Hilbert空间, <span class="math inline">\(\mathcal{H}^{d}\)</span> 上的内积定义为 <span class="math inline">\(&lt;\mathbf{f}, \mathbf{g}&gt;_{\mathcal{H}^{d}}=\sum_{i=1}^{d}&lt;f_{i}, g_{i}&gt;_{\mathcal{H}}\)</span> 。如果觉得上述的介绍太过抽象，可以看附录部分关于RKHS的一个<a href="#jump">toy example</a></p>
<h3 id="lemmas">lemmas</h3>
<p><strong>Stein's Identity</strong> ： <span class="math display">\[
\mathbb{E}_{p}\left[\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)\right]=0
\]</span> 证明的话根据<span class="math inline">\(\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)=\nabla_{x}(\boldsymbol{f}(x) p(x)) / p(x)\)</span>和分布积分法则就可以推导出来。</p>
<p>有了上面的引理，我们可以得到 <span class="math display">\[\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)-\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\left(\boldsymbol{s}_{q}(x)-\boldsymbol{s}_{p}(x)\right) \boldsymbol{f}(x)^{\top}\right]\]</span></p>
<p>也就是说<span class="math inline">\(\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]\)</span>是一个由<span class="math inline">\(f(x)\)</span>加权的期望，对于<span class="math inline">\(\left(s_{q}(x)-s_{p}(x)\right)\)</span>的期望。</p>
<h3 id="ksd">KSD</h3>
<p>借助上面的推导以及RKHS的性质，我们可以定义出核空间下的stein discrepancy： <span class="math display">\[
S(p, q)=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right)\right]
\]</span> （这里我们将Kernelized前后的stein discrepancy分别写为<span class="math inline">\(\mathbb{S}(p,q)\)</span>和<span class="math inline">\(S(p, q)\)</span>。另外需要注意后续部分推导是针对stein discrepancy中的期望项）<br />
我们要用一个可采样的分布<span class="math inline">\(q\)</span>拟合分布<span class="math inline">\(p\)</span>，因此我们希望<span class="math inline">\(S(p, q)\)</span>[]中的式子是与<span class="math inline">\(p\)</span>无关的。把式子展开后可以发现 <span class="math display">\[
\begin{aligned}
S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}-s_{p}\right)\right] \\
&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T}\left(k(\mathbf{x}, \mathbf{y}) s_{q}+\nabla_{y} k(\mathbf{x}, \mathbf{y})-k(\mathbf{x}, \mathbf{y}) s_{p}-\nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})\right)\right] \\
&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T} v(\mathbf{x}, \mathbf{y})\right]
\end{aligned}
\]</span> 其中<span class="math inline">\(v(\mathbf{x}, \mathbf{y})=k(\mathbf{x}, \mathbf{y}) s_{q}(\mathbf{y})+\nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})=\mathcal{A}_{q} k_{\mathbf{x}}(\mathbf{y}), k_{\mathbf{x}}(\cdot)=k(\mathbf{x}, \cdot)\)</span></p>
<p>而对于固定的 <span class="math inline">\(\mathbf{y}\)</span>, 容易证明 <span class="math inline">\(v(\cdot, \mathbf{y})\)</span> 是Stein class of <span class="math inline">\(p(\mathbf{x})\)</span>, 即满足 <span class="math inline">\(\int_{\mathbf{x} \in \mathcal{X}} \nabla_{\mathbf{x}}(v(\mathbf{x}, \mathbf{y}) p(\mathbf{x})) d \mathbf{x}=0\)</span> 。因此就可以进一步将上式写开为 <span class="math display">\[
\begin{aligned}
S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})-\left(\nabla_{\mathbf{x}} \ln p(\mathbf{x})\right)^{T} v(\mathbf{x}, \mathbf{y})\right] \\
&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})\right]-\int d \mathbf{x} d \mathbf{y} p(\mathbf{x}) p(\mathbf{y})\left(\nabla_{\mathbf{x}} \ln p(\mathbf{x})\right)^{T} v(\mathbf{x}, \mathbf{y}) \\
&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})\right]+\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\operatorname{tr} \nabla_{\mathbf{x}} v(\mathbf{x}, \mathbf{y})\right]
\end{aligned}
\]</span></p>
<p>其中<span class="math inline">\(\nabla_{\mathbf{x}} v(\mathbf{x}, \mathbf{y})=\nabla_{\mathbf{x}} k(\mathbf{x}, \mathbf{y}) s_{q}(\mathbf{y})^{T}+\nabla_{\mathbf{x}} \nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})\)</span>为一个矩阵。</p>
<p>这样就得到了前文提到了 <span class="math display">\[
S(p, q)=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[u_{q}(\mathbf{x}, \mathbf{y})\right]
\]</span> 其中<span class="math inline">\(u_{q}(\mathbf{x}, \mathbf{y})\)</span>仅与分布<span class="math inline">\(q\)</span>有关。</p>
<p>上述的推导说明了什么？说明引入核方法的可行性。而另一方面，我们要用核方法来加速内积的计算，如何体现？<br />
借助RKHS的对称性和再生性，我们可以将<span class="math inline">\(S(p,q)\)</span>进行变化： <span class="math display">\[
\begin{aligned}
S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)\right] \\
&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)^{T}&lt;k(\mathbf{x}, \cdot), k(\cdot, \mathbf{y})&gt;_{\mathcal{H}}\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)\right] \\
&amp;=\sum_{i=1}^{d}&lt;\mathbb{E}_{\mathbf{x} \sim p}\left[\left(s_{q}^{i}(\mathbf{x})-s_{p}^{i}(\mathbf{x})\right) k(\mathbf{x}, \cdot)\right], \mathbb{E}_{\mathbf{y} \sim p}\left[\left(s_{q}^{i}(\mathbf{y})-s_{p}^{i}(\mathbf{y})\right) k(\cdot \mathbf{y})\right]&gt;_{\mathcal{H}} \\
&amp;=\sum_{i=1}^{d}&lt;\beta_{i}, \beta_{i}&gt;_{\mathcal{H}} \\
&amp;=\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}^{2}
\end{aligned}
\]</span> 其中<span class="math inline">\(\boldsymbol{\beta}(\mathbf{y})\)</span>是一个向量函数 <span class="math display">\[
\boldsymbol{\beta}(\mathbf{y})=\mathbb{E}_{\mathbf{x} \sim p}\left[\mathcal{A}_{q} k_{\mathbf{y}}(\mathbf{x})\right]=\mathbb{E}_{\mathbf{x} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right) k_{\mathbf{y}}(\mathbf{x})\right]
\]</span></p>
<p>在这里再展示一下最开始的stein discrepancy <span class="math display">\[
\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\mathcal{A}_{q} f(x)\right]\right)^{2}
\]</span> 对于任意向量 <span class="math inline">\(\mathbf{f} \in \mathcal{H}^{d}\)</span> 与 <span class="math inline">\(\boldsymbol{\beta}\)</span> 的内积为 <span class="math display">\[
\begin{aligned}
&lt;\mathbf{f}, \boldsymbol{\beta}&gt;_{\mathcal{H}^{d}} &amp;=\sum_{i=1}^{d}&lt;f_{i}, \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x}) k(\mathbf{x}, \cdot)+\nabla_{x_{i}} k(\mathbf{x}, \cdot)\right]&gt;_{\mathcal{H}} \\
&amp;=\sum_{i=1}^{d} \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x})&lt;f_{i}, k(\mathbf{x}, \cdot)&gt;_{\mathcal{H}}+&lt;f_{i}, \nabla_{x_{i}} k(\mathbf{x}, \cdot)&gt;_{\mathcal{H}}\right] \\
&amp;=\sum_{i=1}^{d} \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x}) f_{i}(\mathbf{x})+\nabla_{x_{i}} f_{i}(\mathbf{x})\right] \\
&amp;=\mathbb{E}_{\mathbf{x} \sim p}\left[\operatorname{tr}\left(\mathcal{A}_{q} \mathbf{f}(\mathbf{x})\right)\right] \leq\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}} (因为任意两个向量内积小于它与自身的内积)
\end{aligned}
\]</span> 因此我们有 <span class="math display">\[
\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}=S(p, q)=\max _{f \in \mathcal{H}^{d}}\left\{\mathbb{E}_{\mathbf{x} \sim p}\left[\operatorname{tr}\left(\mathcal{A}_{q} f(\mathbf{x})\right)\right]\right \}.
\]</span> 上式中<span class="math inline">\(\|f\|_{\mathcal{H}^{d}} \leq 1\)</span>，对应于最初提到的映射到希尔伯特空间中的一个球中的最优向量。这个<span class="math inline">\(S(p,q)\)</span>最大值所对应的向量为 <span class="math inline">\({f}^{*}=\boldsymbol{\beta} /\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}\)</span></p>
<p>简单来说，引入kernel之后，我们可以直接得到stein discrepancy定义式中的函数。也就是说，KSD非常容易就能求得。</p>
<h1 id="stein-variational-gradient-descent-svgd">Stein Variational Gradient Descent (SVGD)</h1>
<p>SVGD的另一个核心公式在于 <span class="math display">\[
\left.\nabla_{\epsilon} \mathrm{KL}\left(q_{[T]} \| p\right)\right|_{\epsilon=0}=-\mathbb{E}_{x \sim q}\left[\operatorname{tr}\left(\mathcal{A}_{p} \phi(x)\right)\right]
\]</span> 也就是说KL散度变分求导等于KSD（具体推导过程见附录），这意味着KL散度变化最快的方向就是KSD所对应的向量函数 <span class="math inline">\(\phi^{*}=\boldsymbol{\beta} /\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}\)</span> <img src="/images/vimcmc/2.png" title="算法伪代码" /> 具体而言粒子<span class="math inline">\(\left\{x_{i}^{l}\right\}_{i=1}^{n}\)</span> 表示第<span class="math inline">\(l\)</span>次达代的第<span class="math inline">\(i\)</span>个粒子, 一共<span class="math inline">\(n\)</span> 个。粒子最开始是从分布<span class="math inline">\(q_{0}\)</span>中采样的, 最初的分布<span class="math inline">\(q\)</span>可以是任意 的。也就是说，该算法不依赖于初始的分布。</p>
<p>算法中的更新项包含了两个部分 <span class="math display">\[
k\left(x_{j}^{\ell}, x\right) \nabla_{x_{j}^{\ell}} \log p\left(x_{j}^{\ell}\right)+\nabla_{x_{j}^{\ell}} k\left(x_{j}^{\ell}, x\right)
\]</span> 其中第一项意味着粒子会朝<span class="math inline">\(p\)</span>分布概率高的地方移动，而第二项代表着粒子将会朝着远离当前迭代轮数$l ll的粒子，从而减轻局部最优的风险。</p>
<p><img src="/images/vimcmc/2.gif" title="SVGD拟合一维分布" /></p>
<h1 id="回顾与总结">回顾与总结</h1>
<p>从上文繁杂的推导中，SVGD算法确保粒子的移动是朝着KL散度的减小最快方向，而这个方向可以有核化的stein discrepancy导出 我们回看一下KL divergence的定义式： <span class="math display">\[\mathrm{KL}(P \| Q)=\int P(x) \log \frac{P(x)}{Q(x)} d x\]</span></p>
<p>对于目标分布<span class="math inline">\(p(x)\)</span>，变分推断 (VI)目标是从一类分布族<span class="math inline">\(\mathcal{Q}\)</span>中找到最优的<span class="math inline">\(q(x)\)</span> <span class="math display">\[
q^{*}=\underset{q \in \mathcal{Q}}{\arg \min }\left\{K L(q \| p)=\mathbb{E}_{q}[\log q(x)]-\mathbb{E}_{q}[\log \bar{p}(x)]+\log Z\right\}
\]</span> 而SVGD在再生核希尔伯特空间下给出了使得KL散度下降最快的确定性方向，类似经典的梯度下降算法，可以理解为迭代构建增量变化的方法。</p>
<h1 id="appendix">Appendix</h1>
<p><span id="jump"> </span></p>
<h2 id="the-reproducing-kernel-hilbert-space">The reproducing kernel Hilbert space</h2>
<p>先回顾一下kernel的定义： Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[
k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}
\]</span></p>
<p>在此基础上，我们用一个异或问题的例子来介绍RKHS。考虑特征映射</p>
<p><span class="math display">\[\phi: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}\]</span> <span class="math display">\[x=\left[\begin{array}{l}
x_{1} \\
x_{2}
\end{array}\right] \quad \mapsto \quad \phi(x)=\left[\begin{array}{c}
x_{1} \\
x_{2} \\
x_{1} x_{2}
\end{array}\right]\]</span> <img src="/images/vimcmc/4.png" title="特征空间和特征映射。希尔伯特空间的元素一般是函数，而函数可以被视为无穷维的向量。因此事实上希尔伯特空间的基底是一组无限维的函数，可以参考傅立叶变化或泰勒展开" /></p>
<p>kernel <span class="math display">\[
k(x, y)=\left[\begin{array}{c}
x_{1} \\
x_{2} \\
x_{1} x_{2}
\end{array}\right]^{\top}\left[\begin{array}{c}
y_{1} \\
y_{2} \\
y_{1} y_{2}
\end{array}\right]
\]</span></p>
<p>接下来我们可以定义一个特征函数： <span class="math display">\[
f(x)=a x_{1}+b x_{2}+c x_{1} x_{2}
\]</span> 这个函数属于从<span class="math inline">\(\mathcal{X}=\mathbb{R}^{2}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>的函数空间。此时，我们也可以把函数<span class="math inline">\(f\)</span>等价表示为： <span class="math display">\[
f(\cdot)=\left[\begin{array}{l}
a \\
b \\
c
\end{array}\right]
\]</span> 至此，我们可以把<span class="math inline">\(f(x)\)</span>写为： <span class="math display">\[
\begin{aligned}
f(x) &amp;=f(\cdot)^{\top} \phi(x) \\
&amp;:=\langle f(\cdot), \phi(x)\rangle_{\mathcal{H}}
\end{aligned}
\]</span> 也就是说，特征函数<span class="math inline">\(f\)</span>在<span class="math inline">\(x\)</span>的值可以被写为特征空间中的内积。<span class="math inline">\(\mathcal{H}\)</span>是一个将<span class="math inline">\(\mathbb{R}^{2}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>的函数空间。上面这些乱七八糟的怎么体现再生性呢？我们仔细看下面的等式 <span class="math display">\[
k(\cdot, y)=\left[\begin{array}{c}
y_{1} \\
y_{2} \\
y_{1} y_{2}
\end{array}\right]=\phi(y)
\]</span> 上式我们参考<span class="math inline">\(f(\cdot)\)</span>类似的定义。具体来说，如果我们令<span class="math inline">\(a=y_{1}, b=y_{2}\)</span>, and <span class="math inline">\(c=y_{1} y_{2}\)</span>，就有 <span class="math display">\[
\langle k(\cdot, y), \phi(x)\rangle_{\mathcal{H}}=a x_{1}+b x_{2}+c x_{1} x_{2}
\]</span></p>
<p>总的来说RKHS两个特性：<br />
每个点的特征映射在特征空间中 <span class="math display">\[
\forall x \in \mathcal{X}, \quad k(\cdot, x) \in \mathcal{H}
\]</span> 再生性：<br />
<span class="math display">\[
\forall x \in \mathcal{X}, \forall f \in \mathcal{H},\langle f, k(\cdot, x)\rangle_{\mathcal{H}}=f(x)
\]</span> <span class="math display">\[
k(x, y)=\langle k(\cdot, x), k(\cdot, y)\rangle_{\mathcal{H}}
\]</span></p>
<h2 id="kl散度一阶导与ksd">KL散度一阶导与KSD</h2>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/26/pd5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/26/pd5/" itemprop="url">Multi-Label Image Recognition with Graph Convolutional Networks [CVPR2019]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-26T23:25:47+08:00">
                2021-10-26
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03582">Multi-Label Image Recognition with Graph Convolutional Networks</a><br />
Code: <a target="_blank" rel="noopener" href="https://github.com/chenzhaomin123/ML_GCN">link</a></p>
<p>针对多标签图像识别 (multi-label image recognition) 问题，旷视研究院提出一种基于图卷积网络的模型取得了良好的表现，该模型包含一个CNN的图像特征提取模块和一个图卷积网络进行标签间关系提取模块。</p>
<h2 id="intro">Intro</h2>
<p>对于多标签图像的识别问题，传统的方法往往是对每个标签进行孤立的二分类，即预测每个物体是否出现。基于概率图模型或RNN模型的方法则考虑显式的建模标签之间的依赖关系。也有方法将图像区域划分后考虑区域间的局部相关性，从而隐式的建模标签相关性。本文提出的基于GCN的端到端模型将标签的表示映射到相互独立的对象分类器上。</p>
<h2 id="related-work">Related Work</h2>
<p>最简单的多标签识别方法就是为每个标签独立训练一个二分类器，这种模型没有考虑标签之间的关系。当数据集中可能的标签数量增长时，可能的标签组合就会指数级增长（当一个数据集包含20个标签，则标签组合就有<span class="math inline">\(2^{20}\)</span>种。基于RNN、LSTM之类的模型将标签嵌入为向量，从而发掘标签间的相关性。<br />
本文提出的模型将多标签构建为有向图，借助GCN在标签间的信息传播来学习图像标签间依赖、共现关系，并实现端到端训练。</p>
<h2 id="framework">Framework</h2>
<p><img src="/images/pd5/2.png" title="模型框架" /></p>
<h3 id="图像特征提取">图像特征提取</h3>
<p>论文用CNN进行图像特征提取，具体为ResNet-101的网络结构，输入图像<span class="math inline">\(I\)</span>，经过cnn和global max-pooling后得到2048维图像特征。 <span class="math display">\[
\boldsymbol{x}=f_{\mathrm{GMP}}\left(f_{\mathrm{cnn}}\left(\boldsymbol{I} ; \theta_{\mathrm{cnn}}\right)\right) \in \mathbb{R}^{D}
\]</span></p>
<h3 id="图卷积">图卷积</h3>
<p>卷积模块与最基本的卷积相同，如下式 <span class="math display">\[
\boldsymbol{H}^{l+1}=h\left(\widehat{\boldsymbol{A}} \boldsymbol{H}^{l} \boldsymbol{W}^{l}\right)
\]</span> 我们主要关注如何构图，在这一方面，本文的idea似乎有些超脱CV领域。模型针对图片数据集构建图，图中的节点为数据中的标签，并使用word embedding（pre-trained glove）对节点特征进行初始化。<br />
而对于图的边，也对应图卷积中的矩阵<span class="math inline">\(\boldsymbol{A}\)</span>（文中称其为相关系数矩阵），模型使用条件概率<span class="math inline">\(P\left(L_{j} \mid L_{i}\right)\)</span>进行建模，已期获得标签相关性信息。 <img src="/images/pd5/3.png" /> 具体而言，论文统计了数据集中的标签对的共现次数，然后构建共现矩阵，并设定一个阈值来进行二值化处理，借此过滤噪声边。 <img src="/images/pd5/1.png" title="基于多标签构建有向图" /></p>
<p>借助模型框架图可以看到，模型中图卷积模块起的是类似辅助分类器的作用，图中每个标签节点就是该标签的一个二分类器，将基于整个数据集训练的分类器<span class="math inline">\(\boldsymbol{W} \in \mathbb{R}^{C \times D}\)</span>与图像的特征<span class="math inline">\(x \in \mathbb{R}^{D}\)</span>进行点积，得到<span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^{C}\)</span>（C表示标签的总数）。图卷积利用的信息也只有图的边，也就是标签的共现，而后借助图卷积与图像特征提取进行共同训练，得到标签之间关系的隐式表示，最终推动更准确的多标签识别。</p>
<h2 id="实验">实验</h2>
<p><img src="/images/pd5/4.png" title="实验结果—非常不错 XD" /> 不过在尝试复现该模型时，本人试验了几个数据集似乎始终无法到达论文中的结果。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/24/pd6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/24/pd6/" itemprop="url">Multi-hop Question Generation with Graph Convolutional Network [Arxiv]</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-24T23:56:42+08:00">
                2021-10-24
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-daily/" itemprop="url" rel="index">
                    <span itemprop="name">Paper_daily</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.09240.pdf">Multi-hop Question Generation with Graph Convolutional Network</a><br />
Code: <a target="_blank" rel="noopener" href="https://github.com/HLTCHKUST/MulQG">link</a></p>
<h2 id="background">Background</h2>
<p>问题生成(QG)是一个从给定的上下文自动生成问题或答案的任务，而多跳问题生成 (Multi-hop Question Generation) 需要从多个不同的段落中推理生成与答案相关的问题。QG可以应用于教育系统，也可以结合QA模型作为双重任务来增强QA系统的推理能力。对于多跳问题生成，核心问题在于如何连接多个段落间的零散的信息以及答案。</p>
<p><img src="/images/pd6/1.png" title="多跳问题生成" /></p>
<h2 id="模型">模型</h2>
<p><img src="/images/pd6/2.png" title="模型框架" /></p>
<h3 id="multi-hop-encoder">Multi-hop Encoder</h3>
<p>对于输入的文本段落和答案，先分割成word-level的token，并分别用pre-trained Glove进行embedding，并在文本的token embedding中加入答案 embedding tag。对于得到的token embedding 输入到LSTM-RNN中学习初步的上下文相关的representation，再输入到Encoder中，模型的Encoder包括三个部分:</p>
<ul>
<li><p>Answer-aware context encoder 这一部分参考了阅读理解中的co-attention reasoning机制: <span class="math display">\[
\begin{aligned}
S &amp;=C_{0}^{T} A_{0} \in R^{n \times m} \\
S^{\prime} &amp;=\operatorname{softmax}(S) \in R^{n \times m} \\
S^{\prime \prime} &amp;=\operatorname{softmax}\left(S^{T}\right) \in R^{m \times n} \\
A_{0}^{\prime} &amp;=C_{0} \cdot S^{\prime} \in R^{d \times m} \\
\tilde{C}_{1} &amp;=\left[A_{0} ; A_{0}^{\prime}\right] \cdot S^{\prime \prime} \in R^{2 d \times n}\\
C_{1} &amp;=\operatorname{BiLSTM}\left(\left[\tilde{C}_{1} ; C_{0}\right]\right) \in R^{d \times n}
\end{aligned}
\]</span> 相关性矩阵S表示答案与上下文的相关性，整个过程比较复杂，这一模块的有效性在阅读理解任务中被验证，大致操作即将答案与文本计算attention后生成新的“答案”而后同样进行一遍相关性计算，最后输入Bi-LSTM中。</p></li>
<li><p>GCN-based entity-aware answer encoder 将上述encoder得到的embedding输入到GCN中进行多跳信息的嵌入。 <img src="/images/pd6/3.png" title="GCN-based entity-aware answer encoder" /> 图中的节点为文本中的命名实体（由BERT自动化提取），如果实体对在同一句子中，则为它们创建边。将上面Answer-aware context encoder 的结果结合到多跳图卷积中，并最终和图的结果结合，输入到Bi-attention模型，进一步得到token的representations <span class="math display">\[
A_{1}=\text { BiAttention }\left(A_{0}, E_{M}\right)
\]</span></p></li>
<li><p>Gated encoder reasoning layer 将前面得到的结果输入到门控网络进行特征融合，进行特征保留或遗忘，得到最终的Encoder结果。</p></li>
</ul>
<h3 id="maxout-pointer-decoder">Maxout Pointer Decoder</h3>
<p>模型采用单向LSTM作为解码器，而Maxout pointer这一模块也并不是由作者提出的，而是参考了他人的模型，用这一模块减少生成结果中的重复项。</p>
<h2 id="实验">实验</h2>
<p>实验部分，作者分别做了与现有multi-hop QG模型对比以及消融实验，取得了SOTA结果，并且证明了框架中每个模块的意义。 <img src="/images/pd6/4.png" title="纵向对比" /> <img src="/images/pd6/5.png" title="消融实验" /></p>
<h2 id="总结">总结</h2>
<p>本文提出的框架总体来说比较复杂。往牛了说可以理解为整个框架模拟了人类的问题生成的过程，包括整体文本和答案的阅读，进行大概了解，而后对文本和答案中的实体进行关注，并寻找他们的联系，最后在生成问题时确定核心和次要信息，生成相关的问题。不过事实上整个框架就是对几个现有模型中的部分模块进行组装，类似“搭积木”的过程。而新加入的multi-hop图卷积部分整体方法也不具有很亮点的想法，从消融实验结果中也可以看出这一模块对最终结果的提升也并不是很明显。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/10/23/maxp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang.JPG">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Homepage">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2021/10/23/maxp/" itemprop="url">2021 MAXP命题赛 基于DGL的图机器学习任务</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-10-23T15:33:06+08:00">
                2021-10-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Project/" itemprop="url" rel="index">
                    <span itemprop="name">Project</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>使用<a target="_blank" rel="noopener" href="https://www.dgl.ai/">Deep Graph Library (DGL)</a>进行图节点分类任务，使用的图数据是基于微软学术文献生成的论文关系图，其中的节点是论文，边是论文间的引用关系。整个图包括约150万个节点，2000万条边。节点包含300维的特征，来自论文的标题和摘要等内容。节点属于约50个类别。<br />
比赛地址: <a target="_blank" rel="noopener" href="https://biendata.xyz/competition/maxp_dgl/">MAXP</a></p>
<p><img src="/images/maxp/1.png" title="比赛数据集" /></p>
<h2 id="数据预处理">数据预处理</h2>
<p>根据所给的数据集，我们需要读取节点及其对应的特征，以及边。根据论文id构建对应的节点id，并分配他们的特征和类别。读取边之后发现存在部分论文没有出现在论文数据中，这部分的节点id分配到最后，这类论文没有类别和特征，这些点的特征可以选择用邻节点特征均值进行赋值。节点特征使用Numpy保存为.npy格式，方便后续读取。<br />
对train文件中的数据进行Train/Valid分割，用于模型评估（9:1），将train/valid/test节点id和labels等数据保存为二进制文件方便快速读取 <img src="/images/maxp/3.png" /></p>
<h2 id="图构建">图构建</h2>
<p>根据上面的到的原论文id-引用论文id以及他们对应的节点id，借助DGL包构建论文引用关系图。 <img src="/images/maxp/2.png" title="构图" /> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">g = dgl.graph((u,v))</span><br><span class="line">g.nodes() <span class="comment">#获取节点id</span></span><br><span class="line">g.edges() <span class="comment">#获取边对应的节点（输出的是两个tensor）</span></span><br><span class="line">g.ndata(<span class="string">&#x27;feature&#x27;</span>) <span class="comment">#访问节点属性</span></span><br><span class="line">g.edata <span class="comment">#访问边属性</span></span><br></pre></td></tr></table></figure></p>
<h2 id="model_baseline">Model_baseline</h2>
<p>预处理和构图之后，我们模型输入的数据包括： <img src="/images/maxp/4.png" /></p>
<p>竞赛的baseline包括三个模型：</p>
<ul>
<li>graphsage</li>
<li>graphconvolution</li>
<li>graphattention</li>
</ul>
<p>其中各个网络由DGL.nn模块调库搭建。经过初步调参后发现网络深度为3层时三个模型结果最好，其中graphsage表现最好，在验证集上准确率接近54。 <img src="/images/maxp/5.png" title="可视化效果" /></p>
<h2 id="proposed-model">Proposed model</h2>
<p>数据中的节点特征已经给定，因此只能改变模型的网络结构，我参考了2020 acl的论文：Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks 所用的网络架构，具体采用的是三层图卷积后连接一层图注意力，并在图卷积与attention直接增加了skip-connection。在验证集上的到准确率为55+，目前排行榜上前10，后续将会做进一步调参来得到更好的结果。<br />
另外，根据给定的特征直接使用MLP等前向传播网络虽然无法利用引用信息，但可以用来辅助最终的分类。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/default/page/2/">2</a><a class="page-number" href="/default/page/3/">3</a><a class="extend next" rel="next" href="/default/page/2/">&gt;</a>
  </nav>




          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/touxiang.JPG"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Bbchen229" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:1109441357@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chen jiayuan</span>

  
</div>





        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
