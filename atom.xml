<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chen&#39;s Homepage</title>
  
  <subtitle>Hello AI</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-05-12T01:56:27.040Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Chen jiayuan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>GNN-LM Language Modeling based on Global Contexts via GNN [ICLR22]</title>
    <link href="http://example.com/2022/05/11/gnn-lm/"/>
    <id>http://example.com/2022/05/11/gnn-lm/</id>
    <published>2022-05-11T12:54:00.000Z</published>
    <updated>2022-05-12T01:56:27.040Z</updated>
    
    <content type="html"><![CDATA[<p>ICLR2022 spotlight</p><p>论文总的目的是以空间换时间和精度，而具体的空间开销以图的形式存储。</p><p>本文提出了GNN-LM，将图神经网络与语言模型相结合，通过在整个训练语料库中引用相似的上下文，扩展了传统的语言模型。使用k近邻检索与输入的表示最相似的邻居，模型为每个输入构建了一个有向异构图，其中节点是来自输入上下文或检索到的邻居上下文的token，边表示token之间的连接。然后利用图神经网络从检索到的上下文中聚合信息，以解码下一个token。实验结果表明，GNN-LM在标准数据集中优于强基线，并且通过与kNN-LM结合，能够在WikiText-103上取得最优效果。</p><p><strong>从闭卷考试到开卷考试</strong> 对于传统的LM，我们会令神经网络学习数据中的知识，而在测试时这些数据是不可见的，这就类似“闭卷考试”。这时，影响模型测试的准确率的因素，或者说影响我们“考试成绩”的因素，就是是否能够准确的记住所有的数据/“知识点”。而作者提到，开卷考试一般比闭卷考试简单，因此令训练数据在测试时是可见的，我们在面对“考试题”的时候会去“书本”上查找最相关的知识点来解答。而这里知识点匹配的算法则是KNN-LM，得到的知识点构建图来回答问题。</p><p><img src="/images/gnn-lm1.png" title="pipeline" /></p><p>如上图右侧所示，对于输入的context，首先借助KNN-LM生成近邻<span class="math inline">\(\mathcal{N}\left(\boldsymbol{c}_{t}\right)=\left\{\boldsymbol{c}_{t_{1}}^{(1)}, \ldots, \boldsymbol{c}_{t_{k}}^{(k)}\right\}\)</span>。而后对检索出来的语句和测试语句构建一个异构图，包含<span class="math inline">\(a_0\)</span>和<span class="math inline">\(a_n\)</span>两类节点，分别对应查询语句中的token和近邻的token，接下来对构建的图应用GNN，得到输入语句的最后一个token的representation用以预测</p><p><img src="/images/gnn-lm2.png" title="性能对比" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ICLR2022 spotlight&lt;/p&gt;
&lt;p&gt;论文总的目的是以空间换时间和精度，而具体的空间开销以图的形式存储。&lt;/p&gt;
&lt;p&gt;本文提出了GNN-LM，将图神经网络与语言模型相结合，通过在整个训练语料库中引用相似的上下文，扩展了传统的语言模型。使用k近邻检索与输入的表</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>HETERMPC A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations [ACL22]</title>
    <link href="http://example.com/2022/05/11/22511/"/>
    <id>http://example.com/2022/05/11/22511/</id>
    <published>2022-05-11T02:34:15.000Z</published>
    <updated>2022-05-12T01:56:23.270Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2203.08500.pdf">HETERMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations</a></p><h1 id="intro">Intro</h1><h2 id="multi-party-conversations">Multi-Party Conversations</h2><p>最基本的对话系统（dialogue system）基于两个对话者，这类对话被称为two-party conversation。与之相对应，更复杂、更实际的mult-party conversation意味着更多的对话参与者。在two-party conversation中，对话为一人一句交替发言的序列。而MPC中的话语可以被任何人说出，也可以在这个对话中对任何人讲话。</p><p><img src="/images/mpc/1.png" /></p><p>如上图所示，MPC更适合被建模为graphical information flow，而不是传统的sequence。因此，对比two-party conversation用的Seq2Seq模型，似乎用graph-structured network建模mpc更加合理。与Multi-hop的一些工作比较类似——从Entity-GCN这类同构图到后面的异构图模型——本文的主要改进也是立足于此。对比IJCAI19的模型GSN: A graph-structured network for multi-party dialogues.构建的以对话为节点的图，本文的模型构建了一个包含对话者和话语的异构图。对话者也是mpc的重要组成部分。对话者之间、话语与对话者之间存在着复杂的交互。因此，单纯的以话语为节点的图无法区分两个连接的话语节点之间的“应答”关系或“被应答”关系。</p><p>MPC中的问题生成任务是指给定对话历史，回复者以及回复的语句后，生成一个合适的回复<span class="math inline">\(r\)</span>，整个问题可以被表示为： <span class="math display">\[\bar{r} =\underset{r}{\operatorname{argmax}} \log P(r \mid \mathbb{G}) =\underset{r}{\operatorname{argmax}} \sum_{k=1}^{|r|} \log P\left(r_{k} \mid \mathbb{G} r_{&lt;k}\right) .\]</span> 其中G代表整个异构图，它包含了对话的历史信息以及待生成回复。回应的发出者和接受者是已知的，但具体内容被mask。回复语句的每个token借助自回归模型生成。<span class="math inline">\(r_k\)</span>和<span class="math inline">\(r_{&lt;k}\)</span> 表示第k个token和第k-1个token。</p><h2 id="heterogeneous-graph">Heterogeneous Graph</h2><p>模型提出了一种基于异构图的神经网络，称为HeterMPC。首先，设计的异构图包含两种类型的节点，分别表示话语和对话者。不同于以往只对话语进行同构图建模的方法，HeterMPC对话语和对话者同时建模，使对话者之间、话语之间、对话者和对话者之间的复杂互动能够被明确地描述。为了刻画对每个(源、边、目标)三元组的异构注意，在计算注意权值和传递消息时引入了依赖于这两类节点和边的模型参数。具体地说，我们引入了六种关系来建模不同的边连接，包括两个话语之间的“回答”和“被回答”，话语与说话者之间的“说话”和“被说话者”，以及话语与接收者之间的“称呼”和“被称呼”。有了这些节点-边类型相关的结构和参数，与传统的同构图相比，HeterMPC可以更好地利用会话的结构知识进行节点表示和响应生成。最后，HeterMPC用Transformer作为其backbone，它的模型参数可以用预训练模型进行初始化。</p><p><img src="/images/mpc/2.png" /></p><h1 id="hetermpc-model">HeterMPC Model</h1><p>HeterMPC采用一种Encder-Decoder架构，多个layers堆叠用于Graph2Seq学习。图编码器的目的是捕获对话结构，并输出图中所有节点的表示，这些节点提供给解码器以生成回复。</p><h2 id="graph-construction">Graph Construction</h2><p>异构图用来捕获对话人以及话语间的显式交互结构。图中节点包含两类： interlocutors I和utterances M。对于不同节点间的连接，模型设计了六种类型的初始边连接{reply, replied-by, speak, spoken-by, address, addressed-by}。比如语句节点n是对另一句话m的回复，则有边<span class="math inline">\(e_{n,m}=reply\)</span>，<span class="math inline">\(e_{m,n}=replied-by\)</span>。如果某一句话m是由谈话人i说出的，则有边<span class="math inline">\(e_{i,m}=speak\)</span>，<span class="math inline">\(e_{m,i}=spoken-by\)</span>。如果某一句话n是针对谈话人i的，则有边<span class="math inline">\(e_{n,i}=address\)</span>，<span class="math inline">\(e_{i,n}=addressed-by\)</span>。其他节点之间则没有边连接。</p><p>每个话语节点初始化时，节点开头会包含一个[cls]，结尾会包含[sep]。而后每个utterance会输入到transformer中进行编码。说话人并不是由token构成，因此直接根据他们的说话顺序的索引进行embedding，在端到端的学习中进行更新。</p><h2 id="node-updating">Node Updating</h2><p><img src="/images/mpc/4.png" /></p><p>初始化的节点表示输入到构建的图中获取上下文信息进行更新，主要借助图注意力和message passing。模型架构跟Transformer比较类似，对于(s,e,t)三元组根据边类型进行attention计算，而后组合成一个向量输入到一个前向神经网络，并引入一个残差连接。 <span class="math display">\[\overline{\boldsymbol{h}}_{t}^{l}=\sum_{s \in S(t)} \operatorname{softmax}\left(w^{l}(s, e, t)\right) \overline{\boldsymbol{v}}^{l}(s)\]</span> <span class="math display">\[\boldsymbol{h}_{t}^{l+1}=F F N_{\tau(t)}\left(\overline{\boldsymbol{h}}_{t}^{l}\right)+\boldsymbol{h}_{t}^{l}\]</span></p><h2 id="decoder">Decoder</h2><p><img src="/images/mpc/5.png" title="Decoder of HeterMPC" /></p><h1 id="exp">Exp</h1><p><img src="/images/mpc/6.png" /></p><p><img src="/images/mpc/3.png" title="Linux dataset" /></p><p>最后关于这篇论文絮絮叨叨几句。首先关于MPC这一问题，抛开生成任务不谈，其本身的信息处理过程跟multi-hop有异曲同工之处。而图结构在整个编码器中所起的作用，其实与图神经网络并不相同。与其说encoder中用的是message passing，不如说是用了attention，而在其中图结构相当于是引入了一个先验，一个拓扑结构的先验信息。虽然我们都希望大道至简，但异构图的复杂结构、边连接能比由单一类型节点构成的同构图包含更多的信息。或许我们也可以参考这类模型的设计流程，以图结构信息引入先验，而GNN则用表达能力更强的transformer的机制来替换。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2203.08500.pdf&quot;&gt;HETERMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conv</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>Probabilistic Graphical Model</title>
    <link href="http://example.com/2022/05/03/pgm/"/>
    <id>http://example.com/2022/05/03/pgm/</id>
    <published>2022-05-03T14:33:37.000Z</published>
    <updated>2022-05-05T07:58:43.397Z</updated>
    
    <content type="html"><![CDATA[<h1 id="intro">Intro</h1><p>概率图模型（probabilistic graphical model）提供了一个概率与图结构结合的途径，因其灵活性、强大的表达能力和在大规模数据集中的学习和推理能力而受到广泛关注。从高层面来说，我们的目标是表示一个关于多元变量<span class="math inline">\(X = {X_1,X_2,...,X_n}\)</span>的联合概率分布<span class="math inline">\(P\)</span>，以及边缘分布<span class="math inline">\(P(X_i)\)</span>。但是，即便每个变量是二值变量（比如0/1），其联合概率分布的计算开销也是随着变量增加而指数级的增长。但是这些变量之间或多或少会存在一些联系，此时，我们就可以引入图结构来进行建模，借助图来直观的表示变量间的条件独立性关系，进而简化计算。两种最常见的PGM包括贝叶斯网络和马尔可夫随机场，分别为有向图和无向图。</p><p>概率图模型有三类主要问题：表示问题：对于一个概率模型，如何通过图结构来描述变量之间的依赖关系。学习问题：图模型的学习包括图结构的学习和参数的学习。推断问题：在已知部分变量时，计算其他变量的条件概率分布。</p><h1 id="representation">Representation</h1><h2 id="bayesian-networks">Bayesian Networks</h2><p><img src="/images/pgm/1.png" /></p><p>贝叶斯网络的核心结构在于有向无环图，图中的节点表示随机变量，而边则表示了变量间的关系。因此，贝叶斯网络中的节点都有一个相应的条件概率分布 <span class="math inline">\(P(X_i|Pa_{X_i})\)</span>，Pa表示父节点。此时，我们就可以得到贝叶斯网络的链式法则。 <span class="math display">\[P_{\mathcal{B}}\left(X_{1}, \ldots, X_{n}\right)=\prod_{i=1}^{n} P\left(X_{i} \mid \mathbf{P} \mathbf{a}_{X_{i}}\right)\]</span> 对于贝叶斯网络，我们有条件独立性假设，即 <span class="math display">\[\left(X_{i} \perp \text { NonDescendants }_{X_{i}} \mid \mathbf{P} \mathbf{a}_{X_{i}}\right)\]</span></p><p><img src="/images/pgm/2.png" /> 对于有向图，我们可以把变量间的概率视作一种流动的过程，考虑一个简单的三节点路径X - Y -Z如果条件概率影响可以通过Z从X流到Y，我们说路径X -Z - Y是激活的。借此可以给出在因果推断中常见的情况:</p><ul><li>Causal path X →Z →Y</li><li>Evidential path X ← Z ← Y</li><li>Common cause X ← Z → Y</li><li>Common effect X → Z ← Y</li></ul><h2 id="markov-networks">Markov Networks</h2><p>第二种常见的概率图模型称为马尔可夫网络或马尔可夫随机场，这些模型基于无向图。在独立性结构和推理任务方面，无向模型还为有向模型提供了不同的、通常更简单的视角。马尔可夫随机场的无向图允许自环存在，且满足局部马尔可夫性，即一个变量X 在给定它的邻居的情况下独立于所有其他变量。在马尔可夫随机场中，我们不能用参数化的概率或条件概率来表示一个变量，而是引入势能函数 potential function <span class="math display">\[\begin{array}{c}P_{\mathcal{H}}\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{Z} P^{\prime}\left(X_{1}, \ldots, X_{n}\right) \\P_{\mathcal{H}}^{\prime}\left(X_{1}, \ldots, X_{n}\right)=\pi_{i}\left[\boldsymbol{D}_{1}\right] \times \pi_{2}\left[\boldsymbol{D}_{2}\right] \times \cdots \times \pi_{m}\left[\boldsymbol{D}_{m}\right]\end{array}\]</span> 联合分布被分解为连通子图/团的势能函数<span class="math inline">\(\pi\)</span>的乘积，并除以一个归一化因子Z。势能函数需要满足非负性，所以一般表示为： <span class="math display">\[\pi[\boldsymbol{D}]=\exp (-\epsilon[\boldsymbol{D}])\]</span> 其中<span class="math inline">\(\epsilon[\boldsymbol{D}]=-\ln \pi[\boldsymbol{D}]\)</span></p><p>由于无向图模型并不提供一个变量的拓扑顺序，因此无法用链式法则对<span class="math inline">\(P(X)\)</span> 进行逐一分解。无向图模型的联合概率一般以全连通子图为单位进行分解。 无向图中的一个全连通子图，称为团（Clique），即团内的所有节点之间都连边。</p><h1 id="inference">Inference</h1><p>有向和无向图模型都代表了多元变量上的完整联合概率分布。对于概率图上的推理问题，我们可以理解为使用联合概率分布来回答一些query。</p><p>最常见的conditional probability query <span class="math inline">\(P(Y|E=e)\)</span>由两部分组成：evidence E和query Y；另一种是寻找剩余变量最有可能的值 <span class="math inline">\(\operatorname{argmax}_{\boldsymbol{y}} P(\boldsymbol{y} \mid \boldsymbol{e})\)</span>。对于以上的一些查询，最容易想到的解决方法就是得到联合概率分布，而后针对特定变量（条件概率查询）进行求和，但这一过程的计算复杂度是我们不能接受的。在这种情况下，我们选择使用概率图结构特点来进行近似推断，并考虑它产生精确结果的条件-变分推断；或者使用基于采样的近似推断来应对小数据集（比如MCMC，从后验分布生成样本）。</p><h1 id="learning">Learning</h1><p>概率图模型的学习任务有两种:参数估计和结构学习。在参数估计任务中，我们假设图模型的结构是已知的。在这种情况下，学习任务只是学习CPD的参数或定义马尔可夫网络势能函数的参数。在结构学习任务中，不需要额外的输入(尽管用户可以提供关于结构的先验知识，例如，以约束的形式)。目标是从训练数据单独提取贝叶斯网络或马尔科夫网络结构和参数。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;
&lt;p&gt;概率图模型（probabilistic graphical model）提供了一个概率与图结构结合的途径，因其灵活性、强大的表达能力和在大规模数据集中的学习和推理能力而受到广泛关注。从高层面来说，我们的目标是表示一个关于多</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
  </entry>
  
  <entry>
    <title>GNN is a Counter? Revisiting GNN for Question Answering [ICLR22]</title>
    <link href="http://example.com/2022/04/12/iclr22-1/"/>
    <id>http://example.com/2022/04/12/iclr22-1/</id>
    <published>2022-04-12T12:00:00.000Z</published>
    <updated>2022-04-20T15:10:28.551Z</updated>
    
    <content type="html"><![CDATA[<h1 id="foreword">Foreword</h1><p>针对Knowledge-based QA问题，一般从问题和答案中抽取关键实体，对这些实体构建图，并在图上进行路径推理从而得到正确答案，GNN 在图上的信息传播过程相当于在图上找路径，这些路径可以解释答案得到的推理步骤。但是复杂的图结构是否是推理所必须的？作者根据模型剪枝的结果发现现有模型的结构太过复杂，实际上一维的图节点就可以完成KGQA任务。 <img src="/images/ICLR22/3.png" /></p><h1 id="分析">分析</h1><p>传统的模型使用预训练模型作为encoder，得到问题中的实体、答案的表示；而后使用知识图谱中显式的知识，从知识图谱中抽取问题相关的子图。接下来将节点表示、边的表示作为输入，过几层GNN后输出最终结果。 作者在这一baseline上使用Sparse Variational Dropout (SparseVD) 进行模型剪枝，具体来说就是寻找GNN各层中对结果没有影响的权重。作者在多个不同的模型上进行实验，结果显示，除了边的embedding之外，节点embedding、节点初始化对最终的结果影响都不大。 <img src="/images/ICLR22/4.png" /> <img src="/images/ICLR22/1.png" /></p><h1 id="gsc">GSC</h1><p>对于上面模型pruning的结果，作者设计了一套简化的GNN，将节点和边都变为1维，直接进行加减作为message passing；而实验结果表明这一简化版的GNN取得了非常好的效果。这个简化版的GNN-GSC主要是两个部分，包括边的encoder和Message passing。前面的实验证明了边的重要性，因此使用2层mlp输出最终为1维的Edge- embedding，作为边的重要性，而1维的message passing只是数值加减，所以可以看作一个计数器。设计的具体模型可以参考下图 <img src="/images/ICLR22/2.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;foreword&quot;&gt;Foreword&lt;/h1&gt;
&lt;p&gt;针对Knowledge-based QA问题，一般从问题和答案中抽取关键实体，对这些实体构建图，并在图上进行路径推理从而得到正确答案，GNN 在图上的信息传播过程相当于在图上找路径，这些路径可以解释答案得到的推</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Revisiting Over-smoothing in BERT from the Perspective of Graph [ICLR22]</title>
    <link href="http://example.com/2022/04/11/iclr22/"/>
    <id>http://example.com/2022/04/11/iclr22/</id>
    <published>2022-04-11T12:00:00.000Z</published>
    <updated>2022-04-20T07:30:22.819Z</updated>
    
    <content type="html"><![CDATA[<h1 id="foreword">Foreword</h1><p>作者提到，基于Transformer的模型NLP以及CV中应用时也会出现over-smoothing问题。由于该问题最初出现于GNN中，也是GNN研究中非常重要的方向，因此作者从Graph machine learning中借鉴了一套处理思路，搬运到Transformer中进行魔改，发现效果不错。另外，作者对self-attention与GNN的内在联系也进行了分析。</p><h1 id="intro">Intro</h1><p>问题纷纷涌现出来：（1）Token uniformity: self-attention会使得token representations identical。（2）Over-smoothing problem for ViT: 不同的patch被映射到一个相似的潜在表示。（3）“overthinking” phenomenon: 在某些问题下，shallow representations比deep representations表现更好。总的来说，这些问题都可以被解释为过平滑问题，也就是token的representation会随着网络的加深趋向一致，丧失自身的特点。</p><p>over-smoothing最初在GNN中被提出。而另一方面self-attention matrix可以被看成加权图的normalized adjacency matrix，其中图节点就是句子的token。</p><h1 id="transformer">Transformer</h1><p>self-attention模块整个公式（包含残差）可以被写成如下： <span class="math display">\[\operatorname{Attn}(\boldsymbol{X})=\boldsymbol{X}+\sum_{k=1}^{h} \sigma\left(\boldsymbol{X} \boldsymbol{W}_{k}^{Q}\left(\boldsymbol{X} \boldsymbol{W}_{k}^{K}\right)^{\top}\right) \boldsymbol{X} \boldsymbol{W}_{k}^{V} \boldsymbol{W}_{k}^{O \top}=\boldsymbol{X}+\sum_{k=1}^{h} \hat{\boldsymbol{A}}_{k} \boldsymbol{X} \boldsymbol{W}_{k}^{V O}\]</span> 其中最关键的注意力系数矩阵可以被看成图的邻接矩阵： <span class="math display">\[\hat{\boldsymbol{A}}=\sigma\left(\boldsymbol{X} \boldsymbol{W}^{Q}\left(\boldsymbol{X} \boldsymbol{W}^{K}\right)^{\top}\right)=\sigma\left(\boldsymbol{Q} \boldsymbol{K}^{\top}\right)\]</span> 对应的transformer中feed-forward layer可以被写为图卷积（GCN）的形式： <span class="math display">\[FF(\boldsymbol{X})=\operatorname{Attn}(\boldsymbol{X})+\operatorname{ReLU}\left(\operatorname{Attn}(\boldsymbol{X}) \boldsymbol{W}_{1}+\boldsymbol{b}_{1}\right) \boldsymbol{W}_{2}+\boldsymbol{b}_{2}\]</span></p><p>ResGCN： <span class="math display">\[\operatorname{ResGCN}(\boldsymbol{X})=\boldsymbol{X}+\operatorname{Re} L U\left(\boldsymbol{D}^{-1 / 2} \boldsymbol{A} \boldsymbol{D}^{-1 / 2} \boldsymbol{X} \boldsymbol{W}\right)=\boldsymbol{X}+\operatorname{Re} L U(\hat{\boldsymbol{A}} \boldsymbol{X} \boldsymbol{W})\]</span></p><h1 id="over-smoothing">over-smoothing</h1><p>既然self-attention被表示为了针对句子构建的全连接图上的图神经网络，我们可以从图角度分析over-smoothing并借鉴其解决方案。在GNN上，节点通过message passing获取邻接点信息并聚合，而后更新自身特征。由于不断聚合，而且使用同一个邻接矩阵，导致节点相似度越来越高。作者用cosine similarity来评估over-smoothing程度： <span class="math display">\[\operatorname{CosSim}=\frac{1}{n(n-1)} \sum_{i \neq j} \frac{\boldsymbol{h}_{i}^{\top} \boldsymbol{h}_{j}}{\left\|\boldsymbol{h}_{i}\right\|_{2}\left\|\boldsymbol{h}_{j}\right\|_{2}}\]</span> 而后作者对BERT中的过平滑问题进行了理论分析，发现Bert中的过平滑主要是在post-normalization这一部分引入。</p><h1 id="hierarchical-fusion-strategy">hierarchical fusion strategy</h1><p>针对过平滑的分析，作者提出了一种方法来缓解bert中的over-smoothing（保留post-normalization）具体而言，就是针对第L层的representation，将其与前面K的结果按照一定的规则进行结合，包括：Concat Fusion/Max Fusion/Gate Fusion。</p><p><img src="/images/multihopQA/13.png" title="实验结果" /></p><p>其实这个方法的idea和早在18年关于图神经网络的模型Jumping Knowledge Networks (JKNet)非常相似</p><p><img src="/images/multihopQA/14.png" title="JKNet" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;foreword&quot;&gt;Foreword&lt;/h1&gt;
&lt;p&gt;作者提到，基于Transformer的模型NLP以及CV中应用时也会出现over-smoothing问题。由于该问题最初出现于GNN中，也是GNN研究中非常重要的方向，因此作者从Graph machine le</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Multi-hop QA [2]</title>
    <link href="http://example.com/2022/04/10/multiqa2/"/>
    <id>http://example.com/2022/04/10/multiqa2/</id>
    <published>2022-04-10T12:00:00.000Z</published>
    <updated>2022-04-20T16:15:34.655Z</updated>
    
    <content type="html"><![CDATA[<h1 id="foreword">Foreword</h1><p>关于MUlti-QA的简介这里不多介绍，主流的Multi-QA模型大致可以根据是否包含显式/隐式的图结构分类两类，第一类借助Graph来将文本中离散的信息捕获并构建图结构（或隐式的构图），将先验信息融入图Message-passing中；第二类方法不借助图结构，而是通过Transformer直接处理长文本，或将问题分解后进行sub-question的问答。</p><h1 id="dynamically-fused-graph-network-for-multi-hop-reasoning">Dynamically Fused Graph Network for Multi-hop Reasoning</h1><p>在上一篇中（Multi-hop QA [1]）中介绍了一篇rethinking的文章，关于图结构在多跳推理中是否是必要的，那篇文章的起源大概可以追溯到这里。具体来说，包括Entity-GCN等一系列文章都没有开源，或者开源出部分代码导致无法完全复现，但是这篇DFGN的代码开源了并且结果是完全可复现的。在广大码农复现过程中，有人发现其中的Graph Fusion模块直接使用Transformer似乎效果会更好，于是后面Is Graph Structure Necessary for Multi-hop Question Answering? 这一文章参照了DFGN的结构设计了一个baseline进行测试，发现其中的GNN确实不如Transformer表现好。（开源确实能促进这一行业的技术发展）</p><blockquote><p>Inspired by human’s step-by-step reasoning behavior, DFGN includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores along the entity graph dynamically built from the text, and gradually finds relevant supporting entities from the given documents.</p></blockquote><p><img src="/images/multihopQA/10.png" /></p><p>上面两个图非常直观的展现整个模型的框架，借助BERT和Graph Attention及其交互机制来获取token的表示。模型通过Doc2Graph和Graph2Doc来进行序列与图的转换。首先，候选Context和问题query一起输入BERT得到Context和query的每个token的向量表示，随后，接了一个双向注意力得到二者相互attended的表示。对于token向量，模型进行pooling后得到entity向量，而后通过Query计算实体图中每个实体的重要性并以此构建实体图。在实体图上进行Graph- attention计算图注意力。在每一层Fusion Block结束后，还会使用新的实体表示通过Bi-Attention来更新Query的表示。最后从entity反变换为context的过程Graph2Doc则是将该层最初输入的Context的表示中的每个token与其对应的实体的表示拼接然后送入LSTM。</p><h1 id="multi-hop-reading-comprehension-across-multiple-documents-by-reasoning-over-heterogeneous-graphs">Multi-hop Reading Comprehension across Multiple Documents by Reasoning over Heterogeneous Graphs</h1><p>基于异构的文档-实体图。图中节点包含文档节点、实体节点、候选节点。具体节点的初始化基于co-attention 和 self-attention (这两者在Single-doc QA中非常有效)。下面介绍一下模型具体的框架，包含Encoding和Graph reasoning两个部分。</p><p>首先对于给出的query，<span class="math inline">\(&lt;s,r,?&gt;\)</span>代表查询的主体，关系以及未知客体，<span class="math inline">\(S_q\)</span>代表文档集，<span class="math inline">\(C_q\)</span>代表候选文章。对于这三者，我们使用GloVe获取其各自的embedding <span class="math inline">\(X\)</span>并输入到Encoder中，具体为Bi-RNN+GRU来编码上下文信息，输出为<span class="math inline">\(H\)</span>。而后进行文档集S中的实体抽取，作为异构图中的实体节点。实体节点的embedding从相应的文档embedding中获取。接下来进行三类节点的co-attention计算，学习query和doc相互作用的embedding，具体的计算公式在这里不做呈现，总体思路跟self-attention非常类似，只不过把作用对象从token与token变为query与<span class="math inline">\(S_q/C_q\)</span>。共同注意力用来产生文档的query-aware embedding，而self-attention pooling被设计为通过选择重要的查询感知信息将顺序上下文表示转换为非顺序特征向量。以上是Encoding部分，总体流程可以概括为初始embedding+co-atten+self-atten三个流程；而后则是进行图上的推理。</p><p>构图时，节点可以分成三类，因此在这一图上，针对不同节点之间的边定义非常复杂。模型总体包含有7类边连接，其中大部分的边定义与前人的模型是相似的。作者在后续实验时验证了不同类型边的有效性。最后图神经网络则是选择Gated-GNN。在wikihop数据集上，对比Entity-GCN，模型的准确率从71.2提升到了74.3。</p><p><img src="/images/multihopQA/15.png" /></p><h1 id="linkbert-pretraining-language-models-with-document-links">LinkBERT: Pretraining Language Models with Document Links</h1><p>上文提到的模型都在整个框架中显式的构建了图结构，并通过图上的message passing来进行消息的传播，但是这篇模型则是借助图结构来辅助Bert进行更好的上下文理解，进而直接对多段文本进行学习。</p><p>目前的language model输入的文本仅来源于单一文档的上下文。LinkBert将不同的文档间建立连接，输入的上下文来源于相连的文档。另外，模型的训练将NSP任务 变为对应的document relation prediction (DRP)。对比原始的Bert，基于LinkBERT构建的LM对解决multi-hop QA有天然的优势。 <img src="/images/multihopQA/11.png" /></p><p>输入的数据为 $ [CLS]X_A[SEP]X_B[SEP]$ 形式，<span class="math inline">\(X_{A/B}\)</span>为文档片段，对于一个Segment A，其后接的Segment B有三种选择：1.A后的连续片段B；2.随机文档的片段B；3.与文档A有hyperlink的文档的片段B</p><p>文档片段之间的链接建立需要考虑一些信息，包括：关联性，可以通过使用超链接或词汇相似性度量来实现；链接的文档是否可以提供当前的LM可能看不到的新的、有用的知识。在这方面，超链接可能比词汇相似性链接更有优势；多样性。在文档图中，一些文档可能具有非常高的程度(例如，许多传入的超链接，如维基百科的美国页面)，而其他文档可能具有较低的程度。如果我们从每个锚段的链接文档中统一采样，可能会在整体训练数据中过于频繁地包含程度较高的文档，从而失去多样性。为了调整使所有文档在训练中以类似的频率出现，我们以与其程度成反比的概率对链接文档进行采样。</p><h1 id="ask-to-understand-question-generation-for-multi-hop-question-answering">Ask to Understand: Question Generation for Multi-hop Question Answering</h1><p>该模型解决Multi-hop QA的思路更贴近人类的逻辑推理过程，将问题根据线索进行分解，而后解答子问题。但是这于这类模型，子问题的生成质量非常关键，因此模型引入了额外的 QG 任务来训练模型的问题生成部分。具体来说，我 们在经典的基于GN的模块的基础上精心设计并添加了一个端到端的QG模块。对比传统的基于QD的方法只依赖问题带来的信息，我们提出的QG模型可以同时基于对原始上下文和问题的理解来生成流畅的、含有内在逻辑的子问题。 <img src="/images/multihopQA/12.png" /> 对于QA模块，模型采用常规的Encoder+GNN的结构，但是QG辅助的作用体现在Encoder会与QG模块贡献，GNN采用的是与DFGN相同的架构；QG模块的整体架构如上图，主要问题在于针对性的数据集的获取，即如何获取训练的子问题。作者提到HotpotQA数据集中大概可以被分类两类问题：Bridge 和 Comparison 其中第一种需要从first-hop中查找线索而后推理下一个目标，后者则是对query中提到的两个实体性质进行比较。</p><p>Kristine Moore Gebbie is a professor at a university founded in what year?这是一个Bridge问题，先找到university再找到成立年份；Do The Importance of Being Icelandic and The Five Obstructions belong to different film genres?这是comparison问题，包含The Importance of Being Icelandic、Five Obstructions、film genre三个实体，这时问题则会被分解为Do The Importance of Being Icelandic belong to which film genres? 与Do The Five Obstructions belong to which film genres?</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;foreword&quot;&gt;Foreword&lt;/h1&gt;
&lt;p&gt;关于MUlti-QA的简介这里不多介绍，主流的Multi-QA模型大致可以根据是否包含显式/隐式的图结构分类两类，第一类借助Graph来将文本中离散的信息捕获并构建图结构（或隐式的构图），将先验信息融入图Mes</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="QA" scheme="http://example.com/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>Nerf Compression using DCT [Nerf]</title>
    <link href="http://example.com/2022/03/29/nerf/"/>
    <id>http://example.com/2022/03/29/nerf/</id>
    <published>2022-03-29T14:24:53.000Z</published>
    <updated>2022-04-19T08:40:35.087Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    <category term="Research" scheme="http://example.com/categories/Research/"/>
    
    
    <category term="3D reconstruction" scheme="http://example.com/tags/3D-reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>For Better Bert</title>
    <link href="http://example.com/2022/03/17/betterbert/"/>
    <id>http://example.com/2022/03/17/betterbert/</id>
    <published>2022-03-16T16:06:58.000Z</published>
    <updated>2022-03-22T13:34:47.591Z</updated>
    
    <content type="html"><![CDATA[<p>TODO</p><p>Self-Attention：计算负责度限制了长文本的输入（如文本生成任务）-&gt;Longformer；BigBird</p><p>T5：Transfer Text-to-Text Transformer<br />将所有 NLP 任务都转化成 Text-to-Text（文本到文本）任务。模型架构：<strong>Encoder-Decoder</strong> vs Decoder；自监督训练方法：<strong>BERT-style，破坏一部分</strong> vs GPT（从左到右预测）vs shuffle（将文本打乱然后还原）；本文破坏的方式：mask（把一些token换成mask）vs<strong>替换一部分span</strong>vs drop一部分 ERNIE（百度）<br />模型主要是针对BERT在中文NLP任务中表现不够好提出的改进，在训练时将短语、实体等先验知识进行mask，强迫模型对其进行建模，学习它们的语义表示。RNIE采用三种masking策略：Basic-Level Masking——跟bert一样对单字进行mask，很难学习到高层次的语义信息；Phrase-Level Masking——输入仍然是单字级别的，mask连续短语；Entity-Level Masking——首先进行实体识别，然后将识别出的实体进行mask。</p><p>multi-modal：VideoBert videoBert将视频转化为一系列“visual words”(可视化单词)。视频由一系列图片构成，一幅图片对应一帧，作者将n个连续的帧构成一个片段clip，使用cv领域的模型进行特征提取，最终抽取了特征向量，随后对所有特征向量做hierarchical vector quantization(分层矢量量化)，即聚类，得到20736 各类，每个视频都有属于自己的一个类，这个类就是文本处理时的token（visual token）。</p><p>模型压缩：TinyBERT(模型蒸馏)</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;TODO&lt;/p&gt;
&lt;p&gt;Self-Attention：计算负责度限制了长文本的输入（如文本生成任务）-&amp;gt;Longformer；BigBird&lt;/p&gt;
&lt;p&gt;T5：Transfer Text-to-Text Transformer&lt;br /&gt;
将所有 NLP 任务都转化</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="Bert" scheme="http://example.com/tags/Bert/"/>
    
  </entry>
  
  <entry>
    <title>Multi-hop QA [1]</title>
    <link href="http://example.com/2022/03/07/MultihopQA/"/>
    <id>http://example.com/2022/03/07/MultihopQA/</id>
    <published>2022-03-07T14:49:35.000Z</published>
    <updated>2022-04-20T01:56:40.952Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1808.09920v3">Question Answering by Reasoning Across Documents with Graph Convolutional Networks</a><br /><a href="https://arxiv.org/abs/2004.03096v2">Is Graph Structure Necessary for Multi-hop Question Answering?</a><br /><a href="http://arxiv.org/abs/2007.14062v2">Big Bird: Transformers for Longer Sequences</a></p><h1 id="图结构qa">图结构QA</h1><p><img src="/images/multihopQA/5.png" title="Multi-hop QA" /> 多跳问答可以看作一个多步推理以及信息结合的过程，Entity-GCN借助图结构的节点建模文本的实体，将其转化为图上的推理问题，取得了非常好的结果。另外，作者提到虽然multi-hop是一个非常具有实际意义的问题，但在此之前的模型仅仅是将文档连接为长文本，而后借助RNN类的模型进行处理。</p><p>对于QA问题，给定一个文档集合和查询，我们要从多个候选回答中选择正确的答案（实体）。其可以表示为<span class="math inline">\(\left\langle q, S_{q}, C_{q}, a^{\star}\right\rangle\)</span>，<span class="math inline">\(q\)</span>表示问题，<span class="math inline">\(S_{q}\)</span> 表示supporting document，<span class="math inline">\(C_{q}C\)</span>表示候选的entity 集合，<span class="math inline">\(a_*\)</span>是<span class="math inline">\(q\)</span>的答案。本文的目的是训练一个神经网络，给定一个查询<span class="math inline">\(q\)</span>，可以输出答案在<span class="math inline">\(C_{q}\)</span>上的一个概率分布。通过最大似然估计模型的参数，输出概率最大的结果作为预测的问题答案。</p><p><img src="/images/multihopQA/8.png" /></p><p>对于一个query，<span class="math inline">\(q=\langle s, r, ?\rangle\)</span>，我们根据supporting documents构建图，其中的节点或为查询的实体，或为候选实体。上图中同样颜色的节点代表同一实体，节点之间的边根据三种规则构建：在同一文档中共现(实线)，不同的mentions之间实体匹配(虚线)，coreference(红线)。接着借助构建的图进行图卷积（Gated-GCN），得到候选节点的特征，并将其与问题的特征结合，作为候选实体的估计。模型encoding的预处理用了ELMo。 <span class="math display">\[P\left(c \mid q, C_{q}, S_{q}\right) \propto \exp \left(\max _{i \in \mathcal{M}_{c}} f_{o}\left(\left[\mathbf{q}, \mathbf{h}_{i}^{(L)}\right]\right)\right)\]</span></p><h1 id="深入分析">深入分析</h1><p>Entity-GCN的成功以及当时GNN在NLP领域的热度让人们开始竞相探索图在多步推理上的应用。大部分的工作都将分布在不同段落间的实体抽取并建模为图结构。Is Graph Structure Necessary for Multi-hop Question Answering? 这篇文章的作者基于HotpotQA数据集构建了一个强大的基线模型并证明了，通过正确地使用预训练模型，图结构对于多步推理问答是不必要的。他们认为图结构和对应的邻接矩阵都可以被看作是一种任务相关的先验知识，并且图注意力可以被看作是自注意力的一种特例。实验和可视化分析都表明图注意力或整个图结构都可以被自注意力或Transformer替代。</p><p>论文提到基于图结构进行多跳问答的模型有很多变种，包括借助有向图的循环层来建模实体间的关系，使用动态实体图来解决抽取式的多步推理问答任务以及引入文档节点和问题节点将实体图拓展为异构图等方法。然而，作者在实验中发现移除图结构并不会影响模型的最终效果。</p><p>这里放一下原论文作者写的博客：<a href="https://mp.weixin.qq.com/s/mXLrcg0ZSaKF4w9pqv5_8w">EMNLP 2020 | 多步推理问答是否真的需要图结构？</a></p><p>这里简单介绍一下论文思路，具体细节可以参考原论文以及上面的博客。作者借助Bert和GCN构建了一个基线模型，如下图所示 <img src="/images/multihopQA/6.png" title="基线模型" /> 模型使用RoBERTa进行上下文的编码，借助Bert进行命名实体识别构建图结构，实体图的连接规则由以下两条规则确定：1）上下文中不同位置出现的相同实体之间有连接。2）同一个句子中出现的不同实体之间有连接。（这里有待考量，下文细🔒）。在HotpotQA数据集上，这个模型取得了非常好的表现： <img src="/images/multihopQA/7.png" title="实验结果" /> 基于这个模型进行后续实验时，作者发现在预训练模型以fine-tuning的方式使用时，包含和不包含图结构的模型都取得了相似的结果。而当我们固定预训练模型的参数后，EM和F1显著下降了9%和10%。如果此时进一步移除图结构，EM和F1会进一步下降4%左右。换句话说，只有当预训练模型以Feature-based的方式使用时，图结构才会起到比较明显的作用。而当预训练模型以Fine-tuning的方式使用时（这是较为通常的方式），图结构并没有对结果起到贡献，换句话说，图结构可能不是解决多步推理问题所必要的结构。</p><p>而后作者提到，图注意力是self-attention的一种特例，用transformer代替两层图神经网络也能取得非常接近的结果。作者指出邻接矩阵和图结构都可以被看作是一种任务相关的先验知识。</p><p>不过笔者私以为，既然作者认为图结构、领接矩阵是一种先验知识，那借助图神经网络可以很好的将这些先验融入到深度神经网络中。另一方面，其实图神经网络的表现很大程度上与图的构建有关，而作者在实验时只采用了这种非常naive的构图思路，而后说明这种图结构是没有作用的，感觉这里的论证略显单薄。另一方面，将先验知识结合到feature中，借助图神经网络的message passing机制进行学习和反向传播，从而将先验知识的融入纳入到神经网络这一框架下，也可以说是体现了图结构的意义。</p><h1 id="transformer-plus">Transformer plus</h1><p>上文提到，图注意力和图结构可以被自注意力或Transformer替代，也就是说基于图结构的模型本质上与Transformer是相似的。</p><p><img src="/images/multihopQA/1.png" title="Self-Attention" /> Transformer中最重要的就是Self-attention机制，从上图可以看到，而self-attention的计算包括Q、K、V三个矩阵以及矩阵乘法，此时就会引出一个复杂度问题。 <span class="math display">\[\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\]</span> Q、K点乘的内存、速度是序列长度的平方复杂度。对于输入为长文本时，我们一般做法是切成512的块，这种做法损失了块与块之间的信息，比如多跳QA问题或者长文本文本摘要，块与块之间的信息起了重要的作用。</p><p>针对这一问题，解决的方法主要可以分为两类，第一类接受长度的限制，寻求绕过这一问题的方法：比如对文本使用滑窗，或者从上下文中选择一个subset输入到transformer中，而后迭代不同的上下文。基于这一思路的模型有： SpanBERT, ORQA, REALM, RAG等。另一类方法认为full attention是没有必要的，这类 Sparse Attention Mechanism就包括Longformer、Big Bird。 <img src="/images/multihopQA/2.png" title="Longerformer" /> <img src="/images/multihopQA/3.png" title="Big Bird" /></p><p>从上图就可以看出，这些方法抛弃了原有的全局attention计算，变为几类attention，包括：</p><ul><li><p>random attention：对于每个Q，都等概率随机关注r个Key。</p></li><li><p>window attention：对于每个Q，都关注相邻的左边w/2个Key，右边w/2个key。这是因为直觉上告诉我们，多数NLP问题上下文更加重要。</p></li><li><p>global attention：在一些预先选择的输入位置上添加全局attention，使这些位置能够关注所有的信息，比如图上所显示的使传统Bert在做分类时，[CLS] token就使用了global attention。针对QA问题时，我们就可以令问题部分的token为global attention。</p></li></ul><p>Big Bird就是使用了这三类Attention，实验结果上也证明了这类模型比传统的Bert表现更好。 <img src="/images/multihopQA/4.png" title="Exp" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1808.09920v3&quot;&gt;Question Answering by Reasoning Across Documents with Graph Convolutional Networks&lt;/a&gt;&lt;br /&gt;</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="QA" scheme="http://example.com/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>A Survey of Data Augmentation Approaches for NLP</title>
    <link href="http://example.com/2022/02/17/dataaug/"/>
    <id>http://example.com/2022/02/17/dataaug/</id>
    <published>2022-02-17T12:42:55.000Z</published>
    <updated>2022-02-20T16:42:01.784Z</updated>
    
    <content type="html"><![CDATA[<p>link: <a href="https://arxiv.org/abs/2105.03075v4" class="uri">https://arxiv.org/abs/2105.03075v4</a></p><h1 id="数据增强的背景">数据增强的背景</h1><p><strong>什么是数据增强？</strong></p><blockquote><p>Data augmentation (DA) refers to strategies for increasing the diversity of training examples without explicitly collecting new data.</p></blockquote><p>作者提到，数据增强是一种在不收集新数据的前提下增强训练样本多样性的策略。从这里可以看出，首先数据增强是根据已有的数据来创造新数据的过程。其次，数据增强并不意味着单纯地增多数据，而是为了模型训练进行服务的。也就是说，我们要在任务目标指导下借助已有的样本来创造新样本。</p><p><strong>为什么要做数据增强</strong><br />当我们获得的数据有限，不足以训练出优秀的模型时，就需要获取更多的数据。但是采集新数据往往要消耗大量的人力物力资源，在这种情况下，数据增强技术就提供了一个很好的解决措施，尤其是现在NLP领域基本是大规模预训练模型大行其道，对大数据更加渴求。而令一方面，数据增强能够按照我们的意愿对数据做一些约束。换句话说，我们可以通过数据增强的方法引入一些先验知识或条件。</p><p><strong>数据增强的合理性</strong><br />从我们直观的角度思考，扩充数据的分布与原始数据的分布既不应该太相似，也不应该太不同。我们设计的数据增强方法得到的新数据是尽可能服从实际情况下的分布。此时，在样本空间上更多的采样点有助于我们进一步探索真实的数据分布。</p><blockquote><p>data augmentation is typically performed in an ad- hoc manner with little understanding of the under- lying theoretical principles</p></blockquote><p>作者提到，目前关于DA为什么有效的研究工作主要停留在表层，对其理论基础和原理研究较少。一些现有的工作包括： 带噪声测试样本的训练可简化为Tikhonov正则化；DA可以增加分类器的positive margin，但只有在许多常见DA方法以指数方式做数据增强时才会如此；将DA转换视为kernels，并发现DA的两种帮助方式:特征平均和方差正则化。</p><p><strong>Data Augmentation in CV</strong></p><p>常见的CV中的数据增强包括：</p><ul><li><p>旋转、平移、翻折</p></li><li><p>缩放：图像可以被放大或缩小。放大时，放大后的图像尺寸会大于原始尺寸。大多数图像处理架构会按照原始尺寸对放大后的图像 进行裁切。</p></li><li><p>随机裁剪，我们随机从图像中选择一部分，然后降这部分图像裁剪出来，然后调整为原图像的大小</p></li><li><p>添加噪声： 过拟合通常发生在神经网络学习高频特征的时候 (因为低频特征神经网络很容易就可以学到，而高频特征只有在最后的时候才可以学到) 而这些特征对于神经网络所做的任务可能没有帮助，而且会对低频特征产生影响，为了消除高频特征我们随机加入噪声数据来消除这些特征。</p></li></ul><p><img src="/images/DA/1.png" title="Data Augmentation in CV vs NLP" /></p><p>相较于机器学习和计算机视觉领域，数据增强在NLP应用并没有前两者这么广泛。在NLP中，输入空间是离散的，我们需要关注如何生成有效的增广例子来捕获所需的不变性。因此它通常被比喻成“蛋糕上的樱桃”，只是提高有限的性能。</p><h1 id="主流技术">主流技术</h1><p>理想的DA技术应该既易于实现又能提高模型性能，但我们往往需要在这两者之间进行权衡。基于规则的技术很容易实现，但通常只能带来有限的性能改进。基于训练的模型的DA技术可能代价更大，但会引入更多的数据变化，导致更好的性能提升。为下游任务定制的基于模型的DA技术对性能有很强的影响，但很难开发和利用。</p><h2 id="rule-based-techniques">Rule-Based Techniques</h2><p>在特征空间内直接进行变化生成新的样本，比如在已知类别的数据之间进行“类比”转换，以扩充新的类；使用迭代的仿射变换和投影来沿着class-manifold最大限度地“拉伸”一个样本。</p><p>还有比如我们可以借助单词嵌入，如Word2Vec, GloVe, FastText, Sent2Vec，使用嵌入空间中最近邻的单词替换句子中的某个单词。 <img src="/images/DA/6.png" /></p><p>EASY DATA AUGMENTATION (EDA)：token-level的随机扰动操作，比如随机插入、删除、交换以及同义替换等。作者发现经过EDA后文本分类的准确率有了很大的提升。</p><p><img src="/images/DA/2.png" title="EDA" /></p><p>Unsupervised Data Augmentation (UDA)。一种基于无监督数据的数据增强方式，该方法通过对<span class="math inline">\((x, DA(x))\)</span>进行consistency training，生成无监督数据与原始无监督数据具备分布的一致性 <img src="/images/DA/3.png" title="UDA" /></p><p>在数据上构建带标记的图，将单个句子作为节点，配对的标签作为带标记的边。使用平衡理论和及物性从这个图中推断扩充句子对。 <img src="/images/DA/4.png" title="Graph Theory DA" /></p><p>Dependency tree morphing DA：受图像裁剪和旋转的启发，Şahin和Steedman提出了依赖树构建方法。对于带有依赖项注释的句子，可以交换或删除具有相同父节点的子节点。 <img src="/images/DA/5.png" title="Dependency tree morphing" /></p><h2 id="example-interpolation-techniques">Example Interpolation Techniques</h2><p>Mixed Sample Data Augmentation (MSDA) /MixUp。这种插值方法在图像处理领域应用非常广泛</p><p><span class="math inline">\(\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}, \quad\)</span> where <span class="math inline">\(x_{i}, x_{j}\)</span> are raw input vectors</p><p><span class="math inline">\(\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}, \quad\)</span> where <span class="math inline">\(y_{i}, y_{j}\)</span> are one-hot label encodings</p><p><img src="/images/DA/10.png" /></p><p>不同于MSDA这种连续插值，CUTMIX用从图像B中采样的一个patch替换图像A中的一个小的子区域，标签按照子区域大小的比例混合。对于NLP来说，例如涉及图像和文本的多模态问题可以借鉴这一工作的思想。</p><p>SEQ2MIXUP：（sequence-level variant of MixUp）对于sequence-to-sequence的模型，将输入/输出序列对进行一定的组合。 <span class="math display">\[\begin{array}{r}(\hat{X}, \hat{Y})=\left(m_{X} \odot X+\left(1-m_{X}\right) \odot X^{\prime}\right. \\\left.m_{Y} \odot Y+\left(1-m_{Y}\right) \odot Y^{\prime}\right)\end{array}\]</span></p><p><span class="math inline">\(m=\left[m_{X}, m_{Y}\right]\)</span>是一个系数向量。这种方法对于基于transformer的机器翻译、语义解析等任务都有明显的提升。</p><h2 id="model-based-techniques">Model-Based Techniques</h2><p>Seq2seq model以及language model都可以被用来做数据增强。</p><p>DiPS 原本是用于Diverse Paraphrasing（复述）任务的模型，该任务的度量标准为语义的相似性以及句子本身的差异性，但是我们也可以借助这些模型来进行数据增强。 <img src="/images/DA/8.png" title="DiPS during decoding to generate k paraphrases" /></p><p>类似的基于Transformer、BERT的模型从预训练的嵌入空间中，使用上下文敏感的、基于注意力的语义邻居混合来增强单词表示。</p><p><img src="/images/DA/7.png" title="综合对比图。Ext.Know是指DA方法是否需要外部知识(如WordNet)，如果需要预训练模型(如BERT)，则需要预训练。Preprocess表示需要进行预处理，Level表示数据被DA修改的深度，Task-Agnostic表示DA方法是否可以应用于不同的任务。Ext.Know、KWE、tok、const和dep分别代表外部知识、关键字提取、字符化、分组解析和依赖解析。" /></p><h1 id="应用">应用</h1><h2 id="low-resource-languages-few-shot-learning">Low-Resource Languages &amp;&amp; Few-Shot Learning</h2><p>低资源语言是DA的一个重要和具有挑战性的应用，尤其是神经机器翻译(NMT)。使用外部知识的技术很难将高资源语言用于低资源语言，特别是当它们具有相似的语言属性时。<br /><img src="/images/DA/9.png" title="Low-Resource Languages NMT" /> 如上图所示，我们可以借助一个high-resource language作为中枢来转化目标语言以及low-resource language。当high-resource language与low-resource language属于相同语系或具有类似性质时，我们将（HRL-ENG）数据集借助数据增强转化为（LRL-ENG） 数据集。对于其他任务，我们也可以通过DA来扩充low-resource language。</p><h2 id="mitigating-bias-fixing-class-imbalance">Mitigating Bias &amp;&amp; Fixing Class Imbalance</h2><p>以性别的偏差为例，可以借助DA来缓解性别偏见：创建一个与原始数据相同但偏向于未被充分代表的性别的增强数据集(使用实体的性别交换，如将“他”替换为“她”)来缓解引用解析中的性别偏见，并对这两个数据集进行联合训练。后续也有更好的工作比如缓解性别偏见的COUNTERFACTUAL DA (CDA)方法来打破性别词和中性词之间关联的因果干预。</p><p>类似也可以用于解决某些类别中的采样不足和采样过度问题。比如通过插值增强少数群体类的例子，以平衡多标签分类的分类。</p><h1 id="当前挑战和未来研究方向">当前挑战和未来研究方向</h1><p>应用到数据增强的NLP任务有很多，首先最基本的就是分类，这也是用来测试数据增强技术效果的基本方式之一。其他的任务包括摘要、问答、序列标注、Data-to-Text自然语言生成等以及多模态的任务如automatic speech recognition。</p><p>但是目前为止的研究存在以下方面的缺陷：</p><ul><li><p>明显缺乏关于DA为什么有效的研究。大多数研究都根据经验表明，以及基于一些实验表明DA技术是有效的，但目前很难在不诉诸于全面实验的情况下衡量技术的优劣。</p></li><li><p>多模态领域做数据增强。尽管多模态数据分析的工作有所增加，但许多研究都集中在单个模态或多个模态的扩展上。任需进一步探索诸如图像和文本同时增强的图像字幕方式。</p></li><li><p>基于范围的任务。例如，随机token替换可能是局部可接受的DA方法，但可能会破坏后面句子的共引用链。此时DA技术必须考虑文本中不同位置之间的依赖关系。</p></li><li><p>在专业领域工作，比如那些具有特定领域词汇和术语的领域(如医学)，许多预先训练的模型和外部知识不能被有效地使用。研究表明，当应用于特定领域数据时，DA变得不那么有效，这可能是因为增广数据的分布可能与原始数据有很大不同。</p></li><li><p>另一方面，针对数据增强所应对的少样本问题，我们还能从半监督学习角度考虑，结合数据增强与半监督学习技术是一个不错的选择。半监督学习能够充分利用大量未标注数据，同时能够使输入空间的变化更加平滑。</p></li></ul><h1 id="补充材料">补充材料</h1><p>一些关于NLP中数据增强方法具体操作的介绍： https://amitness.com/2020/05/data-augmentation-for-nlp/<br />NLP数据增强工具包：<a href="https://github.com/makcedward/nlpaug">nlpaug</a><br />类似的综述性文章：<br /><a href="https://arxiv.org/pdf/2105.11741.pdf">Data Augmentation Approaches in Natural Language Processing: A Survey</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;link: &lt;a href=&quot;https://arxiv.org/abs/2105.03075v4&quot; class=&quot;uri&quot;&gt;https://arxiv.org/abs/2105.03075v4&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;数据增强的背景&quot;&gt;数据增强的背景&lt;/h1&gt;
&lt;</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="Data_Aug" scheme="http://example.com/tags/Data-Aug/"/>
    
  </entry>
  
  <entry>
    <title>中文分词｜Chinese-word-segmentation (3)-基于词</title>
    <link href="http://example.com/2022/02/10/cws3/"/>
    <id>http://example.com/2022/02/10/cws3/</id>
    <published>2022-02-10T05:15:50.000Z</published>
    <updated>2022-02-15T12:23:59.232Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前情提要">前情提要</h1><p>在前面两篇Blog（<a href="/2022/02/07/cws/" title="中文分词1">中文分词1</a>、<a href="/2022/02/09/cws2/" title="中文分词2">中文分词2</a>）介绍了关于中文分词的基本方法以及近年来基于深度学习的部分分词模型，即基于字符的分词技术，接下来讲介绍基于词的中文分词模型。</p><h1 id="基本框架">基本框架</h1><p>Neural Word Segmentation Learning for Chinese</p><p><img src="/images/cws/11.png" title="neural network scoring model" /> 基于的框架如上图所示，不同于基于字符标注的模型，基于词的模型根据字符向量生成词向量。具体而言，模型得到输入的字符向量后，使用一个Gated Combination Neural Network，将L个字符向量组合，生成候选词向量： <span class="math display">\[\mathbf{w}=g\left(\mathbf{W}^{(L)}\left[\begin{array}{c}\mathbf{c}_{1} \\\vdots \\\mathbf{c}_{L}\end{array}\right]\right)\]</span></p><p><span class="math inline">\(\mathbf{W}^{(L)}\)</span>是一个所有词共享的权重。这GCNN中包含reset gate 和 update gate。</p><p><span class="math display">\[\mathbf{w}=\mathbf{z}_{N} \odot \hat{\mathbf{w}}+\sum_{i=1}^{L} \mathbf{z}_{i} \odot \mathbf{c}_{i}\]</span></p><p><span class="math display">\[\hat{\mathbf{w}}=\tanh \left(\mathbf{W}^{(L)}\left[\begin{array}{c}\mathbf{r}_{1} \odot \mathbf{c}_{1} \\\vdots \\\mathbf{r}_{L} \odot \mathbf{c}_{L}\end{array}\right]\right)\]</span></p><p><span class="math inline">\(\mathbf{W}^{(L)} \in \mathbb{R}^{d \times L d}\)</span> 和 <span class="math inline">\(\mathbf{r}_{i} \in \mathbb{R}^{d}(1 \leq i \leq L)\)</span> 表示reset gates，用来决定字符向量的哪部分被结合到词向量中： <span class="math display">\[\left[\begin{array}{c}\mathbf{r}_{1} \\\vdots \\\mathbf{r}_{L}\end{array}\right]=\sigma\left(\mathbf{R}^{(L)}\left[\begin{array}{c}\mathbf{c}_{1} \\\vdots \\\mathbf{c}_{L}\end{array}\right]\right)\]</span> 而update gate则是 <span class="math display">\[\left[\begin{array}{c}\mathbf{z}_{N} \\\mathbf{z}_{1} \\\vdots \\\mathbf{z}_{L}\end{array}\right]=\exp \left(\mathbf{U}^{(L)}\left[\begin{array}{c}\hat{\mathbf{w}} \\\mathbf{c}_{1} \\\vdots \\\mathbf{c}_{L}\end{array}\right]\right) \odot\left[\begin{array}{c}1 / \mathbf{Z} \\1 / \mathbf{Z} \\\vdots \\1 / \mathbf{Z}\end{array}\right]\]</span> 其中<span class="math inline">\(\mathbf{U}^{(L)} \in \mathbb{R}^{(L+1) d \times(L+1) d}\)</span> 是系数矩阵，<span class="math inline">\(\mathbf{Z} \in \mathbb{R}^{d}\)</span>是归一化向量。</p><p>得到候选词向量之后，每个输入句子中的词向量会被转化为一个分数word_score，代表这个词有多大的可能性是一个真实存在的词。这些word_score会与经过LSTM之后的词向量共同组合成sentence score。</p><p>得到分数之后，之前多数序列标注的分词方法多使用Viterbi算法进行动态规划，得到最优的分词方法，但是在这个模型中，由于可能的句子分词方式总数太大，且为了捕捉完整的分割决策（不同于基于字符标注的方法），模型使用了Beam Search算法作为Decoder。</p><h1 id="改进模型">改进模型</h1><p>Fast and Accurate Neural Word Segmentation for Chinese</p><p>首先，这篇文章的作者对字符生成候选词向量的GCNN网络进行了改进。模型引入了一个高频词词典，在词典中则直接对character embedding进行average pooling，而不在词典中则根据构成的字符向量生成词向量，如下图所示 <img src="/images/cws/13.png" /> <span class="math display">\[\operatorname{COMP}\left(c_{1} . . c_{l}\right)=\tanh \left(\mathbf{W}_{l}^{c}\left[\mathbf{r}_{1} \odot \mathbf{c}_{1} ; \ldots ; \mathbf{r}_{l} \odot \mathbf{c}_{l}\right]+\mathbf{b}_{l}^{c}\right)\]</span> <span class="math display">\[\left[\mathbf{r}_{1} ; \ldots ; \mathbf{r}_{l}\right]=\operatorname{sigmoid}\left(\mathbf{W}_{l}^{r}\left[\mathbf{c}_{1} ; \ldots ; \mathbf{c}_{l}\right]+\mathbf{b}_{l}^{r}\right)\]</span> 作者讲gate mechanism进行了简化，使得模型训练更加快速。另外，原模型中的Beam Search也改为了贪心算法，采用了两种训练方法：Early update、LaSO update。Early update指的是一旦最优的分割无法实现，就立即更新。Early update的一个缺点是，搜索可能永远不会到达训练样本的末尾，这意味着数据的其余部分是“浪费”的。而LaSO update在每次更新后都在相同的实例上继续进行正确的假设（将正确的分词序列的对应前缀插入实例中）。</p><p>Transition-Based Neural Word Segmentation</p><p><img src="/images/cws/14.png" title="Transition-Based Neural Model" /> 上图为Transition-Based分词方法，它使用Transition system递增地去分词。Buffer部分用于存储句子已经分词的部分，未分词的在Queue中。action包括SEP-separate和APP-append，分别指分割和把字符pop到Buffer中。模型分词的过程就是寻找一个action序列的过程。</p><p>这个框架中的基本的特征包含三方面的信息。第一个信息是序列q中第一个字符和buffer中的最后一个字符用来分别给SEP和APP动作来打分。第二个信息是通过已经被识别的词来指导SEP。第三个信息是已识别的词的相关信息，比如它们的长度，这个词中的第一个字符或是最后一个字符可以作为额外的特征。从图中可以看到三个RNN分别编码word sequence、character sequence以及action sequence信息。而论文的作者则在这一框架的基础上用LSTM替换了原有的RNN。</p><h1 id="pre-train-model">Pre-train model</h1><p>BERT诞生后横扫各大NLP任务的榜单，中文分词自然也逃不了它的魔爪。BERT作为特征提取器，基于BERT的分词模型事实上大部分属于基于字符的分词技术。而基于BERT的分词模型的关注点则主要包括：在 模型中融合自定义词典、外部知识（领域知识）；如何大模型蒸馏成一个小的模型来提高分词性能；如何通过不同粒度标准的分词预料联合预训练，让分词能够通过某些简单的控制能够适应不用的分词场景。</p><p>部分相关论文：<br />Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter<br />Pre-training with Meta Learning for Chinese Word Segmentation<br />ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations （这个模型缩写就离谱）</p><p><img src="/images/cws/12.png" title="ZEN：Bert+N-gram" /></p><h1 id="写在最后">写在最后</h1><p><img src="/images/cws/15.png" title="paperwithcode上中文分词任务" /> 目前在几个公开数据集上分词模型的分词准确率都达到了97%以上。作为一个对于中文NLP来说非常重要的任务，中文分词可以说基于属于已经解决的任务，毕竟分词本就很难有绝对统一的标准。而就目前来说，相较于刷新榜单，或许更重要的是寻找更好解决OOV问题的方法（毕竟每段时间都有大量新词涌现）以及研发速度更快、体量更小的模型。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前情提要&quot;&gt;前情提要&lt;/h1&gt;
&lt;p&gt;在前面两篇Blog（&lt;a href=&quot;/2022/02/07/cws/&quot; title=&quot;中文分词1&quot;&gt;中文分词1&lt;/a&gt;、&lt;a href=&quot;/2022/02/09/cws2/&quot; title=&quot;中文分词2&quot;&gt;中文分词2&lt;/a&gt;）</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>中文分词｜Chinese-word-segmentation (2)-基于字符</title>
    <link href="http://example.com/2022/02/09/cws2/"/>
    <id>http://example.com/2022/02/09/cws2/</id>
    <published>2022-02-09T09:15:47.000Z</published>
    <updated>2022-02-13T16:37:31.413Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">背景</h1><p>随着深度学习技术的发展，基于深度神经网络的中文分词模型也不断涌现，这类分词技术大致上可以分为两类，其中一类基于字符的中文分词方法将分词看作字符的标注问题。根据字在词中的位置，分别给每个中文字符打上B、M、E、S的标签，将分词转化为字的标签预测。</p><h1 id="深度神经网络">深度神经网络</h1><p>Deep Learning for Chinese Word Segmentation and POS Tagging</p><p><img src="/images/cws/5.png" title="模型框架" /> 这篇论文是早期使用神经网络来做中文分词和词性标注的联合任务。对比机器学习模型，早期的神经网络致力于改善手工设计或选择特征这一问题，借助深层网络自动获取任务相关的字符特征。作为统计语言模型，这种方法利用大规模非标注数据来改善中文字符的内在表示，然后使用这些改善后的表示来提高有监督的分词模型和词性标注模型的性能。</p><p>对于输入句子中的每一个字，神经网络架构将为其可能的每一个TAG进行评分，为了解决不同句子对应的字序列长短不一的问题，本文中采用的是窗口方法。窗口方法假定一个字的tag主要依赖于与其相邻的字。具体而言，如上图所示，首先对输入的句子中的每个字查词典：通过lookup层得到窗口中每个字的字向量。之后将每个窗口长度的字向量首尾相连得到一个新的特征。接下来经过3层基础的神经网络。神经网络的输出是一个包含每个字可能标签的得分的矩阵。最后使用Viterbi算法进行动态规划完成标注的推断。</p><h1 id="lstm">LSTM</h1><p>Long Short-Term Memory Neural Networks for Chinese Word Segmentation</p><p>基于DNN的分词模型所能关注的是每个字符的窗口内邻近字符的特征，而LSTM则能解决句子内字符的长期依存关系问题。举个简单的例子：<br />冬天，能穿/多少/穿/多少，夏天，能穿/多/少/穿/多/少。<br />此时这个“多少”的分词就需要根据句子开头的“冬天”和“夏天”来确定。 <img src="/images/cws/6.png" title="LSTM进行分词" /></p><p>因此模型在原有深度学习分词的框架下，将传统的3层神经网络改为LSTM，借助输入输出门和遗忘门来传递上文信息。</p><p><span class="math display">\[\begin{aligned}\mathbf{i}^{(t)} &amp;=\sigma\left(\mathbf{W}_{i x} \mathbf{x}^{(t)}+\mathbf{W}_{i h} \mathbf{h}^{(t-1)}+\mathbf{W}_{i c} \mathbf{c}^{(t-1)}\right) \\\mathbf{f}^{(t)} &amp;=\sigma\left(\mathbf{W}_{f x} \mathbf{x}^{(t)}+\mathbf{W}_{f h} \mathbf{h}^{(t-1)}+\mathbf{W}_{f c} \mathbf{c}^{(t-1)}\right) \\\mathbf{c}^{(t)} &amp;=\mathbf{f}^{(t)} \odot \mathbf{c}^{(t-1)}+\mathbf{i}^{(t)} \odot \phi\left(\mathbf{W}_{c x} \mathbf{x}^{(t)}+\mathbf{W}_{c h} \mathbf{h}^{(t-1)}\right) \\\mathbf{o}^{(t)} &amp;=\sigma\left(\mathbf{W}_{o x} \mathbf{x}^{(t)}+\mathbf{W}_{o h} \mathbf{h}^{(t-1)}+\mathbf{W}_{o c} \mathbf{c}^{(t)}\right) \\\mathbf{h}^{(t)} &amp;=\mathbf{o}^{(t)} \odot \phi\left(\mathbf{c}^{(t)}\right)\end{aligned}\]</span></p><p>但是在这个模型中，我们可以发现一个明显的问题，模型只能关注字符上距离的上文内容而无法获取下文信息用于辅助分词。针对这一问题，就有了Bi-LSTM，用双向LSTM来利用上下文信息。</p><p>Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation</p><p>这篇论文使用了双向LSTM，即两个并行的LSTM分别来从左到右和从右到左来提取长距离字符特征。另外，模型输出为softmax之后的概率向量，比起之前的工作省略了viterbi推断的过程，变为end-end的模型。 <img src="/images/cws/7.png" title="Bi-LSTM进行分词" /></p><h1 id="multi-criteria-learning">Multi-Criteria Learning</h1><p>Adversarial Multi-Criteria Learning for Chinese Word Segmentation</p><p>不同的语料库有不同的分词标准，这篇文章的作者试图借助对多个不同标准的分词语料进行模型训练，提取中分词方法中最具普适性的部分。具体而言，模型借助Multi-criteria learning来获取多个准则的共享权重和独有权重，另一方面，通过鉴别器的对抗性训练来更好的实现这一过程，使得多准则的共享特性被更好地提取。</p><p>整体模型架构是建立在Bi—LSTM模型下，而针对具体的特征提取网络，作者针对多准则学习设计了三种网络结构，如下图所示 <img src="/images/cws/9.png" /> 其中黄色为共享LSTM层，灰色为私有LSTM层，<span class="math inline">\(\Theta^{m}, \Theta^{s}\)</span>分别代表他们的参数。每种模型都有两种准则进行训练，训练的目标函数是所有语料库上数据的条件似然。</p><p><span class="math display">\[\mathcal{J}_{s e g}\left(\Theta^{m}, \Theta^{s}\right)=\sum_{m=1}^{M} \sum_{i=1}^{N_{m}} \log p\left(Y_{i}^{(m)} \mid X_{i}^{(m)} ; \Theta^{m}, \Theta^{s}\right)\]</span></p><p>而为了确保共享层中没有参杂特定准则的私有信息，模型引入了一个鉴别器进行对抗训练。 <img src="/images/cws/8.png" title="对抗训练（以Model-III为例）" /> 判别器的任务是预测某一特征向量来源于 多准则语料中的哪一个。假如判别器能够准确预测每一个共享特征向量的来源语料，则说明这些共享特征中混入了太多私有信息。而共享LSTM层的目标则是让判别器无法鉴别输出的特征向量来源于哪个预料。模型通过令这两者进行对抗性训练，使得共享层能够提取出多个准则中最本质的分词特性。</p><h1 id="joint-cws-and-pos-tagging">Joint CWS and POS Tagging</h1><p>A Feature-Enriched Neural Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</p><p>本文的模型将Chinese word segmentation与part-of-speech (POS) tagging两个任务进行联合训练，因为其本质上都属于character based sequence labeling task。而论文作者的改进之处在于一个精心设计的特征提取网络 Feature-Enriched Neural Model。</p><p><img src="/images/cws/10.png" title="Feature-Enriched Neural Model" /></p><p>抛开原有的框架，这个模型增加了2个模块——Convolutional layer; Highway layer。</p><p>首先，卷积层用来对标传统方法中的手工字符特征部分。作者认为简单的神经模型只是将字符的局部信息嵌入并连接起来，不能模拟传统机器学习模型中精心设计的特征。为了像传统的基于特征的模型那样更好地建模复杂的字符特征，作者使用卷积层对每个特征分别建模不同的n-gram特征并串联，然后我们k-max池化层来选择最显著的部分。</p><p><span class="math inline">\(\hat{\mathbf{z}}_{i}^{q}\)</span>代表Q-gram特征 (uni-gram, bi-gram, ... ,)，而<span class="math inline">\(\mathbf{W}_{c o v}^{q} \in \mathbf{R}^{q d \times l_{q}}\)</span> 代表convolutional filter</p><p><span class="math display">\[\hat{\mathbf{z}}_{i}^{q}=\tanh \left(\mathbf{W}_{\operatorname{cov}}^{q}{ }^{\top} \times \mathbf{x}_{i-\left|\frac{q-1}{2}\right|: i+\left[\frac{q-1}{2}\right]}^{+}+\mathbf{b}\right), i \in[1, n]\]</span></p><p>而后分别进行concatenation、k-max pooling操作 <span class="math display">\[\mathbf{z}_{i}=\oplus_{q=1}^{Q} \hat{\mathbf{z}}_{i}\]</span></p><p>此时，输入的原始句子就会被表示为<span class="math inline">\(\hat{\mathbf{X}} \in \mathbf{R}^{n \times d}=\)</span> <span class="math inline">\(\left[\hat{\mathbf{x}}_{1}, \hat{\mathbf{x}}_{2}, \ldots, \hat{\mathbf{x}}_{n}\right]^{\top}\)</span>, 其中<span class="math inline">\(\hat{\mathbf{x}}_{i}\)</span>为:</p><p><span class="math display">\[{\hat{\mathbf{x}}}_{i}=k \max \mathbf{z}_{i}, k=d .\]</span></p><p>Highway layer则用来增加结构的深度，来模拟更复杂的组合特征。此外，highway加快了模型的收敛速度，缓解了梯度消失的问题。（这部分是参考Highway Netowrk（2015）的模型）</p><p>我们讲卷积后的句子表示为 <span class="math inline">\(\hat{\mathbf{X}}=\operatorname{Cov}(\mathbf{X})\)</span>，而其经过Highway layer后会得到 <span class="math display">\[\hat{\mathbf{X}}=\operatorname{Cov}(\mathbf{X}) \odot T(\mathbf{X})+\mathbf{X} \odot C(\mathbf{X})\]</span> 其中<span class="math inline">\(\odot\)</span>代表按元素乘运算，<span class="math inline">\(C(\cdot)=1-T(\cdot)\)</span>，而<span class="math inline">\(T(\cdot)\)</span>则可以写为： <span class="math display">\[T(\mathbf{X})=\sigma\left(\mathbf{W}_{T}{ }^{\boldsymbol{\top}} \times \mathbf{X}+\mathbf{b}_{T}\right)\]</span> 其中<span class="math inline">\(\mathbf{W}_{T} \in \mathbf{R}^{d \times d}\)</span>、<span class="math inline">\(\mathbf{b}_{T} \in \mathbf{R}^{d}\)</span>是可训练参数，<span class="math inline">\(\sigma\)</span>是sigmoid函数。</p><p>而接下来就将结果输入到BLSTM中，并借助CRF作为Decoder得到最终的结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;
&lt;p&gt;随着深度学习技术的发展，基于深度神经网络的中文分词模型也不断涌现，这类分词技术大致上可以分为两类，其中一类基于字符的中文分词方法将分词看作字符的标注问题。根据字在词中的位置，分别给每个中文字符打上B、M、E、S的标签，将分词转化为字的</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>中文分词｜Chinese-word-segmentation (1)</title>
    <link href="http://example.com/2022/02/07/cws/"/>
    <id>http://example.com/2022/02/07/cws/</id>
    <published>2022-02-07T13:01:01.000Z</published>
    <updated>2022-02-09T15:42:51.014Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">背景</h1><p>中文文本中词与词之间没有明确的分割标记，而是以连续字符串形式呈现。所以，任何中文自然语言处理任务都必须解决中文序列切分的问题——中文分词。当然对于英文等语言来说自带分隔符，不需要分词。但在英语手写字识别时，由于分隔符没有那么明显，因此也需要使用类似中文分词的技术。</p><p>事实上不同的人对于同一个句子的分词结果并不一定相同，因此将机器分词的结果与人工分词的结果进行比对时，一个98%准确率的分词器与一个99%的分词器很难比较谁效果更好。在中文自然语言处理任务中，诸如机器翻译，自动问答，语音识别等任务，都需要分词技术做支撑。但是对于不同的子任务，分词的细粒度要求也不一样，尤其是对于一些复合词。比如在机器翻译时，清华大学就应该以整个“清华大学”而不是“清华-大学”。可以说中文分词在工程上是一项需要与任务要求紧密结合的技术。</p><p>中文分词存在两个难点，一是歧义，二是未登录词（OOV-out of vocabulary）。语言的歧义性一直伴随着语言学的发展进程，也自然限制着NLP技术。另一方面，词典的选择诸如词典中的复合词选择和词典的大小也影响着中文分词任务。举个直观的例子，对于Baidu搜索引擎的分词模型来说，基于人民日报的词典和基于用户搜索数据的词典得到的模型表现就会有很大差异。</p><h1 id="分词方法的演变">分词方法的演变</h1><h2 id="基于匹配的词典分词">基于匹配的词典分词</h2><p>基于匹配的词典分词是非常自然的想法，我们根据词典扫描一个句子，遇到词典中出现过的词就进行分割，这类方法又被称为机械分词。这类方法简单易实现，而且能取得不错的效果，其主要问题包括如何构建一个完备的词典、如何设计高效的匹配算法、匹配中出现的歧义切分。<br />常见的匹配算法包括:</p><ul><li>正向最大匹配法</li><li>逆向最大匹配法</li><li>双向最大匹配法</li><li>最少切分</li><li>......</li></ul><p>最大匹配法从句子中寻找长词条进行查字典，若查不到则去掉最后一个字直到找到为止。其中双向最大匹配分别从左到右和从右到左进行两次扫描。最小切分则是寻找使每一个句子切出的词数量最少。</p><h2 id="基于标注的机器学习算法">基于标注的机器学习算法</h2><p>不同于基于匹配的机械分词，基于统计语言模型的分词技术有效提高了分词的准确率。其中基于标注的机器学习算法将中文分词转化为字序列标注问题。，B表示开始位置、M表示中间位置、E表示结束位置及S表示单字构词。机器学习算法 需要人工设计特征模板，指定窗口的大小。由于算法的复杂度以及对分词结果准确度要求等原因，窗口大小一般不超过5。下面介绍几个具有代表性的模型：</p><ul><li><p>隐马尔可夫模型（HMM）隐马尔可夫不是一个复杂的数学模型，但能解决大多数自然语言处理问题。其基本的思想是根据观测值序列找到隐状态值序列。在中文分词中，一段文字的每个字符可以看作是一个观测值，而这个字符的位置标签（BEMS）可以看作是隐状态。使用HMM的分词，通过对切分语料库进行统计，可以得到模型中5大要要素：起始概率矩阵，转移概率矩阵，发射概率矩阵，观察值集合，状态值集合。有了三个矩阵和两个集合后，HMM问题最终转化成求解隐藏状态序列最大值的问题，求解这个问题最常使用的是Viterbi算法。</p></li><li><p>最大熵马尔可夫模型（MEMM）把HMM模型和maximum-entropy模型的优点集合程一个产生式模型，这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度。</p></li><li><p>条件随机场（CRF）是用来标注和划分结构数据的概率化结构模型。和HMM类似，当对于给定的输入观测序列<span class="math inline">\(X\)</span>和输出序列<span class="math inline">\(Y\)</span>，CRF通过定义条件概率<span class="math inline">\(P(Y|X)\)</span>，而不是联合概率分布<span class="math inline">\(P(X,Y)\)</span>来描述模型。MEMM模型对每个节点进行独立归一化，存在偏置问题。条件随机场(CRF)结合了多方面优势，对所有特征进行全局归一化，避免了偏置问题，成为传统机器学习中应用最多、最具代表性的模型算法之一。条件随机场能够获得更高的分词准确率，但模型复杂导致分词效率略低。</p></li></ul><h2 id="基于理解的深度学习算法">基于理解的深度学习算法</h2><p>深度学习模型诸如CNN、GRU、LSTM、BiLSTM被引入中文分词，相对于机器学习而言，深度学习算法无需人工进行特征选择。在基础深度学习模型的基础上，有效结合预训练和后处理方式已成为深度学习的一种趋势，一般性流程如下图所示。 <img src="/images/cws/1.png" title="基于深度学习的中文分词" /> 预训练既可以根据领域需要和任务特点进行预训练，也可以直接使用现有的预训练结果进行微调。中文分词预训练的基本单位是词(字)的语义、偏旁、拼音和输人法等。语义表示的预训练模型包括与上下文无关的静态词向量训练模型Word2Vec、Glove以及与上下文相关的动态词向量训练模型ELMo、BERT、XLNet等。<br />近几年的中文分词主要分为两类，一个是基于字符的中文分词（根据字所在词的位置，对每个字打上标签），一类是基于词的中文分词。</p><h1 id="分词工具">分词工具</h1><h2 id="jieba">jieba</h2><p>jieba库是一个简单实用的中文自然语言处理分词库，属于概率语言模型分词。<br />jieba自带一个dict.txt的词典, 里面有2万多条词, 包含了词条出现的次数和词性。将句子根据给定的词典进行查词典操作, 生成所有可能的句子切分，而后根据动态规划查找最大概率路径, 找出基于词频的最大切分组合。对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。<br />（jieba分词对“自然语言处理”的分词结果为 自然语言｜处理）</p><h2 id="ltp">LTP</h2><p>LTP是哈工大开源的一套中文语言处理系统，涵盖了基本功能：分词、词性标注、命名实体识别、依存句法分析、语义角色标注、语义依存分析等。LTP基于结构化感知器（Structured Perceptron,SP）属于基于字符的分词模型，以最大熵准则建模标注序列<span class="math inline">\(Y\)</span>在输入序列<span class="math inline">\(X\)</span>的情况下的score函数，分词结果则等同于最大score函数所对应的标注序列。 <span class="math display">\[S(Y, X)=\sum_{s} \alpha_{s} \Phi_{s}(Y, X)\]</span> <span class="math inline">\(\Phi_{s}(Y, X)\)</span>为特征函数，分词流程为先提取字符特征，计算特征权重值，然后Viterbi解码。</p><h2 id="thula">THULA</h2><p>THULA（THU Lexical Analyzer for Chinese）为清华大学推出的中文词法分析工具包，具有中文分词和词性标注功能，其原理与LTP非常相似，在字符特征选择方面有所不同。<br />测试发现THULA没有针对python3.8进行维护，因此只支持3.7-版本。</p><h2 id="stanford-corenlp">Stanford CoreNLP</h2><p>CoreNLP的中文分词基于CRF模型： <span class="math display">\[P_{w}(y \mid x)=\frac{\exp \left(\sum_{i} w_{i} f_{i}(x, y)\right)}{Z_{w}(x)}\]</span></p><p><span class="math inline">\(f_{i}(x, y)\)</span>为特征函数，<span class="math inline">\(w\)</span>为模型参数。不同于其他分词器采用B、M、E、S四种label来做分词，CoreNLP的中文分词label只有两种，“1”表示当前字符与前一字符连接成词，“0”则表示当前字符为另一词的开始——换言之前一字符为上一个词的结尾。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;
&lt;p&gt;中文文本中词与词之间没有明确的分割标记，而是以连续字符串形式呈现。所以，任何中文自然语言处理任务都必须解决中文序列切分的问题——中文分词。当然对于英文等语言来说自带分隔符，不需要分词。但在英语手写字识别时，由于分隔符没有那么明显，因此</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>Improving Chinese Word Segmentation with Wordhood Memory Networks [ACL20]</title>
    <link href="http://example.com/2022/02/05/pd12/"/>
    <id>http://example.com/2022/02/05/pd12/</id>
    <published>2022-02-05T15:41:35.000Z</published>
    <updated>2022-02-09T16:35:42.372Z</updated>
    
    <content type="html"><![CDATA[<p>link: <a href="https://aclanthology.org/2020.acl-main.734/" class="uri">https://aclanthology.org/2020.acl-main.734/</a><br /><a href="https://github.com/SVAIGBA/WMSeg">代码</a></p><h1 id="背景">背景</h1><p>在中文自然语言处理中，分词是一个非常重要的任务。中文分词技术存在两个主要难点:未登录词（OOV）和歧义消除问题。本文属于基于字符的分词模型，主要思想是利用键值记忆网络来辅助分词，使分词的语义更加完整。</p><h1 id="模型框架">模型框架</h1><p>模型将中文分词作为序列标注问题，核心思想是在传统NER模型的Encoder-Decoder之间添加一个memory network，Encoder可以用BERT或BiLSTM等将汉字序列表示为向量，Decoder可以是Softmax或者CRF。模型总体可以表示为： <span class="math display">\[\widehat{\mathcal{Y}}=\underset{\mathcal{Y} \in \mathcal{T}^{l}}{\arg \max } p(\mathcal{Y} \mid \mathcal{X}, \mathcal{M}(\mathcal{X}, \mathcal{N}))\]</span> 其中<span class="math inline">\(\mathcal{T}\)</span>表示句子中所有分词结果的标签集;<span class="math inline">\(l\)</span>是句子长度。<span class="math inline">\(\widehat{\mathcal{Y}}\)</span>为该模型得到的最佳结果，<span class="math inline">\(\mathcal{N}\)</span>为构造的词典，<span class="math inline">\(\mathcal{X}\)</span>为输入句，<span class="math inline">\(\mathcal{M}\)</span>为本文提出的模型。</p><p><img src="/images/cws/2.png" title="The architecture of $WMS_{EG}$." /></p><h2 id="词典构建">词典构建</h2><p>本文构建的词典实际上是一个N-gram词典，它包含了一个句子中所有可能的N-gram。模型利用前人的模型——Accessor Variety，找出输入句子中所有可能的n-gram集合。根据上图给出的例子，所构建的词典如图底部所示。</p><h2 id="wordhood-memory-networks">Wordhood Memory Networks</h2><p>这部分是本文最重要的部分。作者利用键值记忆网络将字符n-gram与它们的词伙（wordhood）度量相结合。其中key-value分别对应n-grams和wordhood。具体可以分成两个步骤：</p><ul><li><p>Key Addressing<br />首先对该句子构建Lexicon，对每一个汉字<span class="math inline">\(x_i\)</span> ，有可能存在很多包含该汉字的n-gram。比如上面的句子中的第四个字&quot;民&quot;构建Lexicon，可以表示为： <span class="math display">\[K_{4}=[\text {&quot;民&quot;,&quot; 居民&quot;, &quot; 民生&quot;, &quot; 居民生活&quot;] }\]</span>然后将这些n-gram进行key embeding后<span class="math inline">\(e_{i, j}^{k}\)</span>再与Encoder传来的<span class="math inline">\(h_i\)</span> 相乘之后做softmax得到一个概率分布。概率大小就表明了相关程度： <span class="math display">\[p_{i, j}=\frac{\exp \left(h_{i} \cdot e_{i, j}^{k}\right)}{\sum_{j=1}^{m_{i}} \exp \left(h_{i} \cdot e_{i, j}^{k}\right)}\]</span></p></li><li><p>Value Reading<br />先将每个<span class="math inline">\(k_i\)</span>映射到一个标注值V上，因为每个字在不同的n-gram中的位置不同，所以需要映射的值也不同，这里使用B I E S标记法：(B:begin ，I:inside，E:end，S:single)，还是上面的例子，对应Key Addressing时的<span class="math inline">\(K_4\)</span>，得到的value集合为： <span class="math display">\[V_{4}=\left[V_{S}, V_{E}, V_{B}, V_{I}\right]\]</span>而后将每个value进行embedding得到<span class="math inline">\(e_{i, j}^{v}\)</span>，与上一步得到的概率进行相乘累加，得到的结果再与Encoder传入的<span class="math inline">\(h_i\)</span>结合得到最终输出的vector <span class="math inline">\(a_i\)</span>。 <span class="math display">\[O_{i}=\sum_{j=1}^{m_{i}} p_{i, j} e_{i, j}^{v}\]</span></p></li></ul><p>接下来就可以和正常的NER模型一样使用一个decoder（softmax、crf等等） 得到一个句子的分词结果了。</p><h1 id="实验">实验</h1><p>1.消融实验。作者使用了5个基准数据集，分别是CTB6、MSR、PKU、AS和CITYU。将Wordhood Memory Networks加入到目前主流的分词模型中进行比较，实验结果表明，虽然原始的模型有很好的性能，但加入Wordhood Memory Networks后仍有很大的改进<br />2.在五个基准数据集的测试集上，WMSEG和以前的SOTA模型的性能比较。经过实验比较，本文提出的模型在中文分词方面达到了SOTA水平，OOV的召回率有了很大的提高。</p><p><img src="/images/cws/3.png" title="Ablation experiments." /></p><h1 id="例子分析">例子分析</h1><p>作者用“他从小学习电脑技术”这一句子的分词结果进行可视化分析，发现通过Wordhood Memory Networks对n-gram语义的捕捉，给了“从小”一个较高的权重、&quot;小学&quot;一个较低的权重，得到了正确的分词结果。 <img src="/images/cws/4.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;link: &lt;a href=&quot;https://aclanthology.org/2020.acl-main.734/&quot; class=&quot;uri&quot;&gt;https://aclanthology.org/2020.acl-main.734/&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;ht</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>Graph Wavelet Neural Network[ICLR2019]</title>
    <link href="http://example.com/2022/01/26/pd11-1/"/>
    <id>http://example.com/2022/01/26/pd11-1/</id>
    <published>2022-01-26T04:20:13.000Z</published>
    <updated>2022-01-28T08:22:30.110Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1904.07785.pdf">Graph Wavelet Neural Network</a><br />Code: <a href="https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork">link</a></p><p>基于小波变换的图卷积神经网络，其背后的思想及理论基础在这一篇论文中无法很详尽的阐述。不过本篇论文相当于把一套传统方法迁移到图上。</p><h1 id="intro">Intro</h1><p>作者提出了graph wavelet neural network (GWNN)，对于GCN基于传统的图傅里叶变换，GWNN利用graph wavelet transform，用图小波代替图拉普拉斯特征向量作为一组基，利用小波变换和卷积定理定义了卷积算子。使得计算更高效，且在Cora, Citeseer和Pubmed三个图的半监督分类数据集上取得了更好的表现。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.07785.pdf&quot;&gt;Graph Wavelet Neural Network&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;https://github.com/benedekrozembercz</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graphite-Iterative Generative Modeling of Graphs [ICML2019]</title>
    <link href="http://example.com/2022/01/18/pd11/"/>
    <id>http://example.com/2022/01/18/pd11/</id>
    <published>2022-01-18T13:47:01.000Z</published>
    <updated>2022-01-20T01:53:00.164Z</updated>
    
    <content type="html"><![CDATA[<p>ICML2019的论文，作者为Stanford的phd。</p><h1 id="先说几句">先说几句</h1><p>看到摘要</p><blockquote><p>Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding.</p></blockquote><p>基本可以确定模型相当于variational graph autoencoders，但是<a href="/2022/01/18/pd10/" title="VGAE">VGAE</a>这名字已经被人抢了XD。不过对比VGAE，它用了更优的Decoder。另一方面，作者给出了图神经网络中message passing与平均场变分推理之间的理论联系。</p><h1 id="graphite">Graphite</h1><p><img src="/images/pd10/8.png" /> 模型的整体架构与VGAE相似。其中Encoder为 <span class="math display">\[\boldsymbol{\mu}, \boldsymbol{\sigma}=\mathrm{GNN}_{\phi}(\mathbf{A}, \mathbf{X})\]</span> 接下来我们重点来看模型的Decoder。在VGAE中，Decoder是节点隐变量的内积，而Graphite提出了reverse message passing作为Decoder。 <span class="math display">\[\begin{aligned}\widehat{\mathbf{A}} &amp;=\frac{\mathbf{Z Z}^{T}}{\|\mathbf{Z}\|^{2}}+\mathbf{1 1}^{T} \\\mathbf{Z}^{*} &amp;=\operatorname{GNN}_{\theta}(\widehat{\mathbf{A}},[\mathbf{Z} \mid \mathbf{X}])\end{aligned}\]</span> 模型不断迭代上述两个步骤，也就是reverse message passing。 首先第一步借助隐变量矩阵的内积构建一个邻接矩阵（图）<span class="math inline">\(\widehat{\mathbf{A}} \in \mathbb{R}^{n \times n}\)</span>，加上单位矩阵保证非负。第二步中先将Z和X进行级联，而后与构建的图输入到GNN中。通过不断的重复来更新<span class="math inline">\(Z^*\)</span>，最后使用最终的Z进行内积得到生成的邻接矩阵<span class="math inline">\(\hat A\)</span>。另外，由于图学习通常是在大规模图上操作，在迭代过程中的求内积可以借助GNN这一步中的矩阵乘法来降低复杂度。</p><p><img src="/images/pd10/9.png" title="实验结果" /></p><h1 id="theoretical-analysis">Theoretical Analysis</h1><p>这一部分是作者对图神经网络的message passing与近似推断的关系。<br />首先我们定义kennel，<span class="math inline">\(K: \mathcal{Z} \times \mathcal{Z} \rightarrow \mathbb{R}\)</span>；映射函数 <span class="math inline">\(T_{\psi}: \mathcal{P} \rightarrow \mathcal{H}\)</span> 其中<span class="math inline">\(\mathcal{P}\)</span>定义了<span class="math inline">\(\mathcal{Z}\)</span>上所有分布的空间。因此可以定义对变量Z的分布的映射，或者成为kernel embedding： <span class="math display">\[T_{\psi}(p):=\mathbb{E}_{Z \sim p}[\psi(Z)]\]</span> 我们令这种映射<span class="math inline">\(\psi\)</span>是单射的，即对于任意两个分布<span class="math inline">\(p_1,p_2\)</span>，当<span class="math inline">\(p_{1} \neq p_{2}\)</span>，有<span class="math inline">\(T_{\psi}\left(p_{1}\right) \neq T_{\psi}\left(p_{2}\right)\)</span>。接下来我们定义 函数<span class="math inline">\(\mathcal{O}: \mathcal{P} \rightarrow \mathbb{R}^{d}, d \in \mathbb{N}\)</span>，对于每个<span class="math inline">\(T_{\psi}\)</span>和<span class="math inline">\(\mathcal{O}\)</span>，都存在<span class="math inline">\(\tilde{\mathcal{O}}_{\psi}: \mathcal{H} \rightarrow \mathbb{R}^{d}\)</span>使得: <span class="math display">\[\mathcal{O}(p)=\tilde{\mathcal{O}}_{\psi}\left(T_{\psi}(p)\right) \quad \forall p \in \mathcal{P}.\]</span></p><p>在GNN中，我们假设<span class="math inline">\(X\)</span>和<span class="math inline">\(A\)</span>都是观测到且在隐变量的条件概率分布中的相互独立的。也就是说我们期望的图是满足，对于<span class="math inline">\(Z\)</span>上的条件分布，当<span class="math inline">\(A\)</span>、<span class="math inline">\(X\)</span>和由边集<span class="math inline">\(E\)</span>确定的节点i的邻接潜变量时，任意单个<span class="math inline">\(Z_i\)</span>与所有其他<span class="math inline">\(Z_j\)</span>都是独立的。这句话非常抽象，可以从下图来理解。 <img src="/images/pd10/10.png" /></p><p>当图<span class="math inline">\(G\)</span>满足这一条件是，我们就可以用平均场理论 (mean-feild) <span class="math display">\[r\left(\mathbf{Z}_{1}, \cdots, \mathbf{Z}_{n} \mid \mathbf{A}, \mathbf{X}\right) \approx \prod_{i=1}^{n} q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)\]</span> 其中<span class="math inline">\(\phi_{i}\)</span> 代表第i个变分边界的参数。而后我们用真实的条件概率分布与上式的KL散度来优化这些参数: <span class="math display">\[\min _{\phi_{1}, \cdots, \phi_{n}} \mathrm{KL}\left(\prod_{i=1}^{n} q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right) \| r\left(\mathbf{Z}_{1}, \cdots, \mathbf{Z}_{n} \mid \mathbf{A}, \mathbf{X}\right)\right)\]</span></p><p>而最优的变分边界满足以下形式（证明见论文） <span class="math display">\[q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)=\mathcal{O}_{\mathcal{G}}^{M F}\left(\mathbf{Z}_{i},\left\{q_{\phi_{j}}\right\}_{j \in \mathcal{N}(i)}\right)\]</span> 其中<span class="math inline">\(\mathcal{N}(i)\)</span>为<span class="math inline">\(\mathbf{Z}_{i}\)</span>的邻节点。<span class="math inline">\(\mathcal{O}\)</span>是一个由不动点方程确定的函数，它依赖于图自身的性质。这一形式意味着最优的<span class="math inline">\(q_{\phi_{i}}\)</span>的参数只与i的邻节点的q_{}有关。这就意味着平均场近似推断的迭代算法是在图上执行信息传递，直到收敛： <span class="math display">\[q_{\phi_{i}}^{(l)}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)=\mathcal{O}_{\mathcal{G}}^{M F}\left(\mathbf{Z}_{i},\left\{q_{\phi_{j}}^{(l-1)}\right\}_{j \in \mathcal{N}(i)}\right) .\]</span></p><p>令<span class="math inline">\(\boldsymbol{\mu}_{i}=\mathbb{E}_{\mathbf{Z}_{i} \sim q_{\phi_{i}}}\left[\psi\left(\mathbf{Z}_{i}\right)\right]\)</span>，根据上文提到的结论，我们就可以绕开 具体的<span class="math inline">\(\mathcal{O}\)</span>，将其变为 <span class="math display">\[\boldsymbol{\mu}_{i}^{(l)}=\tilde{O}_{\psi, \mathcal{G}}^{M F}\left(\left\{\boldsymbol{\mu}_{j}^{(l-1)}\right\}_{j \in \mathcal{N}(i)}\right)\]</span> 将上式在0处做一阶泰勒展开，有 <span class="math display">\[\mu_{i}^{(l)} \approx \tilde{O}_{\psi, \mathcal{G}}(\mathbf{0})+\mathbf{N}_{i}^{(l-1)} \cdot \nabla \tilde{O}_{\psi, \mathcal{G}}(\mathbf{0})\]</span> 这式从形式上就与message passing机制非常类似。 <span class="math display">\[H_{i}^{(l)}=\eta_{l}\left(B_{l, i}+\sum_{f \in \mathcal{F}_{l}} f\left(\mathbf{A}_{i}\right) \mathbf{H}^{(l-1)} W_{l}\right)\]</span> 由于在看推导过程时把一些分量的维度等忽略了，因此最后一步的具体细节直接在这里贴原文。</p><p><img src="/images/pd10/11.png" title="具体证明过程" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ICML2019的论文，作者为Stanford的phd。&lt;/p&gt;
&lt;h1 id=&quot;先说几句&quot;&gt;先说几句&lt;/h1&gt;
&lt;p&gt;看到摘要&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Our model parameterizes variational autoencoders (VA</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graph Auto-Encoders</title>
    <link href="http://example.com/2022/01/18/pd10/"/>
    <id>http://example.com/2022/01/18/pd10/</id>
    <published>2022-01-18T13:39:46.000Z</published>
    <updated>2022-01-19T12:47:34.022Z</updated>
    
    <content type="html"><![CDATA[<p>本文来自Thomas Kipf的博士论文，其论文其他内容包括GCN、relational GCN、compositional imitation learning and execution等。</p><p>对于图<span class="math inline">\(G=(V,E)\)</span>，有<span class="math inline">\(N=|V|\)</span>，邻接矩阵<span class="math inline">\(A\)</span>为<span class="math inline">\(N \times N\)</span>，用<span class="math inline">\(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\)</span>来度量两个节点ij的相似度，其中<span class="math inline">\({z}_{i}, {z}_{j}\)</span>为节点的embedding</p><h1 id="gae">GAE</h1><p><img src="/images/pd10/1.png" title="encoder-decoder architecture" /></p><p>Graph Auto-Encoder同样采用encoder-decoder架构，其中scoring function <span class="math inline">\(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\)</span>作为decoder，其根据节点嵌入来重构邻接矩阵；Encoder的输入为图的邻接矩阵<span class="math inline">\(A\)</span>以及节点的特征向量<span class="math inline">\(\left\{\mathbf{x}_{i}\right\}_{i \in \mathcal{V}}\)</span>，输出为node representations <span class="math inline">\(\mathbf{Z}_{i}\)</span>。</p><ul><li><p>Encoder<br />GAE的Encoder借助GNN来处理节点的初始化向量和图的结构信息，从而得到节点的表示。比如使用GCN作为Encoder时，有： <span class="math display">\[\mathbf{Z}=\operatorname{GCN}(\mathbf{X}, \mathbf{A})=\widehat{\mathbf{A}} \operatorname{ReLU}\left(\widehat{\mathbf{A}} \mathbf{X} \mathbf{W}_{0}\right) \mathbf{W}_{1}\]</span> 除了用GNN作为encoder之外，还有其他的embedding方法，比如最简单的shallow embedding直接根据节点的编号，以及DeepWalk、node2vec等方法。</p></li><li><p>Decoder<br />Decoder用来根据<span class="math inline">\(Z\)</span>重建邻接矩阵 <span class="math display">\[\mathbf{A}^{\prime}=l\left(\mathbf{Z Z}^{\top}\right)\]</span> 其中<span class="math inline">\(l(.)\)</span>是logistic sigmoid function，也就是说<span class="math inline">\(A_{i, j}^{\prime}=l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)\)</span></p></li></ul><p>不过在图表示学习一书中给出了多种decoder <img src="/images/pd10/4.png" title="decoder" /></p><ul><li>Training<br />使用交叉熵进行训练 <span class="math display">\[\mathcal{L}=-\frac{1}{N^{2}} \sum_{i=1}^{N} \sum_{j=1}^{N} A_{i, j} \log l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)+\left(1-A_{i, j}\right) \log \left(1-l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)\right)\]</span></li></ul><h1 id="vae">VAE</h1><p><img src="/images/pd10/2.png" title="VAE" /></p><p>在介绍变分图自编码器 (VGAE)之前，我们先简单介绍一下变分自编码器Variational Auto-encoders。[Auto-Encoding Variational Bayes]</p><p><img src="/images/pd10/5.png" title="思路" /> 上图中的实线就代表了生成模型<span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\)</span>，也就是根据隐变量z生成目标数据。而这一生成模型中，我们需要用<span class="math inline">\(q_{\phi}(\mathbf{z} \mid \mathbf{x})\)</span>来拟合无法得到的后验分布<span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\)</span>。而这里就涉及到变分推断VI的内容。</p><p>接下来我们具体展开来说。</p><p>给定一个真实样本 <span class="math inline">\(x_{k}\)</span>, 我们假设存在一个后验分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span> 并假设这个分布是正态分布。后续我们要训练一个生成器<span class="math inline">\(x=g(z)\)</span>, 使其能从分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span> 采样出来的一个 <span class="math inline">\(\hat x_{k}\)</span> 还原为<span class="math inline">\(x_{k}\)</span>。<br />对于正态分布的两个参数：均值 <span class="math inline">\(\mu\)</span> 和方差 <span class="math inline">\(\sigma^{2}\)</span>，我们用神经网络进行拟合<span class="math inline">\(\mu_{k}=f_{1}\left(x_{k}\right), \log \sigma_{k}^{2}=f_{2}\left(x_{k}\right)\)</span>。再借助重参数技巧从<span class="math inline">\(z_k\)</span>的分布中采样得到<span class="math inline">\(z_k\)</span>。（对于重参数技巧，就是使采样这一步骤变为可导的一种方法）。</p><p>由于VAE最开始假设了隐变量服从正态分布，这就需要神经网络拟合的分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span>向标准正态分布看起，因为 <span class="math display">\[p(z)=\sum_{x} p(z \mid x) p(x)=\sum_{x} \mathcal{N}(0, I) p(x)=\mathcal{N}(0, I) \sum_{x} p(x)=\mathcal{N}(0, I)\]</span> 使分布与标准正态看齐这一过程借助在loss增加一个额外的loss（生成分布与标准正态分布的KL散度）来实现 <span class="math display">\[\mathcal{L}_{\mu, \sigma^{2}}=\frac{1}{2} \sum_{i=1}^{d}\left(\mu_{(i)}^{2}+\sigma_{(i)}^{2}-\log \sigma_{(i)}^{2}-1\right)\]</span></p><p>上述的过程介绍从Auto-Encoder的角度来介绍VAE，事实上如果阅读原论文会发现这种介绍VAE的思路是很令人费解的。回到最基础的贝叶斯学习，我们需要<span class="math inline">\(q_{\phi}\left(\mathrm{z} \mid \mathrm{x}^{(i)}\right)\)</span> 去逼近真实的后验概率 <span class="math inline">\(p_{\theta}\left(\mathrm{z} \mid \mathrm{x}^{(i)}\right)\)</span>，很自然的我们选择用KL散度作为loss，而后经过变分推断的推到转换为优化变分下界 <span class="math display">\[\tilde{\mathcal{L}}\left(\theta, \phi ; \mathrm{x}^{(i)}\right)=\frac{1}{L} \sum_{l=1}^{L}\left[\log p_{\theta}\left(\mathrm{x}^{(i)}, \mathrm{z}^{(i, l)}\right)-\log q_{\phi}\left(\mathrm{z}^{(i, l)} \mid \mathrm{x}^{(i)}\right)\right]\]</span> 其中, <span class="math inline">\(\mathrm{z}^{(i, l)}=g_{\phi}\left(\epsilon^{(i, l)}, \mathrm{x}^{(i)}\right), \quad \epsilon^{(i, l)} \sim p(\epsilon)\)</span> 。</p><p>而VAE正是给定上述结果中<span class="math inline">\(\epsilon, p_{\theta}(\mathrm{x} \mid \mathrm{z}), q_{\phi}(\mathrm{z} \mid \mathrm{x}), p_{\theta}(\mathrm{z})\)</span> 分布具体形式（正态分布）后的特例。</p><h1 id="vgae">VGAE</h1><p><img src="/images/pd10/3.png" title="VGAE" /></p><p>对于变分图自编码器，简单来看就是输入变为节点特征和邻接矩阵，输出为生成的邻接矩阵。</p><p><span class="math inline">\(p_{\theta}(\mathbf{A} \mid \mathbf{X})\)</span>为节点特征<span class="math inline">\(X\)</span>与邻接矩阵A的条件概率分布 <span class="math display">\[p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{X})=\int p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X}) p(\mathbf{Z} \mid \mathbf{X}) d \mathbf{Z}\]</span> 其中隐变量先验分布独立于特征向量X，只和节点自身有关<span class="math inline">\(p(\mathbf{Z} \mid \mathbf{X})=\prod_{i=1}^{N} p\left(\mathbf{z}_{i}\right)\)</span>。更具体的说，我们令<span class="math inline">\(p\left(\mathbf{z}_{i}\right)=\mathcal{N}\left(\mathbf{z}_{i} ; \mathbf{0}, \mathbf{I}\right)\)</span>。我们的目标是得到最优的参数<span class="math inline">\(\theta\)</span>。</p><p>根据变分推断的框架，我们引入inference model： <span class="math display">\[q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A})=\prod_{i=1}^{N} q_{\phi}\left(\mathbf{z}_{i} \mid \mathbf{X}, \mathbf{A}\right)\]</span> 其中 <span class="math inline">\(q_{\phi}\left(\mathbf{z}_{i} \mid \mathbf{X}, \mathbf{A}\right)=\mathcal{N}\left(\mathbf{z}_{i} ; \boldsymbol{\mu}_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right)\)</span></p><p>具体模型中，我们用两个GCN作为学习inference model的参数： <span class="math display">\[\mu_{i}=\left[\mathrm{GCN}^{(1)}(\mathbf{X}, \mathbf{A})\right]_{i}\]</span> <span class="math display">\[\log \sigma_{i}=\left[\mathrm{GCN}^{(2)}(\mathbf{X}, \mathbf{A})\right]_{i}\]</span></p><p>接下来的generative model，我们假设它与初始输入的节点特征无关，只与隐变量有关， <span class="math display">\[p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X})=\prod_{i=1}^{N} \prod_{j=1}^{N} p_{\boldsymbol{\theta}}\left(A_{i, j} \mid \mathbf{z}_{i}, \mathbf{z}_{j}\right)\]</span></p><p>模型所要优化的KL散度与变分推断的过程相似，可以变为优化 <span class="math display">\[\operatorname{ELBO}=\mathbb{E}_{q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A})}\left[\log p_{\theta}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X})\right]-\operatorname{KL}\left[q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A}) \| p(\mathbf{Z})\right].\]</span></p><p>我们可以将上述过程写的通俗一点，其中编码过程为： <span class="math display">\[q\left(z_{i} \mid X, A\right)=N\left(z_{i} \mid \mu_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right).\]</span> 解码（以内积为例）的过程为： <span class="math display">\[p\left(A_{i j}=1 \mid z_{i}, z_{j}\right)=\sigma\left(z_{i}^{T} z_{j}\right)_{\circ}\]</span> 损失函数为： <span class="math display">\[L=E_{q(Z \mid X, A)}[\log p(A \mid Z)]-K L[q(Z \mid X, A) \| p(Z)]\]</span></p><p>作者在边预测任务上测试了VGAE的表现。不过，在Decoder时不考虑节点的特征<span class="math inline">\(X\)</span>仅仅是为了将模型简化，作者发现这不影响link prediction的准确率。 <img src="/images/pd10/6.png" /></p><p>值得推敲的是，在论文的前面有关于图卷积GCN在这几个数据集上的表现。而作者却没有用统一的评价指标（精度和准确率）来对比这两个模型的表现。</p><p><img src="/images/pd10/7.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文来自Thomas Kipf的博士论文，其论文其他内容包括GCN、relational GCN、compositional imitation learning and execution等。&lt;/p&gt;
&lt;p&gt;对于图&lt;span class=&quot;math inline&quot;&gt;\(G</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>QUINE&#39;S EMPIRICAL ASSUMPTIONS [1968]</title>
    <link href="http://example.com/2021/12/21/pd9/"/>
    <id>http://example.com/2021/12/21/pd9/</id>
    <published>2021-12-21T12:09:17.000Z</published>
    <updated>2021-12-23T02:02:25.713Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><h2 id="絮絮叨叨">絮絮叨叨</h2><p>为什么会突然看一篇60年代的文章？要从几天前我从学校图书馆借了一本叫《数学之美》的书说起。首先安利一下这本书，这书薄薄的似乎只有100页，但是书的作者的文学素养和专业知识让AI问题及其背后的数学充满了浪漫主义色彩。然后在书里刚开始关于自然语言处理的相关介绍中，我看到了一个名字-Noam Chomsky （乔姆斯基）。当时我并没有听说过他，于是就去搜索了一下，搜索结果几个关键词抓住了我的眼球：xxxx创始人、最伟大的学者之一。so, who is he?</p><h2 id="从规则到统计">从“规则”到“统计”</h2><p>自然语言处理作为人工智能下的子领域，自然也陪伴着人工智能走过几个“春季”与“冬季”。具体而言，NLP从早起的基于规则到如今的基于统计，背后不仅是深度学习技术的发展，也是其从“理想主义”为主流到“经验主义”占主导的转变过程。</p><blockquote><p>基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处理是哲学中的理性主义。在哲学领域中经验主义与理性主义的斗争一直是此消彼长，这种矛盾与斗争也反映在具体科学上，如自然语言处理。 早期的自然语言处理具有鲜明的经验主义色彩。如1913年马尔科夫提出马尔科夫随机过程与马尔科夫模型的基础就是“手工查频”，具体说就是统计了《欧根·奥涅金》长诗中元音与辅音出现频度；1948年香农把离散马尔科夫的概率模型应用于语言的自动机，同时采用手工方法统计英语字母的频率。<br />然而这种经验主义到了乔姆斯基时出现了转变。<br />1956年乔姆斯基借鉴香农的工作，把有限状态机作用刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型。这些工作非常伟大，为自然语言和形式语言找到了一种统一的数学描述理论，一个叫做“形式语言理论”的新领域诞生了。但乔老爷子干完这一票之后，挥一挥衣袖，说了一句“有限状态模型不适合用来描述自然语言”。 随后老爷子又补了一刀“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”。 -------《统计自然语言处理》</p></blockquote><p>从如今language model在NLP中的表现来看这句“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”是非常的离谱，而网上的几篇博客（内容几乎一模一样😅）说这句如此绝对的话来源于<em>QUINE'S EMPIRICAL ASSUMPTIONS</em>这篇文章。因此本着对伟人的尊重和求真务实的态度，我就决定去读一下这篇文章。另外这篇文章是在springer出版的书籍。不过在说这篇文章的内容之前还需要补充介绍几个名词</p><h2 id="willard-van-orman-quine-avram-noam-chomsky">Willard Van Orman Quine &amp; Avram Noam Chomsky</h2><p>这里我们首先简单的介绍一下两个人。<br />WVO Quine就是文章标题中的这个Quine，文章中从他的作品<em>word and object</em>开始谈起。直接检索Willard Van Orman Quine会发现百度百科并没有收录这个词条，不过wikipedia倒是有。简单来说，Quine是美国一位著名的哲学家，主张经验主义，另外他倡导Semantic holism-语义整体论。语义整体论可以简单理解为语言的某个部分，无论是术语还是完整的句子，只能通过它与更大语言部分的关系来理解。</p><p>Avram Noam Chomsky也就是乔姆斯基（任在世），是美国哲学家，麻省理工学院语言学的荣誉退休教授。乔姆斯基的《句法结构》被认为是20世纪理论语言学研究上最伟大的贡献。<br />《句法结构》（Syntactic Structures）是乔姆斯基介绍转换生成语法的《语言学理论的逻辑结构》一书的精华版。这一理论认为说话的方式（词序）遵循一定的句法，这种句法是以形式的语法为特征的，具体而言就是一种不受语境影响并带有转换生成规则的语法。儿童被假定为天生具有适用于所有人类语言的基本语法结构的知识。这种与生俱来的知识通常被称作普遍语法理论。</p><h2 id="humean-theory">Humean Theory</h2><p>休谟（David Hume）是苏格兰不可知论哲学家。他认为人的认知是有局限的。在休谟看来，我们所能认知的“自我”，其实只是感知，人的感知受人的感官局限。休谟认为，因果关系是人的理念，我们倾向于把某种序列中理念间的必然联系归于这种因果关系的本质。也就是说，因果只是我们头脑中的理念而已，两个客体造成恒定感知，比如每天我们看到太阳升起——天就亮了，我们就会把二者视为因果关系。但并不是自然界真的存在因果关系。</p><h1 id="正文">正文</h1><p>这篇文章是对奎恩经验主义假设的解读，不过这篇文章阅读下来非常晦涩，因为作者在叙述Quine的理论时会通过各种逗号断句或小括号来表达自己的观点，所以文章的内容没有结构化的组织（就这还语言学家呢，就这就这），另外这一篇十多页的文章居然一个小标题都没有。</p><p>首先quine的理论来源于Humean theory of language acquisition，他认为人们对于语言的知识可以被表示为a network of linguistic，这也意味着人类的theory，比如chemistry这种二级学科或者基础的学科都可以被表示为a fabric of sentences variously associated to one another。进而人类所有的知识都可以用这些结构来描述。quine的理论中提到了“language”和“theory”。乔姆斯基指出理论与语言是相互渗透的，另外理论还涵盖了common-sense和belief。</p><blockquote><p>Beneath the uniformity that unites us in communication there is a chaotic <strong>personal diversity of connections</strong>, and, for each of us, <strong>the connections continue to evolve</strong>. No two of us learn our language alike, nor, in a sense, does any finish learning it while he lives.</p></blockquote><p>奎因表示如果语言是通过<strong>条件反应的机制相互关联</strong>并<strong>与外部刺激相关联</strong>的句子网络，那么一个人对言语行为的倾向可以根据这种网络来表征。按照这种语言的抽象形式，我们如何从语言中获取知识？奎恩提出了一个prelinguistic quality space，其中定义了距离度量（意味着可以度量相似度）。简单来说，在这个空间的某个维度上来看red ball， yellow ball之间的距离比red kerchief要近。这一想法似乎是背离经验主义的，因为这种质量空间可以想象和定义得到的，而非学习得到的。</p><p>然而，奎因在他关于语言是如何学习的假说中回到了经典的经验主义概念。与他认为语言是一个句子网络的观点相一致，他列举了学习句子的三种可能机制。首先，句子可以通过“直接条件反射”到“适当的非语言刺激”来学习，也就是说，通过在适当的条件下重复配对句子和刺激；第二，通过句子与句子的关联；第三，新句子可以通过“类比合成”产生，不过这种类比指的并不是类似英语语法规则的东西，而是在固定的上下文中用一个词代替一个类似的词（“手”、“脚”）。他认为一种语言是相关句子的有限网络，有些也与刺激相关，因为这只是两个假定的机制所产生的结构，具有实质性内容的语言学习。</p><p>但是乔姆斯基认为语言是句子的无限集合构成的。由假定的机制推导出的网络必定是有限的（对应上文的学习句子的机制），它只会包含人们曾经接触过的句子。</p><blockquote><p>Presumably, a complex of dispositions is a structure that can be represented as a set of probabilities for utterances in certain definable 'circumstances' or 'situations'. But it must be recognized that the notion 'probability of a sentence' is an entirely useless one, under any known interpreta- tion of this term.</p></blockquote><p>这里乔姆斯基给出了这句话——句子的概率是没有意义的。他举例说“birds fly”或者“Tuesday follows Monday”这两个英语下句子的概率对日语中产生这两个句子的概率没有意义。他认为probability relative to a situation没有任何意义。如果complex of dispositions是由根据经验观察确定的，那么只有少数传统的问候语、陈词滥调等才有可能与语言的倾向相关联，因为在技术意义上，在任何合理的语料库或数据集中，很少有其他句子可能具有非空的相对频率。且随着语料库的增加，任何给定句子的频率都会无限制地减少。 有人可能会设想用其他方法根据经验为句子分配概率，但乔姆斯基认为，没有一种方法可以避免这些困难。因此，如果一种语言被理解为在正常情况下作出反应的复杂倾向（奎恩的经验主义假说），那么它不仅是有限的、而且非常“小”。</p><p>Quine在提出“言语倾向”时指出了翻译的不确定性问题，简单来说可以理解为每个人的说话习惯几乎没有相似之处，因此根本无法建立与这种倾向相一致的翻译手册。对于理论和语言的有限性假设带来的问题，乔姆斯基提出语言是人类头脑的先天属性所带来的，存在一种“普遍语法”。</p><p>到这里，我们简单的概括一下前文提到的大概内容，即Quine的理论和乔姆斯基的看法:</p><blockquote><p>We are left with the fact that Quine develops his explicit notion of 'language' and 'theory' within a narrowly conceived Humean framework (except for the possible intrusion of a rich system of innate ideas), and that he characterizes language learning (&quot;learning of sentences&quot;) in a way consistent with this narrow interpretation, although the conclusion that <strong>a language (or theory) is a finite fabric of sentences, constructed pairwise by training, or a set of sentences with empirically detectable probabilities of being produced</strong> (hence a nearly empty set) is incompatible with various truisms to which Quine would certainly agree.</p></blockquote><p>Quine依靠他关于知识获取和语言学习的经验主义假设来支持他的一些主要哲学结论。一个重要的例子可以说明这一点。知识的基础是从某些证据上做“分析假设”。对Quine来说，一个关键点是，在基本语言和“常识知识”的情况下，分析假设的正确性并不是“客观问题”，它可以是“对或错”。这些分析假设是超越了 “任何一个本地人的言语行为倾向所隐含的任何东西”。因此，当我们在翻译、学习一门语言时，我们自然而然地会使用这些分析性假设（知识）与母语进行类比。也就是说在Quine的经验主义观点建模下超越言语倾向的知识（分析假设）是一个主观的概念，而这就会带来“翻译的不确定性”。</p><p>另外，Quine对基于数据的分析假设的构建和基于数据的“观察句子的刺激意义”的假设进行了鲜明的区分。他指出，后者只涉及“正常感应”类型的不确定性。显然，包含真值功能连接词的句子的翻译（类似地，学习和理解）中涉及的归纳推理也是如此。在这些情况下，归纳法将我们引向“真正的假设”，这与“分析假设”截然不同（在讨论翻译的不确定性时提到的“分析假设”）。因此，Quine认为“<strong>正常归纳</strong>”与“<strong>假设形成或理论建构</strong>”之间存在区别，前者不涉及严重的认识论问题，后者确实涉及此类问题。毫无疑问，这种区别是可以区分的；然而，Quine没有具体说明“正常归纳”所基于的先验属性。这里，乔姆斯基认为大脑天生具有允许从“正常归纳”到“真实假设”的属性，但不允许“理论建构”和一些可能受到狭隘限制的“分析假设”。也就是说，他认为在经验主义下根据数据进行归纳而后得到一个假设的真值（对或错）这个过程是合理，但是直接归纳知识这一过程是不合理的。<br />因此，一般来说关于语言不可能有一套固定的“分析假设”。我们需要为每种语言（更准确地说，为每种语言的每一个说话者）建立一套新的分析，因为语言的形式没有任何普遍性。</p><p>这里还是强调了乔姆斯基对于统计自然语言处理的观点，他认为每种语言，每个说话者的说话倾向会导致无法建立一套普遍的“分析假设”。因此乔姆斯基认为，当我们学习一门语言时，我们并不是在“学习句子”或通过训练获得“行为技能”。相反，我们以某种方式发展了某些原则（当然是无意识的），这些原则决定了许多句子的形式和意义。</p><h1 id="转换生成语法">转换生成语法</h1><p>在乔姆斯基的《句法结构》一书中，他提出了转换生成语法理论，他认为语言是人类特有的一种先天机制，不仅应该研究语言行为，而且应该研究语言能力，转换-生成语法就是关于语言能力的理论。具体而言，乔姆斯基认为语法主要包括基础和转换两个部分，基础部分生成深层结构，深层结构通过转换得到表层结构，语义部分属于深层结构，它为深层结构作出语义解释。语音部分属于表层结构并为表层结构作出语音解释。强调从认知学的角度对人类语言共性的解释，区分先天的语言能力和后天的语言知识，认为语言有生成能力，是有限规则的无限使用，转换则是生成的重要手段。</p><p>他的思想对当时主流的结构主义语言学产生了重要的影响。他的理论包含了几个关键的思想，首先是语义学是独立于语法学之外的，合乎语法的并不一定有意义。另外，他认为语言能力就像行走一样，是人与生俱来的理解语言及遣词造句的能力。</p><p>转换生成语法自创立以来, 就以对语言现象的解释充分性为目标, 试图建立一套能像数理公式般进行形式运算推理的规则来解释自然语言。期间虽经反复的修改否定再修改, 每一次都会有新的理论突破, 但其研究的对象、方法和原则却始终如一, 从而极大的推动了当代语言学的发展, 并为语言研究开辟了一条新的道路, 展现了一个全新的发展方向。<br />比如说，基于规则的句法剖析主要是使用Chomsky的上下文无关语法。在上下文无关语法的基础上, 学者们提出了自顶向下分析法、自底向上分析法、左角分析法、CYK算法、Earley 算法、线图分析法等行之有效的剖析技术。</p><p>关于基于规则的自然语言处理在工业界中的应用，可以参考这个链接 https://www.zhihu.com/question/30748126</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;h2 id=&quot;絮絮叨叨&quot;&gt;絮絮叨叨&lt;/h2&gt;
&lt;p&gt;为什么会突然看一篇60年代的文章？要从几天前我从学校图书馆借了一本叫《数学之美》的书说起。首先安利一下这本书，这书薄薄的似乎只有100页，但是书的作者的文学素养和专业知识让AI问题及其背</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Gel Candy Evaluation 凝胶糖果测评</title>
    <link href="http://example.com/2021/12/20/candies/"/>
    <id>http://example.com/2021/12/20/candies/</id>
    <published>2021-12-20T06:25:28.000Z</published>
    <updated>2022-01-05T03:20:44.092Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>Todo</p><h1 id="introduction">Introduction</h1><p>改革开放以来，人民生活水平日益提升，人们的物质文化生活也越来越丰富多彩。糖果作为小孩子最爱的零食，种类也越来越丰富。在跟王同学一起在研讨间学习的这段时间，在去往图书馆的路上，我偶尔会从天猫超市[1]或者苏果超市[2]买一包软糖给王同学，希望糖果的甜能为她备战考研的枯燥时光中带来些许快乐。由于笔者在这段时间中有点咳嗽，医生叮嘱不可以吃甜食，因此在测评过程中每包糖果我也只吃了几颗（有时一转眼整包就被某人吃完了），但是保证了每一类糖果中包含的每种味道都吃过。下面将对两个超市中售卖的几种凝胶糖果进行测评和打分。对于糖果的评价指标，由于笔者才疏学浅，只能使用个人主观评价以及王同学对部分糖果的一些评价进行评估。</p><h1 id="method">Method</h1><h2 id="gel-candy">Gel Candy</h2><h1 id="experiment">Experiment</h1><h2 id="evaluation">Evaluation</h2><p>笔者对于这些糖果的味道和口感按照1～5🌟进行打分，另外附带一些文字评价。另外王同学的评价一般包括：就这，还行，味太大，一般。</p><h2 id="alpenliebe">Alpenliebe</h2><ul><li>乐嚼Q动物果果</li><li>乐嚼Q虫虫派对</li><li>乐嚼Q乳酸果果</li><li>乐嚼Q熊猫小队</li></ul><h2 id="skittles">Skittles</h2><h2 id="uha">UHA</h2><h1 id="reference">Reference</h1><p>【1】东南大学九龙湖校区梅园天猫超市<br />【2】东南大学九龙湖校区桃园苏果超市</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Todo&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;改革开放以来，人民生活水平日益提升，人们的物质文化生活也越来越丰富多彩。糖果作为小孩子最爱的零食，种类也越</summary>
      
    
    
    
    <category term="杂七杂八" scheme="http://example.com/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks [ACL2020]</title>
    <link href="http://example.com/2021/12/08/pd8/"/>
    <id>http://example.com/2021/12/08/pd8/</id>
    <published>2021-12-08T05:49:22.000Z</published>
    <updated>2022-03-19T07:00:41.157Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍的内容包含多篇基于图神经网络进行文本分类的论文，从提出GCN后的简单应用到去年ACL上的TextING以及今年的BertGCN，GNN在文本分类上取得了非常好的效果。</p><p><a href="https://arxiv.org/abs/1809.05679">Graph Convolutional Networks for Text Classification</a><br /><a href="https://arxiv.org/abs/1910.02356v2">Text Level Graph Neural Network for Text Classification</a><br /><a href="https://arxiv.org/abs/2004.13826v1">Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks</a><br /><a href="https://arxiv.org/abs/2105.05727">BertGCN: Transductive Text Classification by Combining GCN and BERT</a></p><h1 id="text-classification">Text Classification</h1><p>文本分类的应用非常广泛，包括Sentiment Analysis、News Categorization、Topic Analysis甚至是Question Answering等。而automatic text classification方法大致可以被归为两类：基于规则的^rule-based以及基于机器学习的^数据驱动的。</p><p>基于规则的文本分类需要先验知识，包括pre-defined rules、domain knowledge等。而基于机器学习的方法则是借助标注数据集。通常我们可以把机器学习文本分类的步骤归为两步，首先是文本特征提取，而后将特征输入分类器进行分类。而发展到现在，基于神经网络的文本分类模型大致也随着深度学习的发展从前馈神经网络到CNN、RNN以及attention、transformer到如今pre-trained model如BERT等。</p><p>前馈神经网络进行文本分类通常将文本作为bag of words；RNN则把文本作为词的序列；CNN用于训练提取文本中的关键短语、词组等进行匹配分类；attention机制能识别文本中相互关联的词，可以嵌入到其他深度学习模型中；至于transformer以及BERT这类大规模预训练模型，则是“大力出奇迹”。</p><p>而本文的重点，则是基于GNN-图神经网络的文本分类方法。借助图神经网络进行文本中句法、语义解析树之类的图结构信息挖掘，进而进行文本分类。另外经过下文的介绍我们还能发现，基于GNN的模型能与其他深度神经网络进行级联并进行联合训练，进而有效提升分类准确率。<br />基于图神经网络的文本分类模型的差异大致体现在三个方面：图的构建、节点嵌入的初始化、图神经网络。</p><h1 id="textgcn">TextGCN</h1><p>Graph Convolutional Networks for Text Classification<br /><img src="/images/gcn/2.png" title="Framework of TextGCN" /> TextGCN为AAAI2018的论文，现在很多人看到这篇文章的时候可能会感叹“这也能发？”，但事实上这篇论文是最先构建了transductiive的基于GNN进行文本分类的框架，并取得了非常好的表现。</p><p>模型整体的框架如上图所示，包括图的构建和图神经网络两个模块。其中图神经网络由简单的两层卷积层构成。另一方面，图节点的特征初始化用one-hot vector，输入的信息仅为边信息和节点的结构信息，而其在分类结果上取得的准确率也反映了GNN应用文本分类的合理性。 <span class="math display">\[Z=\operatorname{softmax}\left(\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W_{0}\right) W_{1}\right)\]</span></p><p>对于构图方面，模型基于整个语料库构建一个异构图，图中的节点包括文档节点和词节点。而这两类节点之间的边，word-word、doc-word定义如下 <span class="math display">\[A_{i j}=\left\{\begin{array}{ll}\operatorname{PMI}(i, j) &amp; i, j \text { are words, } \operatorname{PMI}(i, j)&gt;0 \\\operatorname{TF}-\operatorname{IDF}_{i j} &amp; i \text { is document, } j \text { is word } \\1 &amp; i=j \\0 &amp; \text { otherwise }\end{array}\right.\]</span> 其中词i与j之间的PMI (point-wise mutual information) 计算为 <span class="math display">\[\begin{aligned}\operatorname{PMI}(i, j) &amp;=\log \frac{p(i, j)}{p(i) p(j)} \\p(i, j) &amp;=\frac{\# W(i, j)}{\# W} \\p(i) &amp;=\frac{\# W(i)}{\# W}\end{aligned}\]</span> <span class="math inline">\(\#W(i,j)\)</span>代表滑窗中两个词共现次数。另外，图中的词节点会将语料库统计后的低频词过滤掉。至于为什么选择PMI以及TF-IDF这两个指标作为边权重，作者提到是从实验的结果出发作出的选择。 <img src="/images/pd8/2.png" title="Results: TextGCN" /></p><h1 id="text-level-gnn">Text-level GNN</h1><p>Text Level Graph Neural Network for Text Classification [ENMLP2019]<br />TextGCN模型跟大部分直推式GNN模型一样，应用时存在明显的缺陷，即没有办法进行在线测试。当我们要输入的新文本进行分类时，需要将文本加入语料库后重新构建graph训练，这就会带来极大的开销。而这篇Text-level GNN则是构建基于文本级别的图，使得基于GNN的文本分类模型提供在线测试的功能，虽然模型依旧是Transductive。 <img src="/images/pd8/3.png" title="Text-level GNN" /> Text-level图的构建如上图所示，其中词与词之间连接的边权重以及词的embedding为整个语料库全局共享，保存在全局矩阵中（上图的两个矩阵），另外文档中的每个词不止和相邻的词存在边，而由一个超参数控制多跳邻居。</p><p>在图神经网络模块，虽然论文中介绍的是non-spectral message passing mechanism，但事实上与TextGCN本质上是一样的，不过边的权重会在训练过程中进行更新。 <span class="math display">\[\begin{aligned}\mathbf{M}_{\mathbf{n}} &amp;=\max _{a \in \mathcal{N}_{n}^{p}} e_{a n} \mathbf{r}_{\mathbf{a}} \\\mathbf{r}_{\mathbf{n}}^{\prime} &amp;=\left(1-\eta_{n}\right) \mathbf{M}_{\mathbf{n}}+\eta_{n} \mathbf{r}_{\mathbf{n}}\end{aligned}\]</span> 上式中<span class="math inline">\(r\)</span>代表节点特征，<span class="math inline">\(\eta_{n}\)</span>为可训练的参数。<br />最后，使用文本中的所有词的embedding进行类别的推断；而在TextGCN中则是直接使用文档节点的embedding进行分类。 <span class="math display">\[y_{i}=\operatorname{softmax}\left(\operatorname{Relu}\left(\mathbf{W} \sum_{n \in N_{i}} \mathbf{r}_{\mathbf{n}}^{\prime}+\mathbf{b}\right)\right)\]</span></p><p>在这里简单对Corpus-level GNN（TextGCN）和Text-level GNN进行简单的比较。首先两者在下游任务的准确率上有较小的差异，其中后者略胜一筹，不过后者使用了Glove词向量进行初始化，所以事实上将TextGCN使用一些小技巧后两者的准确率是非常接近的。不过Text-level GNN优越性体现在其能够提供在线测试上，当输入新文档进行分类时，它的计算开销会远小于TextGCN。而对于它使用的MPM神经网络而不是GCN，是因为MPM更适合它的构图模式，而不是MPM比常规的GCN更强的信息提取能力。Text-level使得全局的边权重必须成为可训练的参数，而MPM中的另一个可训练的参数<span class="math inline">\(\eta_{n}\)</span>实质上与GCN中结合<span class="math inline">\(I\)</span>的拉普拉斯矩阵<span class="math inline">\(L\)</span>是一致的。</p><h1 id="texting">TextING</h1><p>Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks<br />上文的两个模型以及后续的BertGCN都是transductive，而本文的TextING则是inducive模型。论文发表在ACL2020上，也就是博客标题。 <img src="/images/pd8/4.png" title="TextING" /> 模型针对每篇文档构建一个图，以词共现作为边节点，借助滑窗（size 3）构建图。节点嵌入用Glove进行初始化。 模型的图神经网络模块使用了Gated Graph Neural Networks(GGNN)和图注意力(GAT)。 <span class="math display">\[\begin{aligned}\mathbf{a}^{t} &amp;=\mathbf{A h}^{t-1} \mathbf{W}_{a} \\\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z} \mathbf{a}^{t}+\mathbf{U}_{z} \mathbf{h}^{t-1}+\mathbf{b}_{z}\right) \\\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r} \mathbf{a}^{t}+\mathbf{U}_{r} \mathbf{h}^{t-1}+\mathbf{b}_{r}\right) \\\tilde{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W}_{h} \mathbf{a}^{t}+\mathbf{U}_{h}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)+\mathbf{b}_{h}\right) \\\mathbf{h}^{t} &amp;=\tilde{\mathbf{h}}^{t} \odot \mathbf{z}^{t}+\mathbf{h}^{t-1} \odot\left(1-\mathbf{z}^{t}\right)\end{aligned}\]</span></p><p>上式中<span class="math inline">\(h\)</span>表示节点embedding，<span class="math inline">\(a\)</span>代表接受的信息，<span class="math inline">\(z\)</span>和<span class="math inline">\(r\)</span>分别代表更新和遗忘。而后将节点借助readout模块输出为graph-level的embedding： <span class="math display">\[\begin{array}{l}\mathbf{h}_{v}=\sigma\left(f_{1}\left(\mathbf{h}_{v}^{t}\right)\right) \odot \tanh \left(f_{2}\left(\mathbf{h}_{v}^{t}\right)\right) \\\mathbf{h}_{\mathcal{G}}=\frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \mathbf{h}_{v}+\text { Maxpooling }\left(\mathbf{h}_{1} \ldots \mathbf{h}_{\mathcal{V}}\right)\end{array}\]</span></p><p>上文提到的两个模型中GNN并没有attention模块，这是由于TextGCN的PMI、TF-IDF信息会损失，另一方面Text-level GNN全局的边权重也不应该引入文本图中的attention机制进行更新。</p><p>将上文的三个模型进行简单的对比，可以隐约感到存在一个图的构建与图神经网络之间的trade-off。前两个模型的图构建过程都嵌入了大量的信息（先验信息、全局信息），而他们的图神经网络都非常简单。事实上我曾在TextGCN上做过一些实验，尝试把GAT融入到信息传播过程中，发现准确率会有明显下降。而TextING的构图过程中的信息仅是词节点的共现所包含的上下文信息和结构信息，因此它可以接受更复杂的信息传播和聚合过程。这也使得TextING可以面对新词和新输入的文本直接进行分类。</p><h1 id="bertgcn">BertGCN</h1><p>BertGCN: Transductive Text Classification by Combining GCN and BERT<br />BertGCN是由香侬科技提出，发表在ACL2021上的文章，也是目前文本分类的SOTA模型。不过这篇文章的贡献主要是工程上的。另外，不妨排列组合一下将Bert与TextING结合，应该能取得更好的结果XD（虽然在R8上的结果已经非常接近100了）。<br />回顾TextGCN，模型中图节点初始化用的是one-hot向量，而BertGCN则是用Bert进行embedding的初始化（文档节点用Bert、词节点直接为0），另外将Bert与GNN两个模块进行联合训练，取得了很好的表现。</p><p>两者的结合存在两个问题，一是难收敛：BERT与GCN处理数据的方式不同、模型大小不同；二是GCN是在整个图上运算，而BERT过大的模型无法一次全部加载图中所有结点，这就给BertGCN的训练带来阻碍。 针对第一个问题，模型使用了Interpolating损失。 <span class="math display">\[Z=\lambda Z_{\mathrm{GCN}}+(1-\lambda) Z_{\mathrm{BERT}}, Z_{\mathrm{BERT}}=\operatorname{softmax}(W X)\]</span> 当<span class="math inline">\(\lambda=1\)</span>时，BERT模块没有更新；当<span class="math inline">\(\lambda=0\)</span>时，GCN模块没有更新；当<span class="math inline">\(\lambda \in (0,1)\)</span>时，两个模块都能得到更新，并且通过调节<span class="math inline">\(\lambda\)</span>实现BertGCN整体模块的快速收敛。<br />对于无法整图训练这一问题，BertGNN提出了一个Memory Bank用于保存所有节点特征，每次从中加载batch进行训练并更新，其他保持不变。将整个语料库中的文档特征分批更新，为了防止异步更新带来的不一致性，模型在训练Bert模型时采用了小学习率。</p><p><img src="/images/pd8/1.png" title="Results: BertGCN" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文介绍的内容包含多篇基于图神经网络进行文本分类的论文，从提出GCN后的简单应用到去年ACL上的TextING以及今年的BertGCN，GNN在文本分类上取得了非常好的效果。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.05679&quot;</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="Text Classification" scheme="http://example.com/tags/Text-Classification/"/>
    
  </entry>
  
</feed>
