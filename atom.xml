<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chen&#39;s Homepage</title>
  
  <subtitle>Hello AI</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-03-16T16:06:58.100Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Chen jiayuan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>betterbert</title>
    <link href="http://example.com/2022/03/17/betterbert/"/>
    <id>http://example.com/2022/03/17/betterbert/</id>
    <published>2022-03-16T16:06:58.000Z</published>
    <updated>2022-03-16T16:06:58.100Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Multi-hop QA</title>
    <link href="http://example.com/2022/03/07/MultihopQA/"/>
    <id>http://example.com/2022/03/07/MultihopQA/</id>
    <published>2022-03-07T14:49:35.000Z</published>
    <updated>2022-03-10T06:52:41.259Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1808.09920v3">Question Answering by Reasoning Across Documents with Graph Convolutional Networks</a><br /><a href="https://arxiv.org/abs/2004.03096v2">Is Graph Structure Necessary for Multi-hop Question Answering?</a><br /><a href="http://arxiv.org/abs/2007.14062v2">Big Bird: Transformers for Longer Sequences</a></p><h1 id="图结构qa">图结构QA</h1><p><img src="/images/multihopQA/5.png" title="Multi-hop QA" /> 多跳问答可以看作一个多步推理以及信息结合的过程，Entity-GCN借助图结构的节点建模文本的实体，将其转化为图上的推理问题，取得了非常好的结果。另外，作者提到虽然multi-hop是一个非常具有实际意义的问题，但在此之前的模型仅仅是将文档连接为长文本，而后借助RNN类的模型进行处理。</p><p>对于QA问题，给定一个文档集合和查询，我们要从多个候选回答中选择正确的答案（实体）。其可以表示为<span class="math inline">\(\left\langle q, S_{q}, C_{q}, a^{\star}\right\rangle\)</span>，<span class="math inline">\(q\)</span>表示问题，<span class="math inline">\(S_{q}\)</span> 表示supporting document，<span class="math inline">\(C_{q}C\)</span>表示候选的entity 集合，<span class="math inline">\(a_*\)</span>是<span class="math inline">\(q\)</span>的答案。本文的目的是训练一个神经网络，给定一个查询<span class="math inline">\(q\)</span>，可以输出答案在<span class="math inline">\(C_{q}\)</span>上的一个概率分布。通过最大似然估计模型的参数，输出概率最大的结果作为预测的问题答案。</p><p><img src="/images/multihopQA/8.png" /></p><p>对于一个query，<span class="math inline">\(q=\langle s, r, ?\rangle\)</span>，我们根据supporting documents构建图，其中的节点或为查询的实体，或为候选实体。上图中同样颜色的节点代表同一实体，节点之间的边根据三种规则构建：在同一文档中共现(实线)，不同的mentions之间实体匹配(虚线)，coreference(红线)。接着借助构建的图进行图卷积（Gated-GCN），得到候选节点的特征，并将其与问题的特征结合，作为候选实体的估计。模型encoding的预处理用了ELMo。 <span class="math display">\[P\left(c \mid q, C_{q}, S_{q}\right) \propto \exp \left(\max _{i \in \mathcal{M}_{c}} f_{o}\left(\left[\mathbf{q}, \mathbf{h}_{i}^{(L)}\right]\right)\right)\]</span></p><h1 id="深入分析">深入分析</h1><p>Entity-GCN的成功以及当时GNN在NLP领域的热度让人们开始竞相探索图在多步推理上的应用。大部分的工作都将分布在不同段落间的实体抽取并建模为图结构。Is Graph Structure Necessary for Multi-hop Question Answering? 这篇文章的作者基于HotpotQA数据集构建了一个强大的基线模型并证明了，通过正确地使用预训练模型，图结构对于多步推理问答是不必要的。他们认为图结构和对应的邻接矩阵都可以被看作是一种任务相关的先验知识，并且图注意力可以被看作是自注意力的一种特例。实验和可视化分析都表明图注意力或整个图结构都可以被自注意力或Transformer替代。</p><p>论文提到基于图结构进行多跳问答的模型有很多变种，包括借助有向图的循环层来建模实体间的关系，使用动态实体图来解决抽取式的多步推理问答任务以及引入文档节点和问题节点将实体图拓展为异构图等方法。然而，作者在实验中发现移除图结构并不会影响模型的最终效果。</p><p>这里放一下原论文作者写的博客：<a href="https://mp.weixin.qq.com/s/mXLrcg0ZSaKF4w9pqv5_8w">EMNLP 2020 | 多步推理问答是否真的需要图结构？</a></p><p>这里简单介绍一下论文思路，具体细节可以参考原论文以及上面的博客。作者借助Bert和GCN构建了一个基线模型，如下图所示 <img src="/images/multihopQA/6.png" title="基线模型" /> 模型使用RoBERTa进行上下文的编码，借助Bert进行命名实体识别构建图结构，实体图的连接规则由以下两条规则确定：1）上下文中不同位置出现的相同实体之间有连接。2）同一个句子中出现的不同实体之间有连接。（这里有待考量，下文细🔒）。在HotpotQA数据集上，这个模型取得了非常好的表现： <img src="/images/multihopQA/7.png" title="实验结果" /> 基于这个模型进行后续实验时，作者发现在预训练模型以fine-tuning的方式使用时，包含和不包含图结构的模型都取得了相似的结果。而当我们固定预训练模型的参数后，EM和F1显著下降了9%和10%。如果此时进一步移除图结构，EM和F1会进一步下降4%左右。换句话说，只有当预训练模型以Feature-based的方式使用时，图结构才会起到比较明显的作用。而当预训练模型以Fine-tuning的方式使用时（这是较为通常的方式），图结构并没有对结果起到贡献，换句话说，图结构可能不是解决多步推理问题所必要的结构。</p><p>而后作者提到，图注意力是self-attention的一种特例，用transformer代替两层图神经网络也能取得非常接近的结果。作者指出邻接矩阵和图结构都可以被看作是一种任务相关的先验知识。</p><p>不过笔者私以为，既然作者认为图结构、领接矩阵是一种先验知识，那借助图神经网络可以很好的将这些先验融入到深度神经网络中。另一方面，其实图神经网络的表现很大程度上与图的构建有关，而作者在实验时只采用了这种非常naive的构图思路，而后说明这种图结构是没有作用的，感觉这里的论证略显单薄。另一方面，将先验知识结合到feature中，借助图神经网络的message passing机制进行学习和反向传播，从而将先验知识的融入纳入到神经网络这一框架下，也可以说是体现了图结构的意义。</p><h1 id="transformer-plus">Transformer plus</h1><p>上文提到，图注意力和图结构可以被自注意力或Transformer替代，也就是说基于图结构的模型本质上与Transformer是相似的。</p><p><img src="/images/multihopQA/1.png" title="Self-Attention" /> Transformer中最重要的就是Self-attention机制，从上图可以看到，而self-attention的计算包括Q、K、V三个矩阵以及矩阵乘法，此时就会引出一个复杂度问题。 <span class="math display">\[\operatorname{Attention}(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V\]</span> Q、K点乘的内存、速度是序列长度的平方复杂度。对于输入为长文本时，我们一般做法是切成512的块，这种做法损失了块与块之间的信息，比如多跳QA问题或者长文本文本摘要，块与块之间的信息起了重要的作用。</p><p>针对这一问题，解决的方法主要可以分为两类，第一类接受长度的限制，寻求绕过这一问题的方法：比如对文本使用滑窗，或者从上下文中选择一个subset输入到transformer中，而后迭代不同的上下文。基于这一思路的模型有： SpanBERT, ORQA, REALM, RAG等。另一类方法认为full attention是没有必要的，这类 Sparse Attention Mechanism就包括Longformer、Big Bird。 <img src="/images/multihopQA/2.png" title="Longerformer" /> <img src="/images/multihopQA/3.png" title="Big Bird" /></p><p>从上图就可以看出，这些方法抛弃了原有的全局attention计算，变为几类attention，包括：</p><ul><li><p>random attention：对于每个Q，都等概率随机关注r个Key。</p></li><li><p>window attention：对于每个Q，都关注相邻的左边w/2个Key，右边w/2个key。这是因为直觉上告诉我们，多数NLP问题上下文更加重要。</p></li><li><p>global attention：在一些预先选择的输入位置上添加全局attention，使这些位置能够关注所有的信息，比如图上所显示的使传统Bert在做分类时，[CLS] token就使用了global attention。针对QA问题时，我们就可以令问题部分的token为global attention。</p></li></ul><p>Big Bird就是使用了这三类Attention，实验结果上也证明了这类模型比传统的Bert表现更好。 <img src="/images/multihopQA/4.png" title="Exp" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1808.09920v3&quot;&gt;Question Answering by Reasoning Across Documents with Graph Convolutional Networks&lt;/a&gt;&lt;br /&gt;</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="QA" scheme="http://example.com/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>A Survey of Data Augmentation Approaches for NLP</title>
    <link href="http://example.com/2022/02/17/dataaug/"/>
    <id>http://example.com/2022/02/17/dataaug/</id>
    <published>2022-02-17T12:42:55.000Z</published>
    <updated>2022-02-20T16:42:01.784Z</updated>
    
    <content type="html"><![CDATA[<p>link: <a href="https://arxiv.org/abs/2105.03075v4" class="uri">https://arxiv.org/abs/2105.03075v4</a></p><h1 id="数据增强的背景">数据增强的背景</h1><p><strong>什么是数据增强？</strong></p><blockquote><p>Data augmentation (DA) refers to strategies for increasing the diversity of training examples without explicitly collecting new data.</p></blockquote><p>作者提到，数据增强是一种在不收集新数据的前提下增强训练样本多样性的策略。从这里可以看出，首先数据增强是根据已有的数据来创造新数据的过程。其次，数据增强并不意味着单纯地增多数据，而是为了模型训练进行服务的。也就是说，我们要在任务目标指导下借助已有的样本来创造新样本。</p><p><strong>为什么要做数据增强</strong><br />当我们获得的数据有限，不足以训练出优秀的模型时，就需要获取更多的数据。但是采集新数据往往要消耗大量的人力物力资源，在这种情况下，数据增强技术就提供了一个很好的解决措施，尤其是现在NLP领域基本是大规模预训练模型大行其道，对大数据更加渴求。而令一方面，数据增强能够按照我们的意愿对数据做一些约束。换句话说，我们可以通过数据增强的方法引入一些先验知识或条件。</p><p><strong>数据增强的合理性</strong><br />从我们直观的角度思考，扩充数据的分布与原始数据的分布既不应该太相似，也不应该太不同。我们设计的数据增强方法得到的新数据是尽可能服从实际情况下的分布。此时，在样本空间上更多的采样点有助于我们进一步探索真实的数据分布。</p><blockquote><p>data augmentation is typically performed in an ad- hoc manner with little understanding of the under- lying theoretical principles</p></blockquote><p>作者提到，目前关于DA为什么有效的研究工作主要停留在表层，对其理论基础和原理研究较少。一些现有的工作包括： 带噪声测试样本的训练可简化为Tikhonov正则化；DA可以增加分类器的positive margin，但只有在许多常见DA方法以指数方式做数据增强时才会如此；将DA转换视为kernels，并发现DA的两种帮助方式:特征平均和方差正则化。</p><p><strong>Data Augmentation in CV</strong></p><p>常见的CV中的数据增强包括：</p><ul><li><p>旋转、平移、翻折</p></li><li><p>缩放：图像可以被放大或缩小。放大时，放大后的图像尺寸会大于原始尺寸。大多数图像处理架构会按照原始尺寸对放大后的图像 进行裁切。</p></li><li><p>随机裁剪，我们随机从图像中选择一部分，然后降这部分图像裁剪出来，然后调整为原图像的大小</p></li><li><p>添加噪声： 过拟合通常发生在神经网络学习高频特征的时候 (因为低频特征神经网络很容易就可以学到，而高频特征只有在最后的时候才可以学到) 而这些特征对于神经网络所做的任务可能没有帮助，而且会对低频特征产生影响，为了消除高频特征我们随机加入噪声数据来消除这些特征。</p></li></ul><p><img src="/images/DA/1.png" title="Data Augmentation in CV vs NLP" /></p><p>相较于机器学习和计算机视觉领域，数据增强在NLP应用并没有前两者这么广泛。在NLP中，输入空间是离散的，我们需要关注如何生成有效的增广例子来捕获所需的不变性。因此它通常被比喻成“蛋糕上的樱桃”，只是提高有限的性能。</p><h1 id="主流技术">主流技术</h1><p>理想的DA技术应该既易于实现又能提高模型性能，但我们往往需要在这两者之间进行权衡。基于规则的技术很容易实现，但通常只能带来有限的性能改进。基于训练的模型的DA技术可能代价更大，但会引入更多的数据变化，导致更好的性能提升。为下游任务定制的基于模型的DA技术对性能有很强的影响，但很难开发和利用。</p><h2 id="rule-based-techniques">Rule-Based Techniques</h2><p>在特征空间内直接进行变化生成新的样本，比如在已知类别的数据之间进行“类比”转换，以扩充新的类；使用迭代的仿射变换和投影来沿着class-manifold最大限度地“拉伸”一个样本。</p><p>还有比如我们可以借助单词嵌入，如Word2Vec, GloVe, FastText, Sent2Vec，使用嵌入空间中最近邻的单词替换句子中的某个单词。 <img src="/images/DA/6.png" /></p><p>EASY DATA AUGMENTATION (EDA)：token-level的随机扰动操作，比如随机插入、删除、交换以及同义替换等。作者发现经过EDA后文本分类的准确率有了很大的提升。</p><p><img src="/images/DA/2.png" title="EDA" /></p><p>Unsupervised Data Augmentation (UDA)。一种基于无监督数据的数据增强方式，该方法通过对<span class="math inline">\((x, DA(x))\)</span>进行consistency training，生成无监督数据与原始无监督数据具备分布的一致性 <img src="/images/DA/3.png" title="UDA" /></p><p>在数据上构建带标记的图，将单个句子作为节点，配对的标签作为带标记的边。使用平衡理论和及物性从这个图中推断扩充句子对。 <img src="/images/DA/4.png" title="Graph Theory DA" /></p><p>Dependency tree morphing DA：受图像裁剪和旋转的启发，Şahin和Steedman提出了依赖树构建方法。对于带有依赖项注释的句子，可以交换或删除具有相同父节点的子节点。 <img src="/images/DA/5.png" title="Dependency tree morphing" /></p><h2 id="example-interpolation-techniques">Example Interpolation Techniques</h2><p>Mixed Sample Data Augmentation (MSDA) /MixUp。这种插值方法在图像处理领域应用非常广泛</p><p><span class="math inline">\(\tilde{x}=\lambda x_{i}+(1-\lambda) x_{j}, \quad\)</span> where <span class="math inline">\(x_{i}, x_{j}\)</span> are raw input vectors</p><p><span class="math inline">\(\tilde{y}=\lambda y_{i}+(1-\lambda) y_{j}, \quad\)</span> where <span class="math inline">\(y_{i}, y_{j}\)</span> are one-hot label encodings</p><p><img src="/images/DA/10.png" /></p><p>不同于MSDA这种连续插值，CUTMIX用从图像B中采样的一个patch替换图像A中的一个小的子区域，标签按照子区域大小的比例混合。对于NLP来说，例如涉及图像和文本的多模态问题可以借鉴这一工作的思想。</p><p>SEQ2MIXUP：（sequence-level variant of MixUp）对于sequence-to-sequence的模型，将输入/输出序列对进行一定的组合。 <span class="math display">\[\begin{array}{r}(\hat{X}, \hat{Y})=\left(m_{X} \odot X+\left(1-m_{X}\right) \odot X^{\prime}\right. \\\left.m_{Y} \odot Y+\left(1-m_{Y}\right) \odot Y^{\prime}\right)\end{array}\]</span></p><p><span class="math inline">\(m=\left[m_{X}, m_{Y}\right]\)</span>是一个系数向量。这种方法对于基于transformer的机器翻译、语义解析等任务都有明显的提升。</p><h2 id="model-based-techniques">Model-Based Techniques</h2><p>Seq2seq model以及language model都可以被用来做数据增强。</p><p>DiPS 原本是用于Diverse Paraphrasing（复述）任务的模型，该任务的度量标准为语义的相似性以及句子本身的差异性，但是我们也可以借助这些模型来进行数据增强。 <img src="/images/DA/8.png" title="DiPS during decoding to generate k paraphrases" /></p><p>类似的基于Transformer、BERT的模型从预训练的嵌入空间中，使用上下文敏感的、基于注意力的语义邻居混合来增强单词表示。</p><p><img src="/images/DA/7.png" title="综合对比图。Ext.Know是指DA方法是否需要外部知识(如WordNet)，如果需要预训练模型(如BERT)，则需要预训练。Preprocess表示需要进行预处理，Level表示数据被DA修改的深度，Task-Agnostic表示DA方法是否可以应用于不同的任务。Ext.Know、KWE、tok、const和dep分别代表外部知识、关键字提取、字符化、分组解析和依赖解析。" /></p><h1 id="应用">应用</h1><h2 id="low-resource-languages-few-shot-learning">Low-Resource Languages &amp;&amp; Few-Shot Learning</h2><p>低资源语言是DA的一个重要和具有挑战性的应用，尤其是神经机器翻译(NMT)。使用外部知识的技术很难将高资源语言用于低资源语言，特别是当它们具有相似的语言属性时。<br /><img src="/images/DA/9.png" title="Low-Resource Languages NMT" /> 如上图所示，我们可以借助一个high-resource language作为中枢来转化目标语言以及low-resource language。当high-resource language与low-resource language属于相同语系或具有类似性质时，我们将（HRL-ENG）数据集借助数据增强转化为（LRL-ENG） 数据集。对于其他任务，我们也可以通过DA来扩充low-resource language。</p><h2 id="mitigating-bias-fixing-class-imbalance">Mitigating Bias &amp;&amp; Fixing Class Imbalance</h2><p>以性别的偏差为例，可以借助DA来缓解性别偏见：创建一个与原始数据相同但偏向于未被充分代表的性别的增强数据集(使用实体的性别交换，如将“他”替换为“她”)来缓解引用解析中的性别偏见，并对这两个数据集进行联合训练。后续也有更好的工作比如缓解性别偏见的COUNTERFACTUAL DA (CDA)方法来打破性别词和中性词之间关联的因果干预。</p><p>类似也可以用于解决某些类别中的采样不足和采样过度问题。比如通过插值增强少数群体类的例子，以平衡多标签分类的分类。</p><h1 id="当前挑战和未来研究方向">当前挑战和未来研究方向</h1><p>应用到数据增强的NLP任务有很多，首先最基本的就是分类，这也是用来测试数据增强技术效果的基本方式之一。其他的任务包括摘要、问答、序列标注、Data-to-Text自然语言生成等以及多模态的任务如automatic speech recognition。</p><p>但是目前为止的研究存在以下方面的缺陷：</p><ul><li><p>明显缺乏关于DA为什么有效的研究。大多数研究都根据经验表明，以及基于一些实验表明DA技术是有效的，但目前很难在不诉诸于全面实验的情况下衡量技术的优劣。</p></li><li><p>多模态领域做数据增强。尽管多模态数据分析的工作有所增加，但许多研究都集中在单个模态或多个模态的扩展上。任需进一步探索诸如图像和文本同时增强的图像字幕方式。</p></li><li><p>基于范围的任务。例如，随机token替换可能是局部可接受的DA方法，但可能会破坏后面句子的共引用链。此时DA技术必须考虑文本中不同位置之间的依赖关系。</p></li><li><p>在专业领域工作，比如那些具有特定领域词汇和术语的领域(如医学)，许多预先训练的模型和外部知识不能被有效地使用。研究表明，当应用于特定领域数据时，DA变得不那么有效，这可能是因为增广数据的分布可能与原始数据有很大不同。</p></li><li><p>另一方面，针对数据增强所应对的少样本问题，我们还能从半监督学习角度考虑，结合数据增强与半监督学习技术是一个不错的选择。半监督学习能够充分利用大量未标注数据，同时能够使输入空间的变化更加平滑。</p></li></ul><h1 id="补充材料">补充材料</h1><p>一些关于NLP中数据增强方法具体操作的介绍： https://amitness.com/2020/05/data-augmentation-for-nlp/<br />NLP数据增强工具包：<a href="https://github.com/makcedward/nlpaug">nlpaug</a><br />类似的综述性文章：<br /><a href="https://arxiv.org/pdf/2105.11741.pdf">Data Augmentation Approaches in Natural Language Processing: A Survey</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;link: &lt;a href=&quot;https://arxiv.org/abs/2105.03075v4&quot; class=&quot;uri&quot;&gt;https://arxiv.org/abs/2105.03075v4&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;数据增强的背景&quot;&gt;数据增强的背景&lt;/h1&gt;
&lt;</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="Data_Aug" scheme="http://example.com/tags/Data-Aug/"/>
    
  </entry>
  
  <entry>
    <title>中文分词｜Chinese-word-segmentation (3)-基于词</title>
    <link href="http://example.com/2022/02/10/cws3/"/>
    <id>http://example.com/2022/02/10/cws3/</id>
    <published>2022-02-10T05:15:50.000Z</published>
    <updated>2022-02-15T12:23:59.232Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前情提要">前情提要</h1><p>在前面两篇Blog（<a href="/2022/02/07/cws/" title="中文分词1">中文分词1</a>、<a href="/2022/02/09/cws2/" title="中文分词2">中文分词2</a>）介绍了关于中文分词的基本方法以及近年来基于深度学习的部分分词模型，即基于字符的分词技术，接下来讲介绍基于词的中文分词模型。</p><h1 id="基本框架">基本框架</h1><p>Neural Word Segmentation Learning for Chinese</p><p><img src="/images/cws/11.png" title="neural network scoring model" /> 基于的框架如上图所示，不同于基于字符标注的模型，基于词的模型根据字符向量生成词向量。具体而言，模型得到输入的字符向量后，使用一个Gated Combination Neural Network，将L个字符向量组合，生成候选词向量： <span class="math display">\[\mathbf{w}=g\left(\mathbf{W}^{(L)}\left[\begin{array}{c}\mathbf{c}_{1} \\\vdots \\\mathbf{c}_{L}\end{array}\right]\right)\]</span></p><p><span class="math inline">\(\mathbf{W}^{(L)}\)</span>是一个所有词共享的权重。这GCNN中包含reset gate 和 update gate。</p><p><span class="math display">\[\mathbf{w}=\mathbf{z}_{N} \odot \hat{\mathbf{w}}+\sum_{i=1}^{L} \mathbf{z}_{i} \odot \mathbf{c}_{i}\]</span></p><p><span class="math display">\[\hat{\mathbf{w}}=\tanh \left(\mathbf{W}^{(L)}\left[\begin{array}{c}\mathbf{r}_{1} \odot \mathbf{c}_{1} \\\vdots \\\mathbf{r}_{L} \odot \mathbf{c}_{L}\end{array}\right]\right)\]</span></p><p><span class="math inline">\(\mathbf{W}^{(L)} \in \mathbb{R}^{d \times L d}\)</span> 和 <span class="math inline">\(\mathbf{r}_{i} \in \mathbb{R}^{d}(1 \leq i \leq L)\)</span> 表示reset gates，用来决定字符向量的哪部分被结合到词向量中： <span class="math display">\[\left[\begin{array}{c}\mathbf{r}_{1} \\\vdots \\\mathbf{r}_{L}\end{array}\right]=\sigma\left(\mathbf{R}^{(L)}\left[\begin{array}{c}\mathbf{c}_{1} \\\vdots \\\mathbf{c}_{L}\end{array}\right]\right)\]</span> 而update gate则是 <span class="math display">\[\left[\begin{array}{c}\mathbf{z}_{N} \\\mathbf{z}_{1} \\\vdots \\\mathbf{z}_{L}\end{array}\right]=\exp \left(\mathbf{U}^{(L)}\left[\begin{array}{c}\hat{\mathbf{w}} \\\mathbf{c}_{1} \\\vdots \\\mathbf{c}_{L}\end{array}\right]\right) \odot\left[\begin{array}{c}1 / \mathbf{Z} \\1 / \mathbf{Z} \\\vdots \\1 / \mathbf{Z}\end{array}\right]\]</span> 其中<span class="math inline">\(\mathbf{U}^{(L)} \in \mathbb{R}^{(L+1) d \times(L+1) d}\)</span> 是系数矩阵，<span class="math inline">\(\mathbf{Z} \in \mathbb{R}^{d}\)</span>是归一化向量。</p><p>得到候选词向量之后，每个输入句子中的词向量会被转化为一个分数word_score，代表这个词有多大的可能性是一个真实存在的词。这些word_score会与经过LSTM之后的词向量共同组合成sentence score。</p><p>得到分数之后，之前多数序列标注的分词方法多使用Viterbi算法进行动态规划，得到最优的分词方法，但是在这个模型中，由于可能的句子分词方式总数太大，且为了捕捉完整的分割决策（不同于基于字符标注的方法），模型使用了Beam Search算法作为Decoder。</p><h1 id="改进模型">改进模型</h1><p>Fast and Accurate Neural Word Segmentation for Chinese</p><p>首先，这篇文章的作者对字符生成候选词向量的GCNN网络进行了改进。模型引入了一个高频词词典，在词典中则直接对character embedding进行average pooling，而不在词典中则根据构成的字符向量生成词向量，如下图所示 <img src="/images/cws/13.png" /> <span class="math display">\[\operatorname{COMP}\left(c_{1} . . c_{l}\right)=\tanh \left(\mathbf{W}_{l}^{c}\left[\mathbf{r}_{1} \odot \mathbf{c}_{1} ; \ldots ; \mathbf{r}_{l} \odot \mathbf{c}_{l}\right]+\mathbf{b}_{l}^{c}\right)\]</span> <span class="math display">\[\left[\mathbf{r}_{1} ; \ldots ; \mathbf{r}_{l}\right]=\operatorname{sigmoid}\left(\mathbf{W}_{l}^{r}\left[\mathbf{c}_{1} ; \ldots ; \mathbf{c}_{l}\right]+\mathbf{b}_{l}^{r}\right)\]</span> 作者讲gate mechanism进行了简化，使得模型训练更加快速。另外，原模型中的Beam Search也改为了贪心算法，采用了两种训练方法：Early update、LaSO update。Early update指的是一旦最优的分割无法实现，就立即更新。Early update的一个缺点是，搜索可能永远不会到达训练样本的末尾，这意味着数据的其余部分是“浪费”的。而LaSO update在每次更新后都在相同的实例上继续进行正确的假设（将正确的分词序列的对应前缀插入实例中）。</p><p>Transition-Based Neural Word Segmentation</p><p><img src="/images/cws/14.png" title="Transition-Based Neural Model" /> 上图为Transition-Based分词方法，它使用Transition system递增地去分词。Buffer部分用于存储句子已经分词的部分，未分词的在Queue中。action包括SEP-separate和APP-append，分别指分割和把字符pop到Buffer中。模型分词的过程就是寻找一个action序列的过程。</p><p>这个框架中的基本的特征包含三方面的信息。第一个信息是序列q中第一个字符和buffer中的最后一个字符用来分别给SEP和APP动作来打分。第二个信息是通过已经被识别的词来指导SEP。第三个信息是已识别的词的相关信息，比如它们的长度，这个词中的第一个字符或是最后一个字符可以作为额外的特征。从图中可以看到三个RNN分别编码word sequence、character sequence以及action sequence信息。而论文的作者则在这一框架的基础上用LSTM替换了原有的RNN。</p><h1 id="pre-train-model">Pre-train model</h1><p>BERT诞生后横扫各大NLP任务的榜单，中文分词自然也逃不了它的魔爪。BERT作为特征提取器，基于BERT的分词模型事实上大部分属于基于字符的分词技术。而基于BERT的分词模型的关注点则主要包括：在 模型中融合自定义词典、外部知识（领域知识）；如何大模型蒸馏成一个小的模型来提高分词性能；如何通过不同粒度标准的分词预料联合预训练，让分词能够通过某些简单的控制能够适应不用的分词场景。</p><p>部分相关论文：<br />Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter<br />Pre-training with Meta Learning for Chinese Word Segmentation<br />ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations （这个模型缩写就离谱）</p><p><img src="/images/cws/12.png" title="ZEN：Bert+N-gram" /></p><h1 id="写在最后">写在最后</h1><p><img src="/images/cws/15.png" title="paperwithcode上中文分词任务" /> 目前在几个公开数据集上分词模型的分词准确率都达到了97%以上。作为一个对于中文NLP来说非常重要的任务，中文分词可以说基于属于已经解决的任务，毕竟分词本就很难有绝对统一的标准。而就目前来说，相较于刷新榜单，或许更重要的是寻找更好解决OOV问题的方法（毕竟每段时间都有大量新词涌现）以及研发速度更快、体量更小的模型。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前情提要&quot;&gt;前情提要&lt;/h1&gt;
&lt;p&gt;在前面两篇Blog（&lt;a href=&quot;/2022/02/07/cws/&quot; title=&quot;中文分词1&quot;&gt;中文分词1&lt;/a&gt;、&lt;a href=&quot;/2022/02/09/cws2/&quot; title=&quot;中文分词2&quot;&gt;中文分词2&lt;/a&gt;）</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>中文分词｜Chinese-word-segmentation (2)-基于字符</title>
    <link href="http://example.com/2022/02/09/cws2/"/>
    <id>http://example.com/2022/02/09/cws2/</id>
    <published>2022-02-09T09:15:47.000Z</published>
    <updated>2022-02-13T16:37:31.413Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">背景</h1><p>随着深度学习技术的发展，基于深度神经网络的中文分词模型也不断涌现，这类分词技术大致上可以分为两类，其中一类基于字符的中文分词方法将分词看作字符的标注问题。根据字在词中的位置，分别给每个中文字符打上B、M、E、S的标签，将分词转化为字的标签预测。</p><h1 id="深度神经网络">深度神经网络</h1><p>Deep Learning for Chinese Word Segmentation and POS Tagging</p><p><img src="/images/cws/5.png" title="模型框架" /> 这篇论文是早期使用神经网络来做中文分词和词性标注的联合任务。对比机器学习模型，早期的神经网络致力于改善手工设计或选择特征这一问题，借助深层网络自动获取任务相关的字符特征。作为统计语言模型，这种方法利用大规模非标注数据来改善中文字符的内在表示，然后使用这些改善后的表示来提高有监督的分词模型和词性标注模型的性能。</p><p>对于输入句子中的每一个字，神经网络架构将为其可能的每一个TAG进行评分，为了解决不同句子对应的字序列长短不一的问题，本文中采用的是窗口方法。窗口方法假定一个字的tag主要依赖于与其相邻的字。具体而言，如上图所示，首先对输入的句子中的每个字查词典：通过lookup层得到窗口中每个字的字向量。之后将每个窗口长度的字向量首尾相连得到一个新的特征。接下来经过3层基础的神经网络。神经网络的输出是一个包含每个字可能标签的得分的矩阵。最后使用Viterbi算法进行动态规划完成标注的推断。</p><h1 id="lstm">LSTM</h1><p>Long Short-Term Memory Neural Networks for Chinese Word Segmentation</p><p>基于DNN的分词模型所能关注的是每个字符的窗口内邻近字符的特征，而LSTM则能解决句子内字符的长期依存关系问题。举个简单的例子：<br />冬天，能穿/多少/穿/多少，夏天，能穿/多/少/穿/多/少。<br />此时这个“多少”的分词就需要根据句子开头的“冬天”和“夏天”来确定。 <img src="/images/cws/6.png" title="LSTM进行分词" /></p><p>因此模型在原有深度学习分词的框架下，将传统的3层神经网络改为LSTM，借助输入输出门和遗忘门来传递上文信息。</p><p><span class="math display">\[\begin{aligned}\mathbf{i}^{(t)} &amp;=\sigma\left(\mathbf{W}_{i x} \mathbf{x}^{(t)}+\mathbf{W}_{i h} \mathbf{h}^{(t-1)}+\mathbf{W}_{i c} \mathbf{c}^{(t-1)}\right) \\\mathbf{f}^{(t)} &amp;=\sigma\left(\mathbf{W}_{f x} \mathbf{x}^{(t)}+\mathbf{W}_{f h} \mathbf{h}^{(t-1)}+\mathbf{W}_{f c} \mathbf{c}^{(t-1)}\right) \\\mathbf{c}^{(t)} &amp;=\mathbf{f}^{(t)} \odot \mathbf{c}^{(t-1)}+\mathbf{i}^{(t)} \odot \phi\left(\mathbf{W}_{c x} \mathbf{x}^{(t)}+\mathbf{W}_{c h} \mathbf{h}^{(t-1)}\right) \\\mathbf{o}^{(t)} &amp;=\sigma\left(\mathbf{W}_{o x} \mathbf{x}^{(t)}+\mathbf{W}_{o h} \mathbf{h}^{(t-1)}+\mathbf{W}_{o c} \mathbf{c}^{(t)}\right) \\\mathbf{h}^{(t)} &amp;=\mathbf{o}^{(t)} \odot \phi\left(\mathbf{c}^{(t)}\right)\end{aligned}\]</span></p><p>但是在这个模型中，我们可以发现一个明显的问题，模型只能关注字符上距离的上文内容而无法获取下文信息用于辅助分词。针对这一问题，就有了Bi-LSTM，用双向LSTM来利用上下文信息。</p><p>Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation</p><p>这篇论文使用了双向LSTM，即两个并行的LSTM分别来从左到右和从右到左来提取长距离字符特征。另外，模型输出为softmax之后的概率向量，比起之前的工作省略了viterbi推断的过程，变为end-end的模型。 <img src="/images/cws/7.png" title="Bi-LSTM进行分词" /></p><h1 id="multi-criteria-learning">Multi-Criteria Learning</h1><p>Adversarial Multi-Criteria Learning for Chinese Word Segmentation</p><p>不同的语料库有不同的分词标准，这篇文章的作者试图借助对多个不同标准的分词语料进行模型训练，提取中分词方法中最具普适性的部分。具体而言，模型借助Multi-criteria learning来获取多个准则的共享权重和独有权重，另一方面，通过鉴别器的对抗性训练来更好的实现这一过程，使得多准则的共享特性被更好地提取。</p><p>整体模型架构是建立在Bi—LSTM模型下，而针对具体的特征提取网络，作者针对多准则学习设计了三种网络结构，如下图所示 <img src="/images/cws/9.png" /> 其中黄色为共享LSTM层，灰色为私有LSTM层，<span class="math inline">\(\Theta^{m}, \Theta^{s}\)</span>分别代表他们的参数。每种模型都有两种准则进行训练，训练的目标函数是所有语料库上数据的条件似然。</p><p><span class="math display">\[\mathcal{J}_{s e g}\left(\Theta^{m}, \Theta^{s}\right)=\sum_{m=1}^{M} \sum_{i=1}^{N_{m}} \log p\left(Y_{i}^{(m)} \mid X_{i}^{(m)} ; \Theta^{m}, \Theta^{s}\right)\]</span></p><p>而为了确保共享层中没有参杂特定准则的私有信息，模型引入了一个鉴别器进行对抗训练。 <img src="/images/cws/8.png" title="对抗训练（以Model-III为例）" /> 判别器的任务是预测某一特征向量来源于 多准则语料中的哪一个。假如判别器能够准确预测每一个共享特征向量的来源语料，则说明这些共享特征中混入了太多私有信息。而共享LSTM层的目标则是让判别器无法鉴别输出的特征向量来源于哪个预料。模型通过令这两者进行对抗性训练，使得共享层能够提取出多个准则中最本质的分词特性。</p><h1 id="joint-cws-and-pos-tagging">Joint CWS and POS Tagging</h1><p>A Feature-Enriched Neural Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</p><p>本文的模型将Chinese word segmentation与part-of-speech (POS) tagging两个任务进行联合训练，因为其本质上都属于character based sequence labeling task。而论文作者的改进之处在于一个精心设计的特征提取网络 Feature-Enriched Neural Model。</p><p><img src="/images/cws/10.png" title="Feature-Enriched Neural Model" /></p><p>抛开原有的框架，这个模型增加了2个模块——Convolutional layer; Highway layer。</p><p>首先，卷积层用来对标传统方法中的手工字符特征部分。作者认为简单的神经模型只是将字符的局部信息嵌入并连接起来，不能模拟传统机器学习模型中精心设计的特征。为了像传统的基于特征的模型那样更好地建模复杂的字符特征，作者使用卷积层对每个特征分别建模不同的n-gram特征并串联，然后我们k-max池化层来选择最显著的部分。</p><p><span class="math inline">\(\hat{\mathbf{z}}_{i}^{q}\)</span>代表Q-gram特征 (uni-gram, bi-gram, ... ,)，而<span class="math inline">\(\mathbf{W}_{c o v}^{q} \in \mathbf{R}^{q d \times l_{q}}\)</span> 代表convolutional filter</p><p><span class="math display">\[\hat{\mathbf{z}}_{i}^{q}=\tanh \left(\mathbf{W}_{\operatorname{cov}}^{q}{ }^{\top} \times \mathbf{x}_{i-\left|\frac{q-1}{2}\right|: i+\left[\frac{q-1}{2}\right]}^{+}+\mathbf{b}\right), i \in[1, n]\]</span></p><p>而后分别进行concatenation、k-max pooling操作 <span class="math display">\[\mathbf{z}_{i}=\oplus_{q=1}^{Q} \hat{\mathbf{z}}_{i}\]</span></p><p>此时，输入的原始句子就会被表示为<span class="math inline">\(\hat{\mathbf{X}} \in \mathbf{R}^{n \times d}=\)</span> <span class="math inline">\(\left[\hat{\mathbf{x}}_{1}, \hat{\mathbf{x}}_{2}, \ldots, \hat{\mathbf{x}}_{n}\right]^{\top}\)</span>, 其中<span class="math inline">\(\hat{\mathbf{x}}_{i}\)</span>为:</p><p><span class="math display">\[{\hat{\mathbf{x}}}_{i}=k \max \mathbf{z}_{i}, k=d .\]</span></p><p>Highway layer则用来增加结构的深度，来模拟更复杂的组合特征。此外，highway加快了模型的收敛速度，缓解了梯度消失的问题。（这部分是参考Highway Netowrk（2015）的模型）</p><p>我们讲卷积后的句子表示为 <span class="math inline">\(\hat{\mathbf{X}}=\operatorname{Cov}(\mathbf{X})\)</span>，而其经过Highway layer后会得到 <span class="math display">\[\hat{\mathbf{X}}=\operatorname{Cov}(\mathbf{X}) \odot T(\mathbf{X})+\mathbf{X} \odot C(\mathbf{X})\]</span> 其中<span class="math inline">\(\odot\)</span>代表按元素乘运算，<span class="math inline">\(C(\cdot)=1-T(\cdot)\)</span>，而<span class="math inline">\(T(\cdot)\)</span>则可以写为： <span class="math display">\[T(\mathbf{X})=\sigma\left(\mathbf{W}_{T}{ }^{\boldsymbol{\top}} \times \mathbf{X}+\mathbf{b}_{T}\right)\]</span> 其中<span class="math inline">\(\mathbf{W}_{T} \in \mathbf{R}^{d \times d}\)</span>、<span class="math inline">\(\mathbf{b}_{T} \in \mathbf{R}^{d}\)</span>是可训练参数，<span class="math inline">\(\sigma\)</span>是sigmoid函数。</p><p>而接下来就将结果输入到BLSTM中，并借助CRF作为Decoder得到最终的结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;
&lt;p&gt;随着深度学习技术的发展，基于深度神经网络的中文分词模型也不断涌现，这类分词技术大致上可以分为两类，其中一类基于字符的中文分词方法将分词看作字符的标注问题。根据字在词中的位置，分别给每个中文字符打上B、M、E、S的标签，将分词转化为字的</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>中文分词｜Chinese-word-segmentation (1)</title>
    <link href="http://example.com/2022/02/07/cws/"/>
    <id>http://example.com/2022/02/07/cws/</id>
    <published>2022-02-07T13:01:01.000Z</published>
    <updated>2022-02-09T15:42:51.014Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">背景</h1><p>中文文本中词与词之间没有明确的分割标记，而是以连续字符串形式呈现。所以，任何中文自然语言处理任务都必须解决中文序列切分的问题——中文分词。当然对于英文等语言来说自带分隔符，不需要分词。但在英语手写字识别时，由于分隔符没有那么明显，因此也需要使用类似中文分词的技术。</p><p>事实上不同的人对于同一个句子的分词结果并不一定相同，因此将机器分词的结果与人工分词的结果进行比对时，一个98%准确率的分词器与一个99%的分词器很难比较谁效果更好。在中文自然语言处理任务中，诸如机器翻译，自动问答，语音识别等任务，都需要分词技术做支撑。但是对于不同的子任务，分词的细粒度要求也不一样，尤其是对于一些复合词。比如在机器翻译时，清华大学就应该以整个“清华大学”而不是“清华-大学”。可以说中文分词在工程上是一项需要与任务要求紧密结合的技术。</p><p>中文分词存在两个难点，一是歧义，二是未登录词（OOV-out of vocabulary）。语言的歧义性一直伴随着语言学的发展进程，也自然限制着NLP技术。另一方面，词典的选择诸如词典中的复合词选择和词典的大小也影响着中文分词任务。举个直观的例子，对于Baidu搜索引擎的分词模型来说，基于人民日报的词典和基于用户搜索数据的词典得到的模型表现就会有很大差异。</p><h1 id="分词方法的演变">分词方法的演变</h1><h2 id="基于匹配的词典分词">基于匹配的词典分词</h2><p>基于匹配的词典分词是非常自然的想法，我们根据词典扫描一个句子，遇到词典中出现过的词就进行分割，这类方法又被称为机械分词。这类方法简单易实现，而且能取得不错的效果，其主要问题包括如何构建一个完备的词典、如何设计高效的匹配算法、匹配中出现的歧义切分。<br />常见的匹配算法包括:</p><ul><li>正向最大匹配法</li><li>逆向最大匹配法</li><li>双向最大匹配法</li><li>最少切分</li><li>......</li></ul><p>最大匹配法从句子中寻找长词条进行查字典，若查不到则去掉最后一个字直到找到为止。其中双向最大匹配分别从左到右和从右到左进行两次扫描。最小切分则是寻找使每一个句子切出的词数量最少。</p><h2 id="基于标注的机器学习算法">基于标注的机器学习算法</h2><p>不同于基于匹配的机械分词，基于统计语言模型的分词技术有效提高了分词的准确率。其中基于标注的机器学习算法将中文分词转化为字序列标注问题。，B表示开始位置、M表示中间位置、E表示结束位置及S表示单字构词。机器学习算法 需要人工设计特征模板，指定窗口的大小。由于算法的复杂度以及对分词结果准确度要求等原因，窗口大小一般不超过5。下面介绍几个具有代表性的模型：</p><ul><li><p>隐马尔可夫模型（HMM）隐马尔可夫不是一个复杂的数学模型，但能解决大多数自然语言处理问题。其基本的思想是根据观测值序列找到隐状态值序列。在中文分词中，一段文字的每个字符可以看作是一个观测值，而这个字符的位置标签（BEMS）可以看作是隐状态。使用HMM的分词，通过对切分语料库进行统计，可以得到模型中5大要要素：起始概率矩阵，转移概率矩阵，发射概率矩阵，观察值集合，状态值集合。有了三个矩阵和两个集合后，HMM问题最终转化成求解隐藏状态序列最大值的问题，求解这个问题最常使用的是Viterbi算法。</p></li><li><p>最大熵马尔可夫模型（MEMM）把HMM模型和maximum-entropy模型的优点集合程一个产生式模型，这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度。</p></li><li><p>条件随机场（CRF）是用来标注和划分结构数据的概率化结构模型。和HMM类似，当对于给定的输入观测序列<span class="math inline">\(X\)</span>和输出序列<span class="math inline">\(Y\)</span>，CRF通过定义条件概率<span class="math inline">\(P(Y|X)\)</span>，而不是联合概率分布<span class="math inline">\(P(X,Y)\)</span>来描述模型。MEMM模型对每个节点进行独立归一化，存在偏置问题。条件随机场(CRF)结合了多方面优势，对所有特征进行全局归一化，避免了偏置问题，成为传统机器学习中应用最多、最具代表性的模型算法之一。条件随机场能够获得更高的分词准确率，但模型复杂导致分词效率略低。</p></li></ul><h2 id="基于理解的深度学习算法">基于理解的深度学习算法</h2><p>深度学习模型诸如CNN、GRU、LSTM、BiLSTM被引入中文分词，相对于机器学习而言，深度学习算法无需人工进行特征选择。在基础深度学习模型的基础上，有效结合预训练和后处理方式已成为深度学习的一种趋势，一般性流程如下图所示。 <img src="/images/cws/1.png" title="基于深度学习的中文分词" /> 预训练既可以根据领域需要和任务特点进行预训练，也可以直接使用现有的预训练结果进行微调。中文分词预训练的基本单位是词(字)的语义、偏旁、拼音和输人法等。语义表示的预训练模型包括与上下文无关的静态词向量训练模型Word2Vec、Glove以及与上下文相关的动态词向量训练模型ELMo、BERT、XLNet等。<br />近几年的中文分词主要分为两类，一个是基于字符的中文分词（根据字所在词的位置，对每个字打上标签），一类是基于词的中文分词。</p><h1 id="分词工具">分词工具</h1><h2 id="jieba">jieba</h2><p>jieba库是一个简单实用的中文自然语言处理分词库，属于概率语言模型分词。<br />jieba自带一个dict.txt的词典, 里面有2万多条词, 包含了词条出现的次数和词性。将句子根据给定的词典进行查词典操作, 生成所有可能的句子切分，而后根据动态规划查找最大概率路径, 找出基于词频的最大切分组合。对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。<br />（jieba分词对“自然语言处理”的分词结果为 自然语言｜处理）</p><h2 id="ltp">LTP</h2><p>LTP是哈工大开源的一套中文语言处理系统，涵盖了基本功能：分词、词性标注、命名实体识别、依存句法分析、语义角色标注、语义依存分析等。LTP基于结构化感知器（Structured Perceptron,SP）属于基于字符的分词模型，以最大熵准则建模标注序列<span class="math inline">\(Y\)</span>在输入序列<span class="math inline">\(X\)</span>的情况下的score函数，分词结果则等同于最大score函数所对应的标注序列。 <span class="math display">\[S(Y, X)=\sum_{s} \alpha_{s} \Phi_{s}(Y, X)\]</span> <span class="math inline">\(\Phi_{s}(Y, X)\)</span>为特征函数，分词流程为先提取字符特征，计算特征权重值，然后Viterbi解码。</p><h2 id="thula">THULA</h2><p>THULA（THU Lexical Analyzer for Chinese）为清华大学推出的中文词法分析工具包，具有中文分词和词性标注功能，其原理与LTP非常相似，在字符特征选择方面有所不同。<br />测试发现THULA没有针对python3.8进行维护，因此只支持3.7-版本。</p><h2 id="stanford-corenlp">Stanford CoreNLP</h2><p>CoreNLP的中文分词基于CRF模型： <span class="math display">\[P_{w}(y \mid x)=\frac{\exp \left(\sum_{i} w_{i} f_{i}(x, y)\right)}{Z_{w}(x)}\]</span></p><p><span class="math inline">\(f_{i}(x, y)\)</span>为特征函数，<span class="math inline">\(w\)</span>为模型参数。不同于其他分词器采用B、M、E、S四种label来做分词，CoreNLP的中文分词label只有两种，“1”表示当前字符与前一字符连接成词，“0”则表示当前字符为另一词的开始——换言之前一字符为上一个词的结尾。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;
&lt;p&gt;中文文本中词与词之间没有明确的分割标记，而是以连续字符串形式呈现。所以，任何中文自然语言处理任务都必须解决中文序列切分的问题——中文分词。当然对于英文等语言来说自带分隔符，不需要分词。但在英语手写字识别时，由于分隔符没有那么明显，因此</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>Improving Chinese Word Segmentation with Wordhood Memory Networks [ACL20]</title>
    <link href="http://example.com/2022/02/05/pd12/"/>
    <id>http://example.com/2022/02/05/pd12/</id>
    <published>2022-02-05T15:41:35.000Z</published>
    <updated>2022-02-09T16:35:42.372Z</updated>
    
    <content type="html"><![CDATA[<p>link: <a href="https://aclanthology.org/2020.acl-main.734/" class="uri">https://aclanthology.org/2020.acl-main.734/</a><br /><a href="https://github.com/SVAIGBA/WMSeg">代码</a></p><h1 id="背景">背景</h1><p>在中文自然语言处理中，分词是一个非常重要的任务。中文分词技术存在两个主要难点:未登录词（OOV）和歧义消除问题。本文属于基于字符的分词模型，主要思想是利用键值记忆网络来辅助分词，使分词的语义更加完整。</p><h1 id="模型框架">模型框架</h1><p>模型将中文分词作为序列标注问题，核心思想是在传统NER模型的Encoder-Decoder之间添加一个memory network，Encoder可以用BERT或BiLSTM等将汉字序列表示为向量，Decoder可以是Softmax或者CRF。模型总体可以表示为： <span class="math display">\[\widehat{\mathcal{Y}}=\underset{\mathcal{Y} \in \mathcal{T}^{l}}{\arg \max } p(\mathcal{Y} \mid \mathcal{X}, \mathcal{M}(\mathcal{X}, \mathcal{N}))\]</span> 其中<span class="math inline">\(\mathcal{T}\)</span>表示句子中所有分词结果的标签集;<span class="math inline">\(l\)</span>是句子长度。<span class="math inline">\(\widehat{\mathcal{Y}}\)</span>为该模型得到的最佳结果，<span class="math inline">\(\mathcal{N}\)</span>为构造的词典，<span class="math inline">\(\mathcal{X}\)</span>为输入句，<span class="math inline">\(\mathcal{M}\)</span>为本文提出的模型。</p><p><img src="/images/cws/2.png" title="The architecture of $WMS_{EG}$." /></p><h2 id="词典构建">词典构建</h2><p>本文构建的词典实际上是一个N-gram词典，它包含了一个句子中所有可能的N-gram。模型利用前人的模型——Accessor Variety，找出输入句子中所有可能的n-gram集合。根据上图给出的例子，所构建的词典如图底部所示。</p><h2 id="wordhood-memory-networks">Wordhood Memory Networks</h2><p>这部分是本文最重要的部分。作者利用键值记忆网络将字符n-gram与它们的词伙（wordhood）度量相结合。其中key-value分别对应n-grams和wordhood。具体可以分成两个步骤：</p><ul><li><p>Key Addressing<br />首先对该句子构建Lexicon，对每一个汉字<span class="math inline">\(x_i\)</span> ，有可能存在很多包含该汉字的n-gram。比如上面的句子中的第四个字&quot;民&quot;构建Lexicon，可以表示为： <span class="math display">\[K_{4}=[\text {&quot;民&quot;,&quot; 居民&quot;, &quot; 民生&quot;, &quot; 居民生活&quot;] }\]</span>然后将这些n-gram进行key embeding后<span class="math inline">\(e_{i, j}^{k}\)</span>再与Encoder传来的<span class="math inline">\(h_i\)</span> 相乘之后做softmax得到一个概率分布。概率大小就表明了相关程度： <span class="math display">\[p_{i, j}=\frac{\exp \left(h_{i} \cdot e_{i, j}^{k}\right)}{\sum_{j=1}^{m_{i}} \exp \left(h_{i} \cdot e_{i, j}^{k}\right)}\]</span></p></li><li><p>Value Reading<br />先将每个<span class="math inline">\(k_i\)</span>映射到一个标注值V上，因为每个字在不同的n-gram中的位置不同，所以需要映射的值也不同，这里使用B I E S标记法：(B:begin ，I:inside，E:end，S:single)，还是上面的例子，对应Key Addressing时的<span class="math inline">\(K_4\)</span>，得到的value集合为： <span class="math display">\[V_{4}=\left[V_{S}, V_{E}, V_{B}, V_{I}\right]\]</span>而后将每个value进行embedding得到<span class="math inline">\(e_{i, j}^{v}\)</span>，与上一步得到的概率进行相乘累加，得到的结果再与Encoder传入的<span class="math inline">\(h_i\)</span>结合得到最终输出的vector <span class="math inline">\(a_i\)</span>。 <span class="math display">\[O_{i}=\sum_{j=1}^{m_{i}} p_{i, j} e_{i, j}^{v}\]</span></p></li></ul><p>接下来就可以和正常的NER模型一样使用一个decoder（softmax、crf等等） 得到一个句子的分词结果了。</p><h1 id="实验">实验</h1><p>1.消融实验。作者使用了5个基准数据集，分别是CTB6、MSR、PKU、AS和CITYU。将Wordhood Memory Networks加入到目前主流的分词模型中进行比较，实验结果表明，虽然原始的模型有很好的性能，但加入Wordhood Memory Networks后仍有很大的改进<br />2.在五个基准数据集的测试集上，WMSEG和以前的SOTA模型的性能比较。经过实验比较，本文提出的模型在中文分词方面达到了SOTA水平，OOV的召回率有了很大的提高。</p><p><img src="/images/cws/3.png" title="Ablation experiments." /></p><h1 id="例子分析">例子分析</h1><p>作者用“他从小学习电脑技术”这一句子的分词结果进行可视化分析，发现通过Wordhood Memory Networks对n-gram语义的捕捉，给了“从小”一个较高的权重、&quot;小学&quot;一个较低的权重，得到了正确的分词结果。 <img src="/images/cws/4.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;link: &lt;a href=&quot;https://aclanthology.org/2020.acl-main.734/&quot; class=&quot;uri&quot;&gt;https://aclanthology.org/2020.acl-main.734/&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;ht</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>Graph Wavelet Neural Network[ICLR2019]</title>
    <link href="http://example.com/2022/01/26/pd11-1/"/>
    <id>http://example.com/2022/01/26/pd11-1/</id>
    <published>2022-01-26T04:20:13.000Z</published>
    <updated>2022-01-28T08:22:30.110Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1904.07785.pdf">Graph Wavelet Neural Network</a><br />Code: <a href="https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork">link</a></p><p>基于小波变换的图卷积神经网络，其背后的思想及理论基础在这一篇论文中无法很详尽的阐述。不过本篇论文相当于把一套传统方法迁移到图上。</p><h1 id="intro">Intro</h1><p>作者提出了graph wavelet neural network (GWNN)，对于GCN基于传统的图傅里叶变换，GWNN利用graph wavelet transform，用图小波代替图拉普拉斯特征向量作为一组基，利用小波变换和卷积定理定义了卷积算子。使得计算更高效，且在Cora, Citeseer和Pubmed三个图的半监督分类数据集上取得了更好的表现。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.07785.pdf&quot;&gt;Graph Wavelet Neural Network&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;https://github.com/benedekrozembercz</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graphite-Iterative Generative Modeling of Graphs [ICML2019]</title>
    <link href="http://example.com/2022/01/18/pd11/"/>
    <id>http://example.com/2022/01/18/pd11/</id>
    <published>2022-01-18T13:47:01.000Z</published>
    <updated>2022-01-20T01:53:00.164Z</updated>
    
    <content type="html"><![CDATA[<p>ICML2019的论文，作者为Stanford的phd。</p><h1 id="先说几句">先说几句</h1><p>看到摘要</p><blockquote><p>Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding.</p></blockquote><p>基本可以确定模型相当于variational graph autoencoders，但是<a href="/2022/01/18/pd10/" title="VGAE">VGAE</a>这名字已经被人抢了XD。不过对比VGAE，它用了更优的Decoder。另一方面，作者给出了图神经网络中message passing与平均场变分推理之间的理论联系。</p><h1 id="graphite">Graphite</h1><p><img src="/images/pd10/8.png" /> 模型的整体架构与VGAE相似。其中Encoder为 <span class="math display">\[\boldsymbol{\mu}, \boldsymbol{\sigma}=\mathrm{GNN}_{\phi}(\mathbf{A}, \mathbf{X})\]</span> 接下来我们重点来看模型的Decoder。在VGAE中，Decoder是节点隐变量的内积，而Graphite提出了reverse message passing作为Decoder。 <span class="math display">\[\begin{aligned}\widehat{\mathbf{A}} &amp;=\frac{\mathbf{Z Z}^{T}}{\|\mathbf{Z}\|^{2}}+\mathbf{1 1}^{T} \\\mathbf{Z}^{*} &amp;=\operatorname{GNN}_{\theta}(\widehat{\mathbf{A}},[\mathbf{Z} \mid \mathbf{X}])\end{aligned}\]</span> 模型不断迭代上述两个步骤，也就是reverse message passing。 首先第一步借助隐变量矩阵的内积构建一个邻接矩阵（图）<span class="math inline">\(\widehat{\mathbf{A}} \in \mathbb{R}^{n \times n}\)</span>，加上单位矩阵保证非负。第二步中先将Z和X进行级联，而后与构建的图输入到GNN中。通过不断的重复来更新<span class="math inline">\(Z^*\)</span>，最后使用最终的Z进行内积得到生成的邻接矩阵<span class="math inline">\(\hat A\)</span>。另外，由于图学习通常是在大规模图上操作，在迭代过程中的求内积可以借助GNN这一步中的矩阵乘法来降低复杂度。</p><p><img src="/images/pd10/9.png" title="实验结果" /></p><h1 id="theoretical-analysis">Theoretical Analysis</h1><p>这一部分是作者对图神经网络的message passing与近似推断的关系。<br />首先我们定义kennel，<span class="math inline">\(K: \mathcal{Z} \times \mathcal{Z} \rightarrow \mathbb{R}\)</span>；映射函数 <span class="math inline">\(T_{\psi}: \mathcal{P} \rightarrow \mathcal{H}\)</span> 其中<span class="math inline">\(\mathcal{P}\)</span>定义了<span class="math inline">\(\mathcal{Z}\)</span>上所有分布的空间。因此可以定义对变量Z的分布的映射，或者成为kernel embedding： <span class="math display">\[T_{\psi}(p):=\mathbb{E}_{Z \sim p}[\psi(Z)]\]</span> 我们令这种映射<span class="math inline">\(\psi\)</span>是单射的，即对于任意两个分布<span class="math inline">\(p_1,p_2\)</span>，当<span class="math inline">\(p_{1} \neq p_{2}\)</span>，有<span class="math inline">\(T_{\psi}\left(p_{1}\right) \neq T_{\psi}\left(p_{2}\right)\)</span>。接下来我们定义 函数<span class="math inline">\(\mathcal{O}: \mathcal{P} \rightarrow \mathbb{R}^{d}, d \in \mathbb{N}\)</span>，对于每个<span class="math inline">\(T_{\psi}\)</span>和<span class="math inline">\(\mathcal{O}\)</span>，都存在<span class="math inline">\(\tilde{\mathcal{O}}_{\psi}: \mathcal{H} \rightarrow \mathbb{R}^{d}\)</span>使得: <span class="math display">\[\mathcal{O}(p)=\tilde{\mathcal{O}}_{\psi}\left(T_{\psi}(p)\right) \quad \forall p \in \mathcal{P}.\]</span></p><p>在GNN中，我们假设<span class="math inline">\(X\)</span>和<span class="math inline">\(A\)</span>都是观测到且在隐变量的条件概率分布中的相互独立的。也就是说我们期望的图是满足，对于<span class="math inline">\(Z\)</span>上的条件分布，当<span class="math inline">\(A\)</span>、<span class="math inline">\(X\)</span>和由边集<span class="math inline">\(E\)</span>确定的节点i的邻接潜变量时，任意单个<span class="math inline">\(Z_i\)</span>与所有其他<span class="math inline">\(Z_j\)</span>都是独立的。这句话非常抽象，可以从下图来理解。 <img src="/images/pd10/10.png" /></p><p>当图<span class="math inline">\(G\)</span>满足这一条件是，我们就可以用平均场理论 (mean-feild) <span class="math display">\[r\left(\mathbf{Z}_{1}, \cdots, \mathbf{Z}_{n} \mid \mathbf{A}, \mathbf{X}\right) \approx \prod_{i=1}^{n} q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)\]</span> 其中<span class="math inline">\(\phi_{i}\)</span> 代表第i个变分边界的参数。而后我们用真实的条件概率分布与上式的KL散度来优化这些参数: <span class="math display">\[\min _{\phi_{1}, \cdots, \phi_{n}} \mathrm{KL}\left(\prod_{i=1}^{n} q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right) \| r\left(\mathbf{Z}_{1}, \cdots, \mathbf{Z}_{n} \mid \mathbf{A}, \mathbf{X}\right)\right)\]</span></p><p>而最优的变分边界满足以下形式（证明见论文） <span class="math display">\[q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)=\mathcal{O}_{\mathcal{G}}^{M F}\left(\mathbf{Z}_{i},\left\{q_{\phi_{j}}\right\}_{j \in \mathcal{N}(i)}\right)\]</span> 其中<span class="math inline">\(\mathcal{N}(i)\)</span>为<span class="math inline">\(\mathbf{Z}_{i}\)</span>的邻节点。<span class="math inline">\(\mathcal{O}\)</span>是一个由不动点方程确定的函数，它依赖于图自身的性质。这一形式意味着最优的<span class="math inline">\(q_{\phi_{i}}\)</span>的参数只与i的邻节点的q_{}有关。这就意味着平均场近似推断的迭代算法是在图上执行信息传递，直到收敛： <span class="math display">\[q_{\phi_{i}}^{(l)}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)=\mathcal{O}_{\mathcal{G}}^{M F}\left(\mathbf{Z}_{i},\left\{q_{\phi_{j}}^{(l-1)}\right\}_{j \in \mathcal{N}(i)}\right) .\]</span></p><p>令<span class="math inline">\(\boldsymbol{\mu}_{i}=\mathbb{E}_{\mathbf{Z}_{i} \sim q_{\phi_{i}}}\left[\psi\left(\mathbf{Z}_{i}\right)\right]\)</span>，根据上文提到的结论，我们就可以绕开 具体的<span class="math inline">\(\mathcal{O}\)</span>，将其变为 <span class="math display">\[\boldsymbol{\mu}_{i}^{(l)}=\tilde{O}_{\psi, \mathcal{G}}^{M F}\left(\left\{\boldsymbol{\mu}_{j}^{(l-1)}\right\}_{j \in \mathcal{N}(i)}\right)\]</span> 将上式在0处做一阶泰勒展开，有 <span class="math display">\[\mu_{i}^{(l)} \approx \tilde{O}_{\psi, \mathcal{G}}(\mathbf{0})+\mathbf{N}_{i}^{(l-1)} \cdot \nabla \tilde{O}_{\psi, \mathcal{G}}(\mathbf{0})\]</span> 这式从形式上就与message passing机制非常类似。 <span class="math display">\[H_{i}^{(l)}=\eta_{l}\left(B_{l, i}+\sum_{f \in \mathcal{F}_{l}} f\left(\mathbf{A}_{i}\right) \mathbf{H}^{(l-1)} W_{l}\right)\]</span> 由于在看推导过程时把一些分量的维度等忽略了，因此最后一步的具体细节直接在这里贴原文。</p><p><img src="/images/pd10/11.png" title="具体证明过程" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ICML2019的论文，作者为Stanford的phd。&lt;/p&gt;
&lt;h1 id=&quot;先说几句&quot;&gt;先说几句&lt;/h1&gt;
&lt;p&gt;看到摘要&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Our model parameterizes variational autoencoders (VA</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graph Auto-Encoders</title>
    <link href="http://example.com/2022/01/18/pd10/"/>
    <id>http://example.com/2022/01/18/pd10/</id>
    <published>2022-01-18T13:39:46.000Z</published>
    <updated>2022-01-19T12:47:34.022Z</updated>
    
    <content type="html"><![CDATA[<p>本文来自Thomas Kipf的博士论文，其论文其他内容包括GCN、relational GCN、compositional imitation learning and execution等。</p><p>对于图<span class="math inline">\(G=(V,E)\)</span>，有<span class="math inline">\(N=|V|\)</span>，邻接矩阵<span class="math inline">\(A\)</span>为<span class="math inline">\(N \times N\)</span>，用<span class="math inline">\(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\)</span>来度量两个节点ij的相似度，其中<span class="math inline">\({z}_{i}, {z}_{j}\)</span>为节点的embedding</p><h1 id="gae">GAE</h1><p><img src="/images/pd10/1.png" title="encoder-decoder architecture" /></p><p>Graph Auto-Encoder同样采用encoder-decoder架构，其中scoring function <span class="math inline">\(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\)</span>作为decoder，其根据节点嵌入来重构邻接矩阵；Encoder的输入为图的邻接矩阵<span class="math inline">\(A\)</span>以及节点的特征向量<span class="math inline">\(\left\{\mathbf{x}_{i}\right\}_{i \in \mathcal{V}}\)</span>，输出为node representations <span class="math inline">\(\mathbf{Z}_{i}\)</span>。</p><ul><li><p>Encoder<br />GAE的Encoder借助GNN来处理节点的初始化向量和图的结构信息，从而得到节点的表示。比如使用GCN作为Encoder时，有： <span class="math display">\[\mathbf{Z}=\operatorname{GCN}(\mathbf{X}, \mathbf{A})=\widehat{\mathbf{A}} \operatorname{ReLU}\left(\widehat{\mathbf{A}} \mathbf{X} \mathbf{W}_{0}\right) \mathbf{W}_{1}\]</span> 除了用GNN作为encoder之外，还有其他的embedding方法，比如最简单的shallow embedding直接根据节点的编号，以及DeepWalk、node2vec等方法。</p></li><li><p>Decoder<br />Decoder用来根据<span class="math inline">\(Z\)</span>重建邻接矩阵 <span class="math display">\[\mathbf{A}^{\prime}=l\left(\mathbf{Z Z}^{\top}\right)\]</span> 其中<span class="math inline">\(l(.)\)</span>是logistic sigmoid function，也就是说<span class="math inline">\(A_{i, j}^{\prime}=l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)\)</span></p></li></ul><p>不过在图表示学习一书中给出了多种decoder <img src="/images/pd10/4.png" title="decoder" /></p><ul><li>Training<br />使用交叉熵进行训练 <span class="math display">\[\mathcal{L}=-\frac{1}{N^{2}} \sum_{i=1}^{N} \sum_{j=1}^{N} A_{i, j} \log l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)+\left(1-A_{i, j}\right) \log \left(1-l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)\right)\]</span></li></ul><h1 id="vae">VAE</h1><p><img src="/images/pd10/2.png" title="VAE" /></p><p>在介绍变分图自编码器 (VGAE)之前，我们先简单介绍一下变分自编码器Variational Auto-encoders。[Auto-Encoding Variational Bayes]</p><p><img src="/images/pd10/5.png" title="思路" /> 上图中的实线就代表了生成模型<span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\)</span>，也就是根据隐变量z生成目标数据。而这一生成模型中，我们需要用<span class="math inline">\(q_{\phi}(\mathbf{z} \mid \mathbf{x})\)</span>来拟合无法得到的后验分布<span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\)</span>。而这里就涉及到变分推断VI的内容。</p><p>接下来我们具体展开来说。</p><p>给定一个真实样本 <span class="math inline">\(x_{k}\)</span>, 我们假设存在一个后验分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span> 并假设这个分布是正态分布。后续我们要训练一个生成器<span class="math inline">\(x=g(z)\)</span>, 使其能从分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span> 采样出来的一个 <span class="math inline">\(\hat x_{k}\)</span> 还原为<span class="math inline">\(x_{k}\)</span>。<br />对于正态分布的两个参数：均值 <span class="math inline">\(\mu\)</span> 和方差 <span class="math inline">\(\sigma^{2}\)</span>，我们用神经网络进行拟合<span class="math inline">\(\mu_{k}=f_{1}\left(x_{k}\right), \log \sigma_{k}^{2}=f_{2}\left(x_{k}\right)\)</span>。再借助重参数技巧从<span class="math inline">\(z_k\)</span>的分布中采样得到<span class="math inline">\(z_k\)</span>。（对于重参数技巧，就是使采样这一步骤变为可导的一种方法）。</p><p>由于VAE最开始假设了隐变量服从正态分布，这就需要神经网络拟合的分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span>向标准正态分布看起，因为 <span class="math display">\[p(z)=\sum_{x} p(z \mid x) p(x)=\sum_{x} \mathcal{N}(0, I) p(x)=\mathcal{N}(0, I) \sum_{x} p(x)=\mathcal{N}(0, I)\]</span> 使分布与标准正态看齐这一过程借助在loss增加一个额外的loss（生成分布与标准正态分布的KL散度）来实现 <span class="math display">\[\mathcal{L}_{\mu, \sigma^{2}}=\frac{1}{2} \sum_{i=1}^{d}\left(\mu_{(i)}^{2}+\sigma_{(i)}^{2}-\log \sigma_{(i)}^{2}-1\right)\]</span></p><p>上述的过程介绍从Auto-Encoder的角度来介绍VAE，事实上如果阅读原论文会发现这种介绍VAE的思路是很令人费解的。回到最基础的贝叶斯学习，我们需要<span class="math inline">\(q_{\phi}\left(\mathrm{z} \mid \mathrm{x}^{(i)}\right)\)</span> 去逼近真实的后验概率 <span class="math inline">\(p_{\theta}\left(\mathrm{z} \mid \mathrm{x}^{(i)}\right)\)</span>，很自然的我们选择用KL散度作为loss，而后经过变分推断的推到转换为优化变分下界 <span class="math display">\[\tilde{\mathcal{L}}\left(\theta, \phi ; \mathrm{x}^{(i)}\right)=\frac{1}{L} \sum_{l=1}^{L}\left[\log p_{\theta}\left(\mathrm{x}^{(i)}, \mathrm{z}^{(i, l)}\right)-\log q_{\phi}\left(\mathrm{z}^{(i, l)} \mid \mathrm{x}^{(i)}\right)\right]\]</span> 其中, <span class="math inline">\(\mathrm{z}^{(i, l)}=g_{\phi}\left(\epsilon^{(i, l)}, \mathrm{x}^{(i)}\right), \quad \epsilon^{(i, l)} \sim p(\epsilon)\)</span> 。</p><p>而VAE正是给定上述结果中<span class="math inline">\(\epsilon, p_{\theta}(\mathrm{x} \mid \mathrm{z}), q_{\phi}(\mathrm{z} \mid \mathrm{x}), p_{\theta}(\mathrm{z})\)</span> 分布具体形式（正态分布）后的特例。</p><h1 id="vgae">VGAE</h1><p><img src="/images/pd10/3.png" title="VGAE" /></p><p>对于变分图自编码器，简单来看就是输入变为节点特征和邻接矩阵，输出为生成的邻接矩阵。</p><p><span class="math inline">\(p_{\theta}(\mathbf{A} \mid \mathbf{X})\)</span>为节点特征<span class="math inline">\(X\)</span>与邻接矩阵A的条件概率分布 <span class="math display">\[p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{X})=\int p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X}) p(\mathbf{Z} \mid \mathbf{X}) d \mathbf{Z}\]</span> 其中隐变量先验分布独立于特征向量X，只和节点自身有关<span class="math inline">\(p(\mathbf{Z} \mid \mathbf{X})=\prod_{i=1}^{N} p\left(\mathbf{z}_{i}\right)\)</span>。更具体的说，我们令<span class="math inline">\(p\left(\mathbf{z}_{i}\right)=\mathcal{N}\left(\mathbf{z}_{i} ; \mathbf{0}, \mathbf{I}\right)\)</span>。我们的目标是得到最优的参数<span class="math inline">\(\theta\)</span>。</p><p>根据变分推断的框架，我们引入inference model： <span class="math display">\[q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A})=\prod_{i=1}^{N} q_{\phi}\left(\mathbf{z}_{i} \mid \mathbf{X}, \mathbf{A}\right)\]</span> 其中 <span class="math inline">\(q_{\phi}\left(\mathbf{z}_{i} \mid \mathbf{X}, \mathbf{A}\right)=\mathcal{N}\left(\mathbf{z}_{i} ; \boldsymbol{\mu}_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right)\)</span></p><p>具体模型中，我们用两个GCN作为学习inference model的参数： <span class="math display">\[\mu_{i}=\left[\mathrm{GCN}^{(1)}(\mathbf{X}, \mathbf{A})\right]_{i}\]</span> <span class="math display">\[\log \sigma_{i}=\left[\mathrm{GCN}^{(2)}(\mathbf{X}, \mathbf{A})\right]_{i}\]</span></p><p>接下来的generative model，我们假设它与初始输入的节点特征无关，只与隐变量有关， <span class="math display">\[p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X})=\prod_{i=1}^{N} \prod_{j=1}^{N} p_{\boldsymbol{\theta}}\left(A_{i, j} \mid \mathbf{z}_{i}, \mathbf{z}_{j}\right)\]</span></p><p>模型所要优化的KL散度与变分推断的过程相似，可以变为优化 <span class="math display">\[\operatorname{ELBO}=\mathbb{E}_{q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A})}\left[\log p_{\theta}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X})\right]-\operatorname{KL}\left[q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A}) \| p(\mathbf{Z})\right].\]</span></p><p>我们可以将上述过程写的通俗一点，其中编码过程为： <span class="math display">\[q\left(z_{i} \mid X, A\right)=N\left(z_{i} \mid \mu_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right).\]</span> 解码（以内积为例）的过程为： <span class="math display">\[p\left(A_{i j}=1 \mid z_{i}, z_{j}\right)=\sigma\left(z_{i}^{T} z_{j}\right)_{\circ}\]</span> 损失函数为： <span class="math display">\[L=E_{q(Z \mid X, A)}[\log p(A \mid Z)]-K L[q(Z \mid X, A) \| p(Z)]\]</span></p><p>作者在边预测任务上测试了VGAE的表现。不过，在Decoder时不考虑节点的特征<span class="math inline">\(X\)</span>仅仅是为了将模型简化，作者发现这不影响link prediction的准确率。 <img src="/images/pd10/6.png" /></p><p>值得推敲的是，在论文的前面有关于图卷积GCN在这几个数据集上的表现。而作者却没有用统一的评价指标（精度和准确率）来对比这两个模型的表现。</p><p><img src="/images/pd10/7.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文来自Thomas Kipf的博士论文，其论文其他内容包括GCN、relational GCN、compositional imitation learning and execution等。&lt;/p&gt;
&lt;p&gt;对于图&lt;span class=&quot;math inline&quot;&gt;\(G</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>QUINE&#39;S EMPIRICAL ASSUMPTIONS [1968]</title>
    <link href="http://example.com/2021/12/21/pd9/"/>
    <id>http://example.com/2021/12/21/pd9/</id>
    <published>2021-12-21T12:09:17.000Z</published>
    <updated>2021-12-23T02:02:25.713Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><h2 id="絮絮叨叨">絮絮叨叨</h2><p>为什么会突然看一篇60年代的文章？要从几天前我从学校图书馆借了一本叫《数学之美》的书说起。首先安利一下这本书，这书薄薄的似乎只有100页，但是书的作者的文学素养和专业知识让AI问题及其背后的数学充满了浪漫主义色彩。然后在书里刚开始关于自然语言处理的相关介绍中，我看到了一个名字-Noam Chomsky （乔姆斯基）。当时我并没有听说过他，于是就去搜索了一下，搜索结果几个关键词抓住了我的眼球：xxxx创始人、最伟大的学者之一。so, who is he?</p><h2 id="从规则到统计">从“规则”到“统计”</h2><p>自然语言处理作为人工智能下的子领域，自然也陪伴着人工智能走过几个“春季”与“冬季”。具体而言，NLP从早起的基于规则到如今的基于统计，背后不仅是深度学习技术的发展，也是其从“理想主义”为主流到“经验主义”占主导的转变过程。</p><blockquote><p>基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处理是哲学中的理性主义。在哲学领域中经验主义与理性主义的斗争一直是此消彼长，这种矛盾与斗争也反映在具体科学上，如自然语言处理。 早期的自然语言处理具有鲜明的经验主义色彩。如1913年马尔科夫提出马尔科夫随机过程与马尔科夫模型的基础就是“手工查频”，具体说就是统计了《欧根·奥涅金》长诗中元音与辅音出现频度；1948年香农把离散马尔科夫的概率模型应用于语言的自动机，同时采用手工方法统计英语字母的频率。<br />然而这种经验主义到了乔姆斯基时出现了转变。<br />1956年乔姆斯基借鉴香农的工作，把有限状态机作用刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型。这些工作非常伟大，为自然语言和形式语言找到了一种统一的数学描述理论，一个叫做“形式语言理论”的新领域诞生了。但乔老爷子干完这一票之后，挥一挥衣袖，说了一句“有限状态模型不适合用来描述自然语言”。 随后老爷子又补了一刀“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”。 -------《统计自然语言处理》</p></blockquote><p>从如今language model在NLP中的表现来看这句“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”是非常的离谱，而网上的几篇博客（内容几乎一模一样😅）说这句如此绝对的话来源于<em>QUINE'S EMPIRICAL ASSUMPTIONS</em>这篇文章。因此本着对伟人的尊重和求真务实的态度，我就决定去读一下这篇文章。另外这篇文章是在springer出版的书籍。不过在说这篇文章的内容之前还需要补充介绍几个名词</p><h2 id="willard-van-orman-quine-avram-noam-chomsky">Willard Van Orman Quine &amp; Avram Noam Chomsky</h2><p>这里我们首先简单的介绍一下两个人。<br />WVO Quine就是文章标题中的这个Quine，文章中从他的作品<em>word and object</em>开始谈起。直接检索Willard Van Orman Quine会发现百度百科并没有收录这个词条，不过wikipedia倒是有。简单来说，Quine是美国一位著名的哲学家，主张经验主义，另外他倡导Semantic holism-语义整体论。语义整体论可以简单理解为语言的某个部分，无论是术语还是完整的句子，只能通过它与更大语言部分的关系来理解。</p><p>Avram Noam Chomsky也就是乔姆斯基（任在世），是美国哲学家，麻省理工学院语言学的荣誉退休教授。乔姆斯基的《句法结构》被认为是20世纪理论语言学研究上最伟大的贡献。<br />《句法结构》（Syntactic Structures）是乔姆斯基介绍转换生成语法的《语言学理论的逻辑结构》一书的精华版。这一理论认为说话的方式（词序）遵循一定的句法，这种句法是以形式的语法为特征的，具体而言就是一种不受语境影响并带有转换生成规则的语法。儿童被假定为天生具有适用于所有人类语言的基本语法结构的知识。这种与生俱来的知识通常被称作普遍语法理论。</p><h2 id="humean-theory">Humean Theory</h2><p>休谟（David Hume）是苏格兰不可知论哲学家。他认为人的认知是有局限的。在休谟看来，我们所能认知的“自我”，其实只是感知，人的感知受人的感官局限。休谟认为，因果关系是人的理念，我们倾向于把某种序列中理念间的必然联系归于这种因果关系的本质。也就是说，因果只是我们头脑中的理念而已，两个客体造成恒定感知，比如每天我们看到太阳升起——天就亮了，我们就会把二者视为因果关系。但并不是自然界真的存在因果关系。</p><h1 id="正文">正文</h1><p>这篇文章是对奎恩经验主义假设的解读，不过这篇文章阅读下来非常晦涩，因为作者在叙述Quine的理论时会通过各种逗号断句或小括号来表达自己的观点，所以文章的内容没有结构化的组织（就这还语言学家呢，就这就这），另外这一篇十多页的文章居然一个小标题都没有。</p><p>首先quine的理论来源于Humean theory of language acquisition，他认为人们对于语言的知识可以被表示为a network of linguistic，这也意味着人类的theory，比如chemistry这种二级学科或者基础的学科都可以被表示为a fabric of sentences variously associated to one another。进而人类所有的知识都可以用这些结构来描述。quine的理论中提到了“language”和“theory”。乔姆斯基指出理论与语言是相互渗透的，另外理论还涵盖了common-sense和belief。</p><blockquote><p>Beneath the uniformity that unites us in communication there is a chaotic <strong>personal diversity of connections</strong>, and, for each of us, <strong>the connections continue to evolve</strong>. No two of us learn our language alike, nor, in a sense, does any finish learning it while he lives.</p></blockquote><p>奎因表示如果语言是通过<strong>条件反应的机制相互关联</strong>并<strong>与外部刺激相关联</strong>的句子网络，那么一个人对言语行为的倾向可以根据这种网络来表征。按照这种语言的抽象形式，我们如何从语言中获取知识？奎恩提出了一个prelinguistic quality space，其中定义了距离度量（意味着可以度量相似度）。简单来说，在这个空间的某个维度上来看red ball， yellow ball之间的距离比red kerchief要近。这一想法似乎是背离经验主义的，因为这种质量空间可以想象和定义得到的，而非学习得到的。</p><p>然而，奎因在他关于语言是如何学习的假说中回到了经典的经验主义概念。与他认为语言是一个句子网络的观点相一致，他列举了学习句子的三种可能机制。首先，句子可以通过“直接条件反射”到“适当的非语言刺激”来学习，也就是说，通过在适当的条件下重复配对句子和刺激；第二，通过句子与句子的关联；第三，新句子可以通过“类比合成”产生，不过这种类比指的并不是类似英语语法规则的东西，而是在固定的上下文中用一个词代替一个类似的词（“手”、“脚”）。他认为一种语言是相关句子的有限网络，有些也与刺激相关，因为这只是两个假定的机制所产生的结构，具有实质性内容的语言学习。</p><p>但是乔姆斯基认为语言是句子的无限集合构成的。由假定的机制推导出的网络必定是有限的（对应上文的学习句子的机制），它只会包含人们曾经接触过的句子。</p><blockquote><p>Presumably, a complex of dispositions is a structure that can be represented as a set of probabilities for utterances in certain definable 'circumstances' or 'situations'. But it must be recognized that the notion 'probability of a sentence' is an entirely useless one, under any known interpreta- tion of this term.</p></blockquote><p>这里乔姆斯基给出了这句话——句子的概率是没有意义的。他举例说“birds fly”或者“Tuesday follows Monday”这两个英语下句子的概率对日语中产生这两个句子的概率没有意义。他认为probability relative to a situation没有任何意义。如果complex of dispositions是由根据经验观察确定的，那么只有少数传统的问候语、陈词滥调等才有可能与语言的倾向相关联，因为在技术意义上，在任何合理的语料库或数据集中，很少有其他句子可能具有非空的相对频率。且随着语料库的增加，任何给定句子的频率都会无限制地减少。 有人可能会设想用其他方法根据经验为句子分配概率，但乔姆斯基认为，没有一种方法可以避免这些困难。因此，如果一种语言被理解为在正常情况下作出反应的复杂倾向（奎恩的经验主义假说），那么它不仅是有限的、而且非常“小”。</p><p>Quine在提出“言语倾向”时指出了翻译的不确定性问题，简单来说可以理解为每个人的说话习惯几乎没有相似之处，因此根本无法建立与这种倾向相一致的翻译手册。对于理论和语言的有限性假设带来的问题，乔姆斯基提出语言是人类头脑的先天属性所带来的，存在一种“普遍语法”。</p><p>到这里，我们简单的概括一下前文提到的大概内容，即Quine的理论和乔姆斯基的看法:</p><blockquote><p>We are left with the fact that Quine develops his explicit notion of 'language' and 'theory' within a narrowly conceived Humean framework (except for the possible intrusion of a rich system of innate ideas), and that he characterizes language learning (&quot;learning of sentences&quot;) in a way consistent with this narrow interpretation, although the conclusion that <strong>a language (or theory) is a finite fabric of sentences, constructed pairwise by training, or a set of sentences with empirically detectable probabilities of being produced</strong> (hence a nearly empty set) is incompatible with various truisms to which Quine would certainly agree.</p></blockquote><p>Quine依靠他关于知识获取和语言学习的经验主义假设来支持他的一些主要哲学结论。一个重要的例子可以说明这一点。知识的基础是从某些证据上做“分析假设”。对Quine来说，一个关键点是，在基本语言和“常识知识”的情况下，分析假设的正确性并不是“客观问题”，它可以是“对或错”。这些分析假设是超越了 “任何一个本地人的言语行为倾向所隐含的任何东西”。因此，当我们在翻译、学习一门语言时，我们自然而然地会使用这些分析性假设（知识）与母语进行类比。也就是说在Quine的经验主义观点建模下超越言语倾向的知识（分析假设）是一个主观的概念，而这就会带来“翻译的不确定性”。</p><p>另外，Quine对基于数据的分析假设的构建和基于数据的“观察句子的刺激意义”的假设进行了鲜明的区分。他指出，后者只涉及“正常感应”类型的不确定性。显然，包含真值功能连接词的句子的翻译（类似地，学习和理解）中涉及的归纳推理也是如此。在这些情况下，归纳法将我们引向“真正的假设”，这与“分析假设”截然不同（在讨论翻译的不确定性时提到的“分析假设”）。因此，Quine认为“<strong>正常归纳</strong>”与“<strong>假设形成或理论建构</strong>”之间存在区别，前者不涉及严重的认识论问题，后者确实涉及此类问题。毫无疑问，这种区别是可以区分的；然而，Quine没有具体说明“正常归纳”所基于的先验属性。这里，乔姆斯基认为大脑天生具有允许从“正常归纳”到“真实假设”的属性，但不允许“理论建构”和一些可能受到狭隘限制的“分析假设”。也就是说，他认为在经验主义下根据数据进行归纳而后得到一个假设的真值（对或错）这个过程是合理，但是直接归纳知识这一过程是不合理的。<br />因此，一般来说关于语言不可能有一套固定的“分析假设”。我们需要为每种语言（更准确地说，为每种语言的每一个说话者）建立一套新的分析，因为语言的形式没有任何普遍性。</p><p>这里还是强调了乔姆斯基对于统计自然语言处理的观点，他认为每种语言，每个说话者的说话倾向会导致无法建立一套普遍的“分析假设”。因此乔姆斯基认为，当我们学习一门语言时，我们并不是在“学习句子”或通过训练获得“行为技能”。相反，我们以某种方式发展了某些原则（当然是无意识的），这些原则决定了许多句子的形式和意义。</p><h1 id="转换生成语法">转换生成语法</h1><p>在乔姆斯基的《句法结构》一书中，他提出了转换生成语法理论，他认为语言是人类特有的一种先天机制，不仅应该研究语言行为，而且应该研究语言能力，转换-生成语法就是关于语言能力的理论。具体而言，乔姆斯基认为语法主要包括基础和转换两个部分，基础部分生成深层结构，深层结构通过转换得到表层结构，语义部分属于深层结构，它为深层结构作出语义解释。语音部分属于表层结构并为表层结构作出语音解释。强调从认知学的角度对人类语言共性的解释，区分先天的语言能力和后天的语言知识，认为语言有生成能力，是有限规则的无限使用，转换则是生成的重要手段。</p><p>他的思想对当时主流的结构主义语言学产生了重要的影响。他的理论包含了几个关键的思想，首先是语义学是独立于语法学之外的，合乎语法的并不一定有意义。另外，他认为语言能力就像行走一样，是人与生俱来的理解语言及遣词造句的能力。</p><p>转换生成语法自创立以来, 就以对语言现象的解释充分性为目标, 试图建立一套能像数理公式般进行形式运算推理的规则来解释自然语言。期间虽经反复的修改否定再修改, 每一次都会有新的理论突破, 但其研究的对象、方法和原则却始终如一, 从而极大的推动了当代语言学的发展, 并为语言研究开辟了一条新的道路, 展现了一个全新的发展方向。<br />比如说，基于规则的句法剖析主要是使用Chomsky的上下文无关语法。在上下文无关语法的基础上, 学者们提出了自顶向下分析法、自底向上分析法、左角分析法、CYK算法、Earley 算法、线图分析法等行之有效的剖析技术。</p><p>关于基于规则的自然语言处理在工业界中的应用，可以参考这个链接 https://www.zhihu.com/question/30748126</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;h2 id=&quot;絮絮叨叨&quot;&gt;絮絮叨叨&lt;/h2&gt;
&lt;p&gt;为什么会突然看一篇60年代的文章？要从几天前我从学校图书馆借了一本叫《数学之美》的书说起。首先安利一下这本书，这书薄薄的似乎只有100页，但是书的作者的文学素养和专业知识让AI问题及其背</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Gel Candy Evaluation 凝胶糖果测评</title>
    <link href="http://example.com/2021/12/20/candies/"/>
    <id>http://example.com/2021/12/20/candies/</id>
    <published>2021-12-20T06:25:28.000Z</published>
    <updated>2022-01-05T03:20:44.092Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>Todo</p><h1 id="introduction">Introduction</h1><p>改革开放以来，人民生活水平日益提升，人们的物质文化生活也越来越丰富多彩。糖果作为小孩子最爱的零食，种类也越来越丰富。在跟王同学一起在研讨间学习的这段时间，在去往图书馆的路上，我偶尔会从天猫超市[1]或者苏果超市[2]买一包软糖给王同学，希望糖果的甜能为她备战考研的枯燥时光中带来些许快乐。由于笔者在这段时间中有点咳嗽，医生叮嘱不可以吃甜食，因此在测评过程中每包糖果我也只吃了几颗（有时一转眼整包就被某人吃完了），但是保证了每一类糖果中包含的每种味道都吃过。下面将对两个超市中售卖的几种凝胶糖果进行测评和打分。对于糖果的评价指标，由于笔者才疏学浅，只能使用个人主观评价以及王同学对部分糖果的一些评价进行评估。</p><h1 id="method">Method</h1><h2 id="gel-candy">Gel Candy</h2><h1 id="experiment">Experiment</h1><h2 id="evaluation">Evaluation</h2><p>笔者对于这些糖果的味道和口感按照1～5🌟进行打分，另外附带一些文字评价。另外王同学的评价一般包括：就这，还行，味太大，一般。</p><h2 id="alpenliebe">Alpenliebe</h2><ul><li>乐嚼Q动物果果</li><li>乐嚼Q虫虫派对</li><li>乐嚼Q乳酸果果</li><li>乐嚼Q熊猫小队</li></ul><h2 id="skittles">Skittles</h2><h2 id="uha">UHA</h2><h1 id="reference">Reference</h1><p>【1】东南大学九龙湖校区梅园天猫超市<br />【2】东南大学九龙湖校区桃园苏果超市</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Todo&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;改革开放以来，人民生活水平日益提升，人们的物质文化生活也越来越丰富多彩。糖果作为小孩子最爱的零食，种类也越</summary>
      
    
    
    
    <category term="杂七杂八" scheme="http://example.com/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks [ACL2020]</title>
    <link href="http://example.com/2021/12/08/pd8/"/>
    <id>http://example.com/2021/12/08/pd8/</id>
    <published>2021-12-08T05:49:22.000Z</published>
    <updated>2021-12-14T13:04:34.134Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍的内容包含多篇基于图神经网络进行文本分类的论文，从提出GCN后的简单应用到去年ACL上的TextING以及今年的BertGCN，GNN在文本分类上取得了非常好的效果。</p><p><a href="https://arxiv.org/abs/1809.05679">Graph Convolutional Networks for Text Classification</a><br /><a href="https://arxiv.org/abs/1910.02356v2">Text Level Graph Neural Network for Text Classification</a><br /><a href="https://arxiv.org/abs/2004.13826v1">Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks</a><br /><a href="https://arxiv.org/abs/2105.05727">BertGCN: Transductive Text Classification by Combining GCN and BERT</a></p><h1 id="text-classification">Text Classification</h1><p>文本分类的应用非常广泛，包括Sentiment Analysis、News Categorization、Topic Analysis甚至是Question Answering等。而automatic text classification方法大致可以被归为两类：基于规则的^rule-based以及基于机器学习的^数据驱动的。</p><p>基于规则的文本分类需要先验知识，包括pre-defined rules、domain knowledge等。而基于机器学习的方法则是借助标注数据集。通常我们可以把机器学习文本分类的步骤归为两步，首先是文本特征提取，而后将特征输入分类器进行分类。而发展到现在，基于神经网络的文本分类模型大致也随着深度学习的发展从前馈神经网络到CNN、RNN以及attention、transformer到如今pre-trained model如BERT等。</p><p>前馈神经网络进行文本分类通常将文本作为bag of words；RNN则把文本作为词的序列；CNN用于训练提取文本中的关键短语、词组等进行匹配分类；attention机制能识别文本中相互关联的词，可以嵌入到其他深度学习模型中；至于transformer以及BERT这类大规模预训练模型，则是“大力出奇迹”。</p><p>而本文的重点，则是基于GNN-图神经网络的文本分类方法。借助图神经网络进行文本中句法、语义解析树之类的图结构信息挖掘，进而进行文本分类。另外经过下文的介绍我们还能发现，基于GNN的模型能与其他深度神经网络进行级联并进行联合训练，进而有效提升分类准确率。<br />基于图神经网络的文本分类模型的差异大致体现在三个方面：图的构建、节点嵌入的初始化、图神经网络。</p><h1 id="textgcn">TextGCN</h1><p>Graph Convolutional Networks for Text Classification<br /><img src="/images/gcn/2.png" title="Framework of TextGCN" /> TextGCN为AAAI2018的论文，现在很多人看到这篇文章的时候可能会感叹“这也能发？”，但事实上这篇论文是最先构建了transductiive的基于GNN进行文本分类的框架，并取得了非常好的表现。</p><p>模型整体的框架如上图所示，包括图的构建和图神经网络两个模块。其中图神经网络由简单的两层卷积层构成。另一方面，图节点的特征初始化用one-hot vector，输入的信息仅为边信息和节点的结构信息，而其在分类结果上取得的准确率也反映了GNN应用文本分类的合理性。 <span class="math display">\[Z=\operatorname{softmax}\left(\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W_{0}\right) W_{1}\right)\]</span></p><p>对于构图方面，模型基于整个语料库构建一个异构图，图中的节点包括文档节点和词节点。而这两类节点之间的边，word-word、doc-word定义如下 <span class="math display">\[A_{i j}=\left\{\begin{array}{ll}\operatorname{PMI}(i, j) &amp; i, j \text { are words, } \operatorname{PMI}(i, j)&gt;0 \\\operatorname{TF}-\operatorname{IDF}_{i j} &amp; i \text { is document, } j \text { is word } \\1 &amp; i=j \\0 &amp; \text { otherwise }\end{array}\right.\]</span> 其中词i与j之间的PMI (point-wise mutual information) 计算为 <span class="math display">\[\begin{aligned}\operatorname{PMI}(i, j) &amp;=\log \frac{p(i, j)}{p(i) p(j)} \\p(i, j) &amp;=\frac{\# W(i, j)}{\# W} \\p(i) &amp;=\frac{\# W(i)}{\# W}\end{aligned}\]</span> <span class="math inline">\(\#W(i,j)\)</span>代表滑窗中两个词共现次数。另外，图中的词节点会将语料库统计后的低频词过滤掉。至于为什么选择PMI以及TF-IDF这两个指标作为边权重，作者提到是从实验的结果出发作出的选择。 <img src="/images/pd8/2.png" title="Results: TextGCN" /></p><h1 id="text-level-gnn">Text-level GNN</h1><p>Text Level Graph Neural Network for Text Classification [ENMLP2019]<br />TextGCN模型跟大部分直推式GNN模型一样，应用时存在明显的缺陷，即没有办法进行在线测试。当我们要输入的新文本进行分类时，需要将文本加入语料库后重新构建graph训练，这就会带来极大的开销。而这篇Text-level GNN则是构建基于文本级别的图，使得基于GNN的文本分类模型提供在线测试的功能，虽然模型依旧是Transductive。 <img src="/images/pd8/3.png" title="Text-level GNN" /> Text-level图的构建如上图所示，其中词与词之间连接的边权重以及词的embedding为整个语料库全局共享，保存在全局矩阵中（上图的两个矩阵），另外文档中的每个词不止和相邻的词存在边，而由一个超参数控制多跳邻居。</p><p>在图神经网络模块，虽然论文中介绍的是non-spectral message passing mechanism，但事实上与TextGCN本质上是一样的，不过边的权重会在训练过程中进行更新。 <span class="math display">\[\begin{aligned}\mathbf{M}_{\mathbf{n}} &amp;=\max _{a \in \mathcal{N}_{n}^{p}} e_{a n} \mathbf{r}_{\mathbf{a}} \\\mathbf{r}_{\mathbf{n}}^{\prime} &amp;=\left(1-\eta_{n}\right) \mathbf{M}_{\mathbf{n}}+\eta_{n} \mathbf{r}_{\mathbf{n}}\end{aligned}\]</span> 上式中<span class="math inline">\(r\)</span>代表节点特征，<span class="math inline">\(\eta_{n}\)</span>为可训练的参数。<br />最后，使用文本中的所有词的embedding进行类别的推断；而在TextGCN中则是直接使用文档节点的embedding进行分类。 <span class="math display">\[y_{i}=\operatorname{softmax}\left(\operatorname{Relu}\left(\mathbf{W} \sum_{n \in N_{i}} \mathbf{r}_{\mathbf{n}}^{\prime}+\mathbf{b}\right)\right)\]</span></p><p>在这里简单对Corpus-level GNN（TextGCN）和Text-level GNN进行简单的比较。首先两者在下游任务的准确率上有较小的差异，其中后者略胜一筹，不过后者使用了Glove词向量进行初始化，所以事实上将TextGCN使用一些小技巧后两者的准确率是非常接近的。不过Text-level GNN优越性体现在其能够提供在线测试上，当输入新文档进行分类时，它的计算开销会远小于TextGCN。而对于它使用的MPM神经网络而不是GCN，是因为MPM更适合它的构图模式，而不是MPM比常规的GCN更强的信息提取能力。Text-level使得全局的边权重必须成为可训练的参数，而MPM中的另一个可训练的参数<span class="math inline">\(\eta_{n}\)</span>实质上与GCN中结合<span class="math inline">\(I\)</span>的拉普拉斯矩阵<span class="math inline">\(L\)</span>是一致的。</p><h1 id="texting">TextING</h1><p>Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks<br />上文的两个模型以及后续的BertGCN都是transductive，而本文的TextING则是inducive模型。论文发表在ACL2020上，也就是博客标题。 <img src="/images/pd8/4.png" title="TextING" /> 模型针对每篇文档构建一个图，以词共现作为边节点，借助滑窗（size 3）构建图。节点嵌入用Glove进行初始化。 模型的图神经网络模块使用了Gated Graph Neural Networks(GGNN)和图注意力(GAT)。 <span class="math display">\[\begin{aligned}\mathbf{a}^{t} &amp;=\mathbf{A h}^{t-1} \mathbf{W}_{a} \\\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z} \mathbf{a}^{t}+\mathbf{U}_{z} \mathbf{h}^{t-1}+\mathbf{b}_{z}\right) \\\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r} \mathbf{a}^{t}+\mathbf{U}_{r} \mathbf{h}^{t-1}+\mathbf{b}_{r}\right) \\\tilde{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W}_{h} \mathbf{a}^{t}+\mathbf{U}_{h}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)+\mathbf{b}_{h}\right) \\\mathbf{h}^{t} &amp;=\tilde{\mathbf{h}}^{t} \odot \mathbf{z}^{t}+\mathbf{h}^{t-1} \odot\left(1-\mathbf{z}^{t}\right)\end{aligned}\]</span></p><p>上式中<span class="math inline">\(h\)</span>表示节点embedding，<span class="math inline">\(a\)</span>代表接受的信息，<span class="math inline">\(z\)</span>和<span class="math inline">\(r\)</span>分别代表更新和遗忘。而后将节点借助readout模块输出为graph-level的embedding： <span class="math display">\[\begin{array}{l}\mathbf{h}_{v}=\sigma\left(f_{1}\left(\mathbf{h}_{v}^{t}\right)\right) \odot \tanh \left(f_{2}\left(\mathbf{h}_{v}^{t}\right)\right) \\\mathbf{h}_{\mathcal{G}}=\frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \mathbf{h}_{v}+\text { Maxpooling }\left(\mathbf{h}_{1} \ldots \mathbf{h}_{\mathcal{V}}\right)\end{array}\]</span></p><p>上文提到的两个模型中GNN并没有attention模块，这是由于TextGCN的PMI、TF-IDF信息会损失，另一方面Text-level GNN全局的边权重也不应该引入文本图中的attention机制进行更新。</p><p>将上文的三个模型进行简单的对比，可以隐约感到存在一个图的构建与图神经网络之间的trade-off。前两个模型的图构建过程都嵌入了大量的信息（先验信息、全局信息），而他们的图神经网络都非常简单。事实上我曾在TextGCN上做过一些实验，尝试把GAT融入到信息传播过程中，发现准确率会有明显下降。而TextING的构图过程中的信息仅是词节点的共现所包含的上下文信息和结构信息，因此它可以接受更复杂的信息传播和聚合过程。这也使得TextING可以面对新词和新输入的文本直接进行分类。</p><h1 id="bertgcn">BertGCN</h1><p>BertGCN: Transductive Text Classification by Combining GCN and BERT<br />BertGCN是由香侬科技提出，发表在ACL2021上的文章，也是目前文本分类的SOTA模型。不过这篇文章的贡献主要是工程上的。另外，不妨排列组合一下将Bert与TextING结合，应该能取得更好的结果XD（虽然在R8上的结果已经非常接近100了）。<br />回顾TextGCN，模型中图节点初始化用的是one-hot向量，而BertGCN则是用Bert进行embedding的初始化，另外将Bert与GNN两个模块进行联合训练，取得了很好的表现。</p><p>两者的结合存在两个问题，一是难收敛：BERT与GCN处理数据的方式不同、模型大小不同；二是GCN是在整个图上运算，而BERT过大的模型无法一次全部加载图中所有结点，这就给BertGCN的训练带来阻碍。 针对第一个问题，模型使用了Interpolating损失。 <span class="math display">\[Z=\lambda Z_{\mathrm{GCN}}+(1-\lambda) Z_{\mathrm{BERT}}, Z_{\mathrm{BERT}}=\operatorname{softmax}(W X)\]</span> 当<span class="math inline">\(\lambda=1\)</span>时，BERT模块没有更新；当<span class="math inline">\(\lambda=0\)</span>时，GCN模块没有更新；当<span class="math inline">\(\lambda \in (0,1)\)</span>时，两个模块都能得到更新，并且通过调节<span class="math inline">\(\lambda\)</span>实现BertGCN整体模块的快速收敛。<br />对于无法整图训练这一问题，BertGNN提出了一个Memory Bank用于保存所有节点特征，每次从中加载batch进行训练并更新，其他保持不变。将整个语料库中的文档特征分批更新，为了防止异步更新带来的不一致性，模型在训练Bert模型时采用了小学习率。</p><p><img src="/images/pd8/1.png" title="Results: BertGCN" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文介绍的内容包含多篇基于图神经网络进行文本分类的论文，从提出GCN后的简单应用到去年ACL上的TextING以及今年的BertGCN，GNN在文本分类上取得了非常好的效果。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.05679&quot;</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="Text Classification" scheme="http://example.com/tags/Text-Classification/"/>
    
  </entry>
  
  <entry>
    <title>就叫“深入浅出图卷积(GCN)”吧</title>
    <link href="http://example.com/2021/11/28/gcn/"/>
    <id>http://example.com/2021/11/28/gcn/</id>
    <published>2021-11-28T10:35:25.000Z</published>
    <updated>2021-12-02T01:53:08.111Z</updated>
    
    <content type="html"><![CDATA[<p>图卷积网络作为图神经网络之一，具有非常广泛的应用。因此网上也有非常多的关于GCN的介绍，但是各种博客看的多了搞得我脑壳嗡嗡的，而刚好最近的一个工作涉及到GCN的内核，因此借助这篇博客整理对GCN进行整理。</p><p>在介绍GCN先介绍几篇文献：<br />Semi-Supervised Classification with Graph Convolutional Networks-首次提出GCN的论文（ICLR2017)<br />Data Analytics on Graphs-图机器学习的教材<br /><a href="https://www.zhihu.com/question/54504471" class="uri">https://www.zhihu.com/question/54504471</a>-介绍GCN的博客</p><h2 id="gnn-message-passing">GNN &amp; Message Passing</h2><p>GNN作为一种graph embedding的手段，可以借助节点特征的message passing提取图结构信息 <span class="math display">\[\begin{aligned} \mathbf{h}_{u}^{(k+1)} &amp;=\operatorname{UPDATE}^{(k)}\left(\mathbf{h}_{u}^{(k)}, \text { AGGREGATE }^{(k)}\left(\left\{\mathbf{h}_{v}^{(k)}, \forall v \in \mathcal{N}(u)\right\}\right)\right) \\ &amp;=\operatorname{UPDATE}^{(k)}\left(\mathbf{h}_{u}^{(k)}, \mathbf{m}_{\mathcal{N}(u)}^{(k)}\right) \end{aligned}\]</span></p><p><img src="/images/gcn/1.png" title="GNN Framework" /> GNN进行k轮迭代，每轮包括一个聚合（aggregate）和更新（update）操作。聚合来获取邻节点的信息，而后更新节点的自身特征。这种多轮message passaging的机制使得图的结构信息以及邻节点特征被提取到节点的特征中。广义上来说，GCN也是GNN中的一种。GCN的message passaging非常的简单。从下式（最基础形式的GCN）可以看出，GCN借助边信息对节点信息进行聚合。 <span class="math display">\[f\left(H^{(l)}, A\right)=\sigma\left(A H^{(l)} W^{(l)}\right)\]</span></p><h2 id="gnn-cnn">GNN &amp; CNN</h2><p>对于图像来说，nxn的卷积核可以作为图像中的特征提取器（为了防止图和图像这两个词的太过接近导致可能出现的问题，下文提到图时将用graph）。但是这种卷积操作无法直接用在Graph上，为什么？<br />由于图像与graph的数据特性和卷积操作的特性。首先，图像具有局部平移不变性(local translational invariance)，使得卷积核能够对图像矩阵进行扫描卷积；而graph作为非欧空间的数据 (Non Euclidean Structure)，每个节点邻接点的各异导致传统CNN操作无法应用。第二，卷积核是参数共享的，且可以实现层次化特征提取-卷积层可以在前一层的基础上提取更高阶的特征；而graph的层数加深则是使节点获取更广的感受野。</p><h2 id="spectrum">Spectrum</h2><p>参考<a href="https://zhuanlan.zhihu.com/p/120311352" class="uri">https://zhuanlan.zhihu.com/p/120311352</a><br />在空间域上的图卷积碰壁并不意味着在图上没法进行操作，我们可以从频域中进行分析。</p><h3 id="图的拉普拉斯矩阵">图的拉普拉斯矩阵</h3><p>首先定义几个概念：<br />在图上最基本的拉普拉斯矩阵Laplacian matrix为： <span class="math display">\[\mathbf{L}=\mathbf{D}-\mathbf{A}\]</span> 其中<span class="math inline">\(\mathbf{D}\)</span>为度矩阵，<span class="math inline">\(\mathbf{A}\)</span>为邻接矩阵。拉普拉斯矩阵有一些基本的性质：对称 (<span class="math inline">\(\mathbf{L}^{T}=\mathbf{L}\)</span>)；半正定 (<span class="math inline">\(\mathbf{x}^{\top} \mathbf{L} \mathbf{x} \geq 0, \forall \mathbf{x} \in \mathbb{R}^{|\mathcal{V}|}\)</span>)，这也意味着拉普拉斯矩阵的特征值都是非负的：<span class="math inline">\(0=\lambda_{|\mathcal{V}|} \leq \lambda_{|\mathcal{V}|-1} \leq \ldots \leq \lambda_{1}\)</span>。 <span class="math display">\[\begin{aligned}\mathbf{x}^{\top} \mathbf{L} \mathbf{x} &amp;=\frac{1}{2} \sum_{u \in \mathcal{V}} \sum_{v \in \mathcal{V}} \mathbf{A}[u, v](\mathbf{x}[u]-\mathbf{x}[v])^{2} \\&amp;=\sum_{(u, v) \in \mathcal{E}}(\mathbf{x}[u]-\mathbf{x}[v])^{2}\end{aligned}\]</span> 另外，对称规范化拉普拉斯矩阵symmetric normalized Laplacian定义如下，这是GCN相关工作中比较常用的。 <span class="math display">\[\mathbf{L}_{\mathrm{sym}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}}\]</span></p><h3 id="拉普拉斯算子">拉普拉斯算子</h3><p>接下来我们来一步步理解为什么要这样定义图的拉普拉斯矩阵。对于空间中的任意函数<span class="math inline">\(f\)</span>来说， <span class="math display">\[\Delta f=\nabla^{2} f=\nabla \cdot \nabla f =\sum_{i=1}^{n} \frac{\partial^{2} f}{\partial x_{i}^{2}}\]</span> 拉普拉斯算子 (Laplacian)是欧式空间中的函数梯度的散度 (Divergence)对应的微分算子。在n维空间中计算的是函数各个维度二阶偏导的和。在二维空间中，可以<strong>近似</strong>为差分的计算 <span class="math display">\[\begin{aligned}\Delta f(x, y) &amp;=\frac{\partial^{2} f}{\partial x^{2}}+\frac{\partial^{2} f}{\partial y^{2}} \\&amp;=[f(x+1, y)+f(x-1, y))-2 f(x, y)]+[f(x, y+1)+f(x, y-1))-2 f(x, y)] \\&amp;=f(x+1, y)+f(x-1, y))+f(x, y+1)+f(x, y-1))-4 f(x, y)\end{aligned}\]</span> 上式事实上就是在图像上作用拉普拉斯卷积核 <span class="math display">\[\begin{array}{|r|r|r|}\hline 0 &amp; 1 &amp; 0 \\\hline 1 &amp; -4 &amp; 1 \\\hline 0 &amp; 1 &amp; 0 \\\hline\end{array}\]</span> 因此拉普拉斯算子可以理解为——在所有自由度上进行微小变化后所获得的增益。<br />而将其推广到有N节点的graph上时，<span class="math inline">\(f\)</span>维度最高为N，<span class="math inline">\(f=\left(f_{1}, \ldots, f_{N}\right)\)</span>。其中<span class="math inline">\(f_{i}\)</span> 表示函数<span class="math inline">\(f\)</span>在网络图中节点i处的函数值, 类比<span class="math inline">\(f(x, y)\)</span>为函数<span class="math inline">\(f\)</span>在 <span class="math inline">\((\mathrm{x}, \mathrm{y})\)</span>的函数值。<br />因此当拉普拉斯算子作用在加权graph（<strong>边权重为</strong><span class="math inline">\(w_{i j}\)</span>）上时，借助差分近似后有： <span class="math display">\[\begin{aligned}\Delta \boldsymbol{f}_{i} &amp;=\sum_{j \in N_{i}} \frac{\partial f_{i}}{\partial j^{2}} \\&amp; \approx \sum_{j} w_{i j}\left(f_{i}-f_{j}\right) \\&amp;=\sum_{j} w_{i j}\left(f_{i}-f_{j}\right) \\&amp;=\left(\sum_{j} w_{i j}\right) f_{i}-\sum_{j} w_{i j} f_{j} \\&amp;=d_{i} f_{i}-w_{i:} f\end{aligned}\]</span> 对于任意<span class="math inline">\(i \in N\)</span>都成立，所以就得到了： <span class="math display">\[\begin{aligned}\Delta f=\left(\begin{array}{c}\Delta f_{1} \\\vdots \\\Delta f_{N}\end{array}\right) &amp;=\left(\begin{array}{cc}d_{1} f_{1}-w_{1:} f \\\vdots \\d_{N} f_{N}-w_{N:} f\end{array}\right) \\&amp;=\left(\begin{array}{ccc}d_{1} &amp; \cdots &amp; 0 \\\vdots &amp; \ddots &amp; \vdots \\0 &amp; \cdots &amp; d_{N}\end{array}\right) f-\left(\begin{array}{c}w_{1:} \\\vdots \\w_{N:}\end{array}\right) f \\&amp;=\operatorname{diag}\left(d_{i}\right) f-\mathbf{W} f \\&amp;=(\mathbf{D}-\mathbf{W}) f \\&amp;=\mathbf{L} f\end{aligned}\]</span> 这就意味着，对由图节点特征构成的向量<span class="math inline">\(f\)</span>做拉普拉斯等价于图拉普拉斯矩阵与向量<span class="math inline">\(f\)</span>进行点积。</p><h3 id="graph-fourier-transformer">Graph Fourier Transformer</h3><p>拉普拉斯矩阵的特征分解 <span class="math display">\[\mathbf{L} \mathbf{u}_{\mathbf{k}}=\lambda_{k} \mathbf{u}_{\mathbf{k}}\]</span> 继而进行正交相似对角化后就得到 <span class="math display">\[\mathbf{L}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{-1}=\mathbf{U}\left(\begin{array}{ccc}\lambda_{1} &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \\&amp; &amp; \lambda_{n}\end{array}\right) \mathbf{U^{-1}}=\mathbf{U}\boldsymbol{\Lambda} \mathbf{U}^{T}\]</span> 其中<span class="math inline">\(\boldsymbol{\Lambda}\)</span> 为特征值构成对角矩阵, <span class="math inline">\(\mathbf{U}\)</span> 为特征向量构成的正交矩阵。</p><p>在这里补充一条性质 <span class="math display">\[\Delta e^{-i \omega t} =\frac{\partial^{2} e^{-i \omega t}}{\partial t^{2}}= -\omega^{2} e^{-i \omega t}\]</span> 从广义上来看，这符合特征方程<span class="math inline">\(AV=\lambda V\)</span>的定义，也就是说<span class="math inline">\(e^{-i \omega t}\)</span>是拉普拉斯算子的特征函数</p><p>把传统的傅里叶变换以及卷积迁移到Graph上来, 核心工作其实就是把<strong>拉普拉斯算子的特征函数<span class="math inline">\(e^{-i \omega t}\)</span></strong> 变为Graph对应的<strong>拉普拉斯矩阵的特征向量</strong>。 傅立叶变化是信号函数<span class="math inline">\(f(t)\)</span>与基函数<span class="math inline">\(e^{-i \omega t}\)</span>的内积 <span class="math display">\[\mathcal{F}_{T}(\omega)=\int_{-\infty}^{+\infty} f(t) e^{-i \omega t} d t\]</span> 因此对于Graph我们就可以定义 <span class="math display">\[F\left(\lambda_{l}\right)=\hat{f}\left(\lambda_{l}\right)=\sum_{i=1}^{N} f(i) u_{l}^{*}(i)\]</span> 其中<span class="math inline">\(f\)</span>是graph上的N维向量，<span class="math inline">\(f(i)\)</span>对应于graph上的第i个顶点，<span class="math inline">\(u_{l}(i)\)</span>表示第l个特征向量的第i个分量。特征值<span class="math inline">\(\lambda_l\)</span>（频率）下的<span class="math inline">\(f\)</span>的graph傅立叶变换就是与<span class="math inline">\(\lambda_l\)</span>对应的特征向量<span class="math inline">\(u_{l}\)</span>进行内积运算。将上式推广到矩阵形式，就有 <span class="math display">\[\left(\begin{array}{c}\hat{f}\left(\lambda_{1}\right) \\\hat{f}\left(\lambda_{2}\right) \\\vdots \\\hat{f}\left(\lambda_{N}\right)\end{array}\right)=\left(\begin{array}{cccc}u_{1}(1) &amp; u_{1}(2) &amp; \ldots &amp; u_{1}(N) \\u_{2}(1) &amp; u_{2}(2) &amp; \ldots &amp; u_{2}(N) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\u_{N}(1) &amp; u_{N}(2) &amp; \ldots &amp; u_{N}(N)\end{array}\right)\left(\begin{array}{c}f(1) \\f(2) \\\vdots \\f(N)\end{array}\right)\]</span> <span class="math display">\[\hat{f} = U^T f\]</span></p><p>图上的傅立叶逆变换类似于传统傅立叶逆变换的对频率求积分： <span class="math display">\[f(i)=\sum_{l=1}^{N} \hat{f}\left(\lambda_{l}\right) u_{l}(i)\]</span> <span class="math display">\[f=U \hat{f}\]</span></p><h2 id="gcn">GCN</h2><p>上文从百草园讲到三味书屋，终于要讲到了本文的主角-GCN。在上面graph傅立叶变换的基础上，我们可以将卷积推广到graph上。 <span class="math display">\[f * h=\mathcal{F}^{-1}[\hat{f}(\omega) \hat{h}(\omega)]=\frac{1}{2 \Pi} \int \hat{f}(\omega) \hat{h}(\omega) e^{i \omega t} d \omega\]</span> 类比到graph上，函数<span class="math inline">\(f\)</span>与卷积核<span class="math inline">\(h\)</span>在graph上的卷积 <span class="math display">\[(f * h)_{G}=U\left(\begin{array}{lll}\hat{h}\left(\lambda_{1}\right) &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \hat{h}\left(\lambda_{n}\right)\end{array}\right) U^{T} f\]</span> 式中<span class="math inline">\(\hat{h}\left(\lambda_{l}\right)=\sum_{i=1}^{N} h(i) u_{l}^{*}(i)\)</span>是根据需要设计的卷积核<span class="math inline">\(h\)</span>在graph上的傅立叶变换。 图表示学习中用的定义为 <span class="math display">\[(f * h)_{G}=U\left(\left(U^{T} h\right) \odot\left(U^{T} f\right)\right)\]</span> <span class="math inline">\(\odot\)</span> 表示Hadamard product，对两个维度相同的向量、矩阵进行对应位置的逐元素乘积。</p><p>将神经网络与graph卷积结合，只需令卷积核变为可学习的参数，也就得到 <span class="math display">\[y_{\text {output }}=\sigma\left(U \left(\begin{array}{lll}\theta_{1} &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \theta_{n}\end{array}\right) U^{T} x\right)\]</span> 我们将卷积核记为<span class="math inline">\(g(\Lambda)\)</span> (<span class="math inline">\(\Lambda\)</span>就是大写的<span class="math inline">\(\lambda\)</span>)。<br />这种图卷积被称为<a href="https://arxiv.org/abs/1312.6203">第一代图卷积</a>，但这类图卷积计算开销非常大，且卷积核有n个参数，这种图卷积很难处理工业级的数据。 <a href="https://arxiv.org/pdf/1606.09375.pdf">第二代图卷积</a>使用了polynomial filter <span class="math display">\[g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} \Lambda^{k}=\left(\begin{array}{ccc}\sum_{j=0}^{K} \alpha_{j} \lambda_{1}^{j} &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \\&amp; &amp; &amp; \sum_{j=0}^{K} \alpha_{j} \lambda_{n}^{j}\end{array}\right)\]</span> 其中<span class="math inline">\(\theta \in \mathbb{R}^{K}\)</span>是多项式系数向量。进而可以推出 <span class="math display">\[U \sum_{j=0}^{K} \alpha_{j} \Lambda^{j} U^{T}=\sum_{j=0}^{K} \alpha_{j} U \Lambda^{j} U^{T}=\sum_{j=0}^{K} \alpha_{j} L^{j}\]</span> <span class="math display">\[y_{\text {output }}=\sigma\left(\sum_{j=0}^{K-1} \alpha_{j} L^{j} x\right)\]</span> 此时，我们计算图卷积运算就不需要再乘上特征向量矩阵<span class="math inline">\(U\)</span>，而是直接使用拉普拉斯矩阵<span class="math inline">\(L\)</span>的k 次方（K远小于n），这样就避免了进行特征分解。而我们可以事先计算好<span class="math inline">\(L^K\)</span> ，这样就只需要计算矩阵相乘。 此外，高阶拉普拉斯可以用切比雪夫展开来近似，因此卷积核还可以用切比雪夫多项式来表示 <span class="math display">\[g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} T_{k}(\tilde{\Lambda})\]</span> 而将切比雪夫多项式的阶数限制为2的时候就得到在下游任务中常用的图卷积公式： <span class="math display">\[H^{(l+1)}=\sigma\left(\widetilde{D}^{-\frac{1}{2}} \widetilde{A} \widetilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)\]</span> 上式中，<span class="math inline">\(\widetilde{A}=A+I, \mathrm{~A}\)</span>为邻接矩阵, <span class="math inline">\(I\)</span>为单位矩阵, 所以<span class="math inline">\(\widetilde{A}\)</span> 为添加自连接的邻接矩阵;<span class="math inline">\(W^{(l)}\)</span> 为神经网络第<span class="math inline">\(l\)</span>层的权重矩阵; <span class="math inline">\(\sigma(\cdot)\)</span>是激活函数</p><h2 id="gcn-self-attention">GCN &amp; Self-attention</h2><p>上面从频域分析graph convolution，当我们从空间域上分析得到的卷积公式时，可以看出它仍是一种message passing机制。 <span class="math display">\[A  H^{(l)}  W^{(l)}\]</span> 上式可以分为两个步骤，首先是<span class="math inline">\(H\)</span>与参数矩阵<span class="math inline">\(W\)</span>做一个线性映射，而后与邻节点及其边信息进行聚合汇总。这一框架在某种意义上说与self- attention是非常类似的。</p><p>self- attention包含query、key、value；其中输入的query与每个key计算相似度，而后得到一个注意力系数<span class="math inline">\(\alpha\)</span>，再由注意力系数对value进行加权求和输出最终的结果。抛开邻节点后，这两者的计算机制可以说非常类似，都可以被囊括在message passing这一框架下，而self-attention也可以理解为在完成图（所有节点都相连）上的GCN。而Transformer以及GNN在NLP中的广泛应用也从某种意义上说明了两者之间存在某种相似性。</p><h2 id="gcn应用">GCN应用</h2><h3 id="textgcn">TextGCN</h3><p><img src="/images/gcn/2.png" title="TextGCN" /> 论文<a href="https://arxiv.org/abs/1809.05679v3">Graph Convolutional Networks for Text Classification</a>所构建的TextGCN将GCN用于文本分类中，在电影评价、新闻等数据集上都取得了不错的表现。<br />如上图所示，模型将语料库中的每一篇文档和语料库中词作为节点，联合构建了一个异构图，再借助GCN进行特征传播，得到每个文档节点的embedding后进行softmax分类。<br />文章中节点都使用one-hot vector进行初始化，而文档-词边、词-词边分别用TF-IDF、PMI赋以不同的权重，而最终得到的分类准确率比传统的CNN、LSTM等网络效果要高，足以证明GCN在NLP任务中的潜力。此外，后续用BERT进行节点初始化的BERTGCN也是目前的文本分类的SOTA模型。</p><h3 id="st-gcn">ST-GCN</h3><p><img src="/images/gcn/4.png" title="ST-GCN" /> <a href="https://arxiv.org/pdf/1801.07455.pdf">ST-GCN</a>可以说是GCN在骨骼行为识别里面的开山之作。</p><h3 id="sgc">SGC</h3><p><img src="/images/gcn/3.png" title="Simplifying Graph Convolutional Networks (SGC)" /> 作者将图卷积层中的激活函数去掉，得到了SGC在许多NLP任务上更优的结果，且模型速度有了极大的提升。</p><h2 id="再回首">再回首</h2><p>回顾一下上文的内容，首先GCN是GNN的一种，从公式上看聚合函数采用的是图的拉普拉斯矩阵。当我们从频域上分析图的拉普拉斯矩阵及其特征分解之后可以发现，拉普拉斯矩阵的特征向量可以作为傅里叶变换的基、特征值表示频率，从而就可以定义图上的傅立叶变换，进而扩展到卷积操作。而将图卷积与神经网络结合后，借助多项式优化后就得到了现在常用的卷积公式。而从空间域中看，图卷积本质上也就是一种信息传播机制，借助边的权重信息对邻节点的特征做限制后传播、聚合、更新节点原本的特征。</p><p>在频域分析过程中我们可以得到，在由Graph确定的<span class="math inline">\(n\)</span>维空间中，越小的特征值 <span class="math inline">\(\lambda_{l}\)</span> 表明：拉普拉斯矩阵 <span class="math inline">\(L\)</span> 其所对应的基 <span class="math inline">\(u_{l}\)</span> 上的分量、&quot;信息&quot;越少、高频部分。所以图卷积有时候也被认知为是图上的高斯平滑，一种滤波的过程，这也导出了图卷积中的一大问题：over-smooshing。当图卷积层数加深时，图上节点自身的特征会因为不断的传播后导致自身的特征消失，所有节点的特征会越来越接近，进而使得下游任务准确率下降。</p><h3 id="gcn-vs-cnn">GCN vs CNN</h3><p>GCN可以退化为CNN...TODO</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;图卷积网络作为图神经网络之一，具有非常广泛的应用。因此网上也有非常多的关于GCN的介绍，但是各种博客看的多了搞得我脑壳嗡嗡的，而刚好最近的一个工作涉及到GCN的内核，因此借助这篇博客整理对GCN进行整理。&lt;/p&gt;
&lt;p&gt;在介绍GCN先介绍几篇文献：&lt;br /&gt;
Semi-S</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>读书人的事，能叫偷吗？ [EMNLP2021/AAAI2021]</title>
    <link href="http://example.com/2021/11/23/pd7/"/>
    <id>http://example.com/2021/11/23/pd7/</id>
    <published>2021-11-23T14:09:00.000Z</published>
    <updated>2022-01-06T01:42:20.071Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>最近似乎吃到了一个瓜</p><blockquote><p>如何看待EMNLP'21的文章涉嫌抄袭EMNLP'20上的文章?<br />本人在阅读EMNLP2021的文章时，偶然发现一篇名为Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model的文章，这篇文章的内容与EMNLP2020上的一篇文章Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders高度相似。</p></blockquote><p>无独有偶，AAAI2021的论文 Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance 也被人指出涉嫌抄袭ACL2020的文章 A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</p><p>先不谈到底有没有抄袭事件，我们来看一看这几篇文章的所做的工作</p><h1 id="emnlp20">EMNLP'20</h1><p>Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</p><h2 id="intro">Intro</h2><p>本文针对实体关系联合抽取问题。现有的方法将NER和RE使用Table-Filling的方法完成联合抽取，但他们在学习过程中仅使用单个encoder来捕获两个任务的信息，而在本文提出的模型中，作者设计了表格式编码器和序列式编码器来进行表示学习。</p><h2 id="model">Model</h2><p><img src="/images/pd7/1.png" title="table filling for NER and RE" /> 对于传统的联合抽取问题，首先构建一个二维表，而后将NER问题视为序列标注为题（沿着表对角线），将RE问题视为对表中的其他条目进行标注，这个二维表能解决两个任务，并能实现两个任务间的潜在交互。<br />但是作者认为这种现有的方式会出现两个问题，一是用于同样的特征表示进行NER和RE(关系分类)两个任务，可能会对模型的学习造成误解 (Feature Confusiong)；二是基于Table-Filling方法去完成联合抽取的工作，会将表结构转化成一个序列结构，这样导致丢失了重要的结构信息。</p><p>对此，本文中作者设计了两个encoder：sequence representations and table representations, for NER and RE respectively。而且设计了一个两者之间交互机制来获取两个任务间的潜在联系。</p><h3 id="text-embedding">Text Embedding</h3><p>对于一个输入的包含n个words的句子，其词向量（LSTM）、字符向量（LSTM）和BERT词向量的共同构成了每个word的表示。 <span class="math display">\[\boldsymbol{S}_{0}=\operatorname{Linear}\left(\left[\boldsymbol{x}^{c} ; \boldsymbol{x}^{w} ; \boldsymbol{x}^{\ell}\right]\right)\]</span></p><p><img src="/images/pd7/2.png" title="table-sequence encoders" /></p><h3 id="table-encoder">Table Encoder</h3><p>Table中第i行第j列分别代表句子中的第i和第j个词，而Table encoder则要学习每个单元格的向量表示。文中使用基于GRU结构的MD-RNN(多维RNN)作为Text Encoder，在更新表格中当前cell的信息时，通过MDRNN融合其上下左右四个方向上的信息，从而利用了表格的结构特点： <img src="/images/pd7/4.png" title="GRU结构的MD-RNN" /> <span class="math display">\[T_{l, i, j}=\operatorname{GRU}\left(X_{l, i, j}, T_{l-1, i, j}, T_{l, i-1, j}, T_{l, i, j-1}\right)\]</span> 同时引入当前cell所对应的两个词在Sequence Encoder下的表示，使得Table Encoder和Sequence Encoder之间发生信息的交流；</p><p>另外，Text Embeddings部分有用到BERT, 因此将BERT中各个层上多头attention每个头上的 atention权重堆叠起来, 得到张量<span class="math inline">\(T^{l} \in \mathbb{R}^{N \times N \times\left(L^{l} \times A^{l}\right)}\)</span>。基于<span class="math inline">\(T^{l}\)</span> 和Text Embedding中每个词的表示，构成Table的初始化输入表示。</p><h3 id="sequence-encoder">Sequence Encoder</h3><p>Sequence Encoder的结构与Transformer类似，不同之处在于将Transformer中的scaled dot-product attention 替 换为了文中提出的table-guided attention。具体地, 将Transformer中计算 <span class="math inline">\(g\left(Q_{i}, K_{j}\right)\)</span> 的部分, 直接替换为对应两个word在表格中的 向量表示 <span class="math inline">\(T_{i, j}\)</span> 。由于 <span class="math inline">\(T_{i, j}\)</span> 融合了四个方向上的信息, 能够更加充分的捕捉上下文信息以及词与词之间的关系, 同时也使Table Encoder和Sequence Encoder之间产生了双向的信息交流。</p><h1 id="emnlp21">EMNLP'21</h1><p>20的文章所针对的问题是实体和关系的联合抽取，而21的文章所针对的问题是情感三元组抽取，但本质上二者所解决的问题都属于信息抽取中的关系抽取。 <img src="/images/pd7/3.png" title="论文部分截图" /> 这篇文章相当于将上面的模型换了一个任务跑，文章的配图除了配色外几乎一模一样。“幸运”的事审稿人应该没有读过20年的文章，而本文的作者也没有在论文中提及或引用，最终使得文章能成功过审。</p><h1 id="acl20">ACL'20</h1><p><strong>A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</strong></p><h2 id="intro-1">Intro</h2><p>文章针对多模态机器翻译问题 (Multi-modal Neural Machine Translation) 。为了更好的捕获细粒度的不同模态语义单元间的对应关系，论文提出了一种基于图的多模态融合编码器 graph-based multi-modal fusion encoder。具体而言，模型首先把图和句子用一个统一多模态图来表示。然后基于该图，我们堆叠基于多个图的多模态融合层，这些层迭代地表示节点之间的语义交互以进行图编码， 依次进行模态内和模态间融合以学习多模态节点表示。 最后，解码器可以通过注意机制来利用这些表示形式。</p><h2 id="multi-modal-graph">Multi-modal graph</h2><p><img src="/images/pd7/5.png" title="多模态图构建" /></p><p>上图中绿色和蓝色的节点分别代表视觉节点和文本节点，节点间的边包括跨模态的边和单模态内部的边。具体而言，首先对于节点而言，每个节点为a textual word or a visual object。每个词作为一个节点；而后利用斯坦福检测器 (Stanford parser) 识别输入句子中的所有名词短语，然后应用可视化基础工具包检测每个名词短语的边界框，得到对应的视觉节点。intra-modal edge用于连接单模态内部的节点，inter-modal edge用于将视觉节点与其对应的文本节点相连</p><h2 id="encoder-decoder">Encoder-Decoder</h2><p><img src="/images/pd7/6.png" title="模型整体框架" /> 首先借助embedding layer对节点嵌入进行初始化。<br />对于文本节点，使用其词嵌入和位置编码的和；对于视觉节点，首先从Faster-RCNN中的全连接层中提取视觉特征，而后映射到相应维度上。</p><p>在embedding层的后堆叠了多个基于图的多模态融合层进行编码。在每个融合层，模型依次进行单模态内融合和跨模态间融合以更新所有节点状态。最终节点状态对模态内和模态间信息同时进行编码。由于视觉节点和文本节点是包含不同模态信息的两种语义单元，因此我们分别应用相似的操作，但使用不同的参数来对它们的状态更新过程进行建模。</p><ul><li>Intra-modal fusion</li></ul><p>对于单模态内的，文本信息和视觉信息都用mulit-head attention来提取： <span class="math display">\[\mathbf{C}_{x}^{(l)}=\operatorname{MultiHead}\left(\mathbf{H}_{x}^{(l-1)}, \mathbf{H}_{x}^{(l-1)}, \mathbf{H}_{x}^{(l-1)}\right)\]</span> <span class="math display">\[\mathbf{C}_{o}^{(l)}=\operatorname{MultiHead}\left(\mathbf{H}_{o}^{(l-1)}, \mathbf{H}_{o}^{(l-1)}, \mathbf{H}_{o}^{(l-1)}\right)\]</span></p><ul><li>Inter-modal fusion</li></ul><p>跨模态的信息，作者设计了跨模态门控机制。对于文本跨模态信息<span class="math inline">\(M_x\)</span>，<span class="math inline">\(A\left(v_{x_{i}}\right)\)</span>是与词节点<span class="math inline">\(v_{x_{i}}\)</span>相邻的视觉节点，<span class="math inline">\(W^{(l)}\)</span>为参数矩阵。<span class="math inline">\(\alpha_{ij}\)</span>指第i个词节点对第j个视觉节点的权重，结合了节点的语义信息和视觉信息。相应的也可以得到视觉跨模态信息<span class="math inline">\(M_o\)</span>。 <span class="math display">\[\begin{aligned}M_{x_{i}}^{(l)} &amp;=\sum_{j \in A\left(v_{x_{i}}\right)} \alpha_{i, j} \odot C_{o_{j}}^{(l)} \\\alpha_{i, j} &amp;=\operatorname{Sigmoid}\left(\mathbf{W}_{1}^{(l)} C_{x_{i}}^{(l)}+\mathbf{W}_{2}^{(l)} C_{o_{j}}^{(l)}\right)\end{aligned}\]</span> <span class="math display">\[\begin{aligned}M_{o_{j}}^{(l)} &amp;=\sum_{i \in A\left(v_{o_{j}}\right)} \beta_{j, i} \odot C_{x_{i}}^{(l)} \\\beta_{j, i} &amp;=\operatorname{Sigmoid}\left(\mathbf{W}_{3}^{(l)} C_{o_{j}}^{(l)}+\mathbf{W}_{4}^{(l)} C_{x_{i}}^{(l)}\right)\end{aligned}\]</span> 而后分别经过对应的全连接前馈神经网络： <span class="math display">\[\begin{array}{l}\mathbf{H}_{x}^{(l)}=\operatorname{FFN}\left(\mathbf{M}_{x}^{(l)}\right) \\\mathbf{H}_{o}^{(l)}=\operatorname{FFN}\left(\mathbf{M}_{o}^{(l)}\right)\end{array}\]</span></p><p>Decoder的结构与常规的Transformer的decoder非常类似。由于视觉信息已通过多模式融合层合并到所有文本节点中，因此解码器仅使用文本节点状态来动态利用多模式上下文信息。输入的embedding会经过两个attention，分别是decoder内部的和encoder-decoder之间的attention。而后经过全连接层、Softmax得到输出结果。</p><h1 id="aaai21">AAAI'21</h1><p><strong>Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance</strong></p><p>论文的关于多模态命名实体识别(Multi-modal named entity recognition, MNER)，模型能发现文本中的命名实体，并利用图像将其分类为预定义的类型。图片用于辅助分类时，本文模型在图片中找到实体（来源于文本信息）所在区域，截出对应的部分进行视觉信息抽取。</p><p><img src="/images/pd7/7.png" title="模型整体架构" /> 先来看一下这篇论文的主图，从图上可以看出模型的整个框架与2020年的论文非常类似。其中图节点嵌入的初始化分别用BERT和ResNet来代替；而后的encoder-decoder与20年的模型基本一致。</p><p><img src="/images/pd7/8.png" title="Graph-based Multi-modal Fusion" /> 这一部分从论文中也可以看出，多模态图中跨模态间和单模态内的处理与20年论文的处理方法一致。</p><h1 id="p.s.">p.s.</h1><p>最近尝试复现旷视的<a href="/2021/10/26/pd5/" title="ML-GCN">ML-GCN</a>也遇到了大无语事件，其一是用了好几个版本的Pytorch都无法较好的收敛，看网友的一些讨论说居然要用19年特定的0.3.1版本；其二就是ML-GCN中最主要的模块GCN对模型的准确率没有任何贡献，去掉该模块反而有更好的效果。😅<br />怪不得会有人用“魔改”这种略带侮辱性的词来形容AI的科研。希望能有越来越多的人做有意义的科研，也希望“独角兽”们和“master”们把挣钱和科研分开。“挣钱嘛，不寒碜“ （让子弹飞赶紧申遗），但既然想挣钱就别往科研里钻，整些无意义灌水甚至是剽窃他人的成果。<br />当然，抛开环境现状不谈来批判某些个体的行为似乎有些耍流氓。还记得大二时候导师跟我和zb说要做一个一身正气的科研工作者。（虽然现在也说不准以后是进学术界还是工业界）国家快速发展总会留下一些弊病，希望能从高校老师和学生开始，逐渐构建一片净土，让热爱学术的人能够坚定不移的走下去。至少以后学术造假等事件曝光时，我们看到的主角不应该是中国人的名字。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;最近似乎吃到了一个瓜&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如何看待EMNLP&#39;21的文章涉嫌抄袭EMNLP&#39;20上的文章?&lt;br /&gt;
本人在阅读EMNLP2021的文章时，偶然发现一篇名为Seeking Common but D</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="NER" scheme="http://example.com/tags/NER/"/>
    
  </entry>
  
  <entry>
    <title>SVGD补充</title>
    <link href="http://example.com/2021/11/18/svgd-2/"/>
    <id>http://example.com/2021/11/18/svgd-2/</id>
    <published>2021-11-18T02:03:58.000Z</published>
    <updated>2021-11-18T05:41:21.352Z</updated>
    
    <content type="html"><![CDATA[<p>关于<a href="/2021/11/09/svgd/" title="SVGD">SVGD</a>的内容，这一篇博客中大致做了简单的介绍，包括从上到下的数学推导和简单的逻辑链。但整个贝叶斯推断背后还有许多思想，且SVGD背后也隐藏的一些思维过程。这篇Blog将作为SVGD相关内容的补充以及涉及到的知识体系的简要归纳。</p><h1 id="svgd实验">SVGD实验</h1><h2 id="拟合高斯分布">拟合高斯分布</h2><h2 id="贝叶斯神经网络">贝叶斯神经网络</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;关于&lt;a href=&quot;/2021/11/09/svgd/&quot; title=&quot;SVGD&quot;&gt;SVGD&lt;/a&gt;的内容，这一篇博客中大致做了简单的介绍，包括从上到下的数学推导和简单的逻辑链。但整个贝叶斯推断背后还有许多思想，且SVGD背后也隐藏的一些思维过程。这篇Blog将作为SVG</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="SVGD" scheme="http://example.com/tags/SVGD/"/>
    
  </entry>
  
  <entry>
    <title>Kernels and Hilbert Space</title>
    <link href="http://example.com/2021/11/16/kernel/"/>
    <id>http://example.com/2021/11/16/kernel/</id>
    <published>2021-11-16T11:43:16.000Z</published>
    <updated>2021-11-17T12:21:34.962Z</updated>
    
    <content type="html"><![CDATA[<p>Reference：<br />‘Introduction to Hilbert Spaces with Application.’<br />‘Introduction to RKHS, and some simple kernel algorithms.’</p><p>Since Kernel trick is one of the core methods in SVM and SVGD also involves expertise related to RKHS. I looked up several books on Kernel method, trying to get a systematic understanding of Kernel and Hilbert space. This blog can also be regarded as a summary and summary of the book ‘Introduction to Hilbert Spaces with Application ’.</p><h1 id="introduction">Introduction</h1><p><img src="/images/kernel/1.png" title="XOR example" /> <img src="/images/kernel/2.png" title="Document classification example" /></p><h2 id="kernel">Kernel</h2><p>Definition: Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}\]</span></p><h1 id="normed-vector-spaces">Normed Vector Spaces</h1><p>First, the space defined in mathematics can be divided from simple to complex as:</p><ul><li><p>Vector Space<br />a nonempty set <span class="math inline">\(E\)</span> with two operations: <em>addition</em> and <em>multiplication by scalars</em>.<br />e.g. <span class="math inline">\(\mathbb{R}^{N}\)</span> <span class="math inline">\(\mathbb{C}^{N}\)</span></p></li><li><p>Normed Space<br />norm is an abstract generalization of the length of a vector:<br />function <span class="math inline">\(x \mapsto\|x\|\)</span> from a vector space <span class="math inline">\(E\)</span> into <span class="math inline">\(\mathbb{R}\)</span></p></li><li><p>Banach Space: complete normed space<br />A normed space is complete if and only if every absolutely convergent series converges. (The contents of Cauchy sequence and Cauchy series are put in the appendix)<br />Actually, Banach space introduces the concept of Limits</p></li><li><p>Inner Product Spaces<br />The space that defines the <a href="#jump">inner product</a>.</p></li><li><p>Hilbert Spaces: A complete inner product space</p></li></ul><h1 id="hilbert-spaces">Hilbert Spaces</h1><h1 id="appendix">Appendix</h1><h2 id="cauchy-sequence-and-cauchy-series">Cauchy sequence and Cauchy series</h2><p>Definition of <strong><em>Cauchy sequence</em></strong>. A sequence <span class="math inline">\(\left\{f_{n}\right\}_{n=1}^{\infty}\)</span> of elements in a normed space <span class="math inline">\(\mathcal{H}\)</span> is said to be a Cauchy sequence if for every <span class="math inline">\(\epsilon&gt;0\)</span>, there exists <span class="math inline">\(N=N(\varepsilon) \in \mathbb{N}\)</span>, such that for all <span class="math inline">\(n, m \geq N,\left\|f_{n}-f_{m}\right\|_{\mathcal{H}}&lt;\epsilon\)</span></p><h2 id="inner-product">Inner product</h2><p><span id="jump"> </span> Definition of <strong><em>Inner product</em></strong>. Let <span class="math inline">\(\mathcal{H}\)</span> be a vector space over <span class="math inline">\(\mathbb{R}\)</span>. A function <span class="math inline">\(\langle\cdot, \cdot\rangle_{\mathcal{H}}: \mathcal{H} \times \mathcal{H} \rightarrow \mathbb{R}\)</span> is said to be an inner product on <span class="math inline">\(\mathcal{H}\)</span> if:</p><ul><li><p><span class="math inline">\(\left\langle\alpha_{1} f_{1}+\alpha_{2} f_{2}, g\right\rangle_{\mathcal{H}}=\alpha_{1}\left\langle f_{1}, g\right\rangle_{\mathcal{H}}+\alpha_{2}\left\langle f_{2}, g\right\rangle_{\mathcal{H}}\)</span></p></li><li><p><span class="math inline">\(\langle f, g\rangle_{\mathcal{H}}=\langle g, f\rangle_{\mathcal{H}}{ }^{1}\)</span></p></li><li><p><span class="math inline">\(\langle f, f\rangle_{\mathcal{H}} \geq 0\)</span> and <span class="math inline">\(\langle f, f\rangle_{\mathcal{H}}=0\)</span> if and only if <span class="math inline">\(f=0\)</span>.</p></li></ul><p>the inner product between matrices <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(B \in\)</span> <span class="math inline">\(\mathbb{R}^{m \times n}\)</span> is <span class="math display">\[\langle A, B\rangle=\operatorname{trace}\left(A^{\top} B\right)\]</span></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Reference：&lt;br /&gt;
‘Introduction to Hilbert Spaces with Application.’&lt;br /&gt;
‘Introduction to RKHS, and some simple kernel algorithms.’&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="Kernels" scheme="http://example.com/tags/Kernels/"/>
    
  </entry>
  
  <entry>
    <title>Stein variational gradient descent (NIPS2018)</title>
    <link href="http://example.com/2021/11/09/svgd/"/>
    <id>http://example.com/2021/11/09/svgd/</id>
    <published>2021-11-09T12:28:58.000Z</published>
    <updated>2021-11-18T05:48:02.952Z</updated>
    
    <content type="html"><![CDATA[<h1 id="intro">Intro</h1><p>这一工作是清华大学liu qiang老师提出的，相关论文从2016年开始也一直在更新，分别发表在NIPS、ICLR等顶会上。<br /><a href="https://arxiv.org/abs/1704.07520">Stein Variational Gradient Descent as Gradient Flow</a><br /><a href="https://arxiv.org/abs/1810.11693">Stein Variational Gradient Descent as Moment Matching</a><br /><a href="https://arxiv.org/pdf/1608.04471.pdf">Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a></p><p>从定义上来说，SVGD是一种确定性的采样算法，用一组粒子来近似给定的分布。基于这两点，它和MCMC以及VI都有共通之处。但SVGD即保证了在大量数据下的计算速度，也比变分推断具有更高的准确性。</p><p>从整体上来看，这一工作通过引入Stein discrepancy来度量两个分布之间的距离，再借助RKHS使其容易计算，最后借助gradient descent进行优化。因此下文也就从这三部分一一介绍。</p><h1 id="background">Background</h1><h2 id="steins-method">Stein's method</h2><p>首先我们需要引入几个定义：</p><ul><li><p><em>Stein score function</em> <span class="math display">\[\boldsymbol{s}_{p}=\nabla_{x} \log p(x)=\frac{\nabla_{x} p(x)}{p(x)}\]</span> 这一函数被称为<span class="math inline">\(q(x)\)</span>的Stein score function</p></li><li><p><em>Stein class</em><br />当函数<span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}\)</span>满足下式时则称其在stein class中 <span class="math display">\[\int_{x \in \mathcal{X}} \nabla_{x}(f(x) p(x)) d x=0\]</span> 其中<span class="math inline">\(\mathcal{X}\)</span> 是<span class="math inline">\(\mathbb{R}^{d}\)</span>下的子集，而<span class="math inline">\(p(x)\)</span>则是在<span class="math inline">\(\mathcal{X}\)</span> 下连续可微的分布。</p></li><li><p><em>Stein's operator</em>：作用在<span class="math inline">\(p\)</span>上的线性操作 <span class="math display">\[\mathcal{A}_{p} f(x)=\boldsymbol{s}_{p}(x) f(x)+\nabla_{x} f(x)\]</span> 其中<span class="math inline">\(s_{p}\)</span>和<span class="math inline">\(\mathcal{A}_{p} f\)</span> 都是<span class="math inline">\(d \times 1\)</span> 函数(mapping from <span class="math inline">\(\mathcal{X}\)</span> to <span class="math inline">\(\mathbb{R}^{d}.\)</span>)</p></li></ul><p>有了以上三个定义后，我们可以尝试得到stein discrepancy。首先作为一个度量手段，必然需要满足一些条件。<br />当且仅当 <span class="math display">\[\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]=0   \qquad   (1) \]</span> <span class="math inline">\(p(x)\)</span> 和 <span class="math inline">\(q(x)\)</span>是相等的。而当两个分布<span class="math inline">\(p=q\)</span>时又被称为stein identity。<br />借助 (1) 式，我们可以定义Stein discrepancy来度量两个分布<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>之间的差异： <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]\right)^{2}\]</span> 借助之前定义的stein operator，也可以把上式写为 <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\mathcal{A}_{q} f(x)\right]\right)^{2}\]</span></p><p><span class="math inline">\(\mathcal{F}\)</span>是一系列连续可微的且满足<span class="math inline">\(\mathbb{S}(p, q)\)</span>不为0(<span class="math inline">\(p \neq q\)</span>时)函数集合。当<span class="math inline">\(p \neq q\)</span>时，<span class="math inline">\(\mathbb{S}(p,q)&gt;0\)</span>，而<span class="math inline">\(max\)</span>则是因为我们希望距离尽可能明显。</p><p><span class="math inline">\(\mathbb{S}(p, q)\)</span>并没有被广泛应用在机器学习中，因为其计算和优化的复杂性: <span class="math inline">\(q(x)=f(x) / Z\)</span> 而<span class="math inline">\(Z=\int f(x) d x\)</span>的计算往往设计高维积分。<br />但是论文提出了将函数<span class="math inline">\(\mathcal{F}\)</span>用核函数代替时，会得到易于计算的Stein discrepancy <span class="math inline">\(\mathbb{S}(p, q)\)</span>。具体而言，我们令<span class="math inline">\(\mathcal{F}\)</span>来源于希尔伯特再生核空间的一个球 (reproducing kernel Hilbert space (RKHS))。</p><h2 id="kernelized-stein-discrepancy">Kernelized Stein Discrepancy</h2><p>对于映射后的函数，对应的正定核<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>，我们有 <span class="math display">\[\mathbb{S}(p, q)=\mathbb{E}_{x, x^{\prime} \sim p}\left[u_{q}\left(x, x^{\prime}\right)\right]\]</span> 其中<span class="math inline">\(x, x^{\prime}\)</span>是<span class="math inline">\(p\)</span>中独立同分布的两个变量，函数<span class="math inline">\(u_{q}\left(x, x^{\prime}\right)\)</span>由<span class="math inline">\(q\)</span>确定，如果展开的话实际上是： <span class="math display">\[u_{q}\left(x, x^{\prime}\right)= \boldsymbol{s}_{q}(x)^{\top} k\left(x, x^{\prime}\right) \boldsymbol{s}_{q}\left(x^{\prime}\right)+\boldsymbol{s}_{q}(x)^{\top} \nabla_{x^{\prime}} k\left(x, x^{\prime}\right)+\nabla_{x} k\left(x, x^{\prime}\right)^{\top} \boldsymbol{s}_{q}\left(x^{\prime}\right)+\operatorname{trace}\left(\nabla_{x, x^{\prime}} k\left(x, x^{\prime}\right)\right)\]</span></p><p>当我们从未知分布<span class="math inline">\(p(x)\)</span>采样出一个样本<span class="math inline">\({x_i}\)</span>时，我们可以进行近似计算 <span class="math display">\[\hat{\mathbb{S}}(p, q)=\frac{1}{n(n-1)} \sum_{i \neq j} u_{q}\left(x_{i}, x_{j}\right)\]</span></p><p>接下来我们详细介绍上述的过程。</p><h3 id="kernels-and-reproducing-kernel-hilbert-spaces">Kernels and Reproducing Kernel Hilbert Spaces</h3><a href="/2021/11/16/kernel/" title="Kernel and Hilbert Spaces介绍 (未完待续)">Kernel and Hilbert Spaces介绍 (未完待续)</a><p>令<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>为一个正定核，根据Mercer’s theorem我们对其进行谱分解： <span class="math display">\[k\left(x, x^{\prime}\right)=\sum_{j} \lambda_{j} e_{j}(x) e_{j}\left(x^{\prime}\right)\]</span> 其中<span class="math inline">\(\left\{e_{j}\right\},\left\{\lambda_{j}\right\}\)</span>分别是正交特征函数和正特征值，满足<span class="math inline">\(\int e_{i}(x) e_{j}(x) d x=\mathbb{I}[i=j]\)</span>, for <span class="math inline">\(\forall i, j\)</span></p><p>对于一个正定核，它可以分解为RKHS中特征函数的线性组合（空间中任何一个函数可以用这组基的线性组合来表示）。由一个特定的核函数能产生一个唯一的Hilbert空间，有性质 <span class="math display">\[f(x)=\langle f, k(\cdot, x)\rangle_{\mathcal{H}}, \quad k\left(x, x^{\prime}\right)=\left\langle k(\cdot, x), k\left(\cdot, x^{\prime}\right)\right\rangle_{\mathcal{H}}\]</span> 当我们定义 <span class="math inline">\(\mathcal{H}^{d}=\mathcal{H} \times \mathcal{H} \times \cdots \mathcal{H}\)</span> 为 <span class="math inline">\(d\)</span> 维向量函数 <span class="math inline">\(\mathbf{f}=\left\{f_{i}: f_{i} \in \mathcal{H} \quad i=1, \cdots, d\right\}\)</span> 组成的 Hilbert空间, <span class="math inline">\(\mathcal{H}^{d}\)</span> 上的内积定义为 <span class="math inline">\(&lt;\mathbf{f}, \mathbf{g}&gt;_{\mathcal{H}^{d}}=\sum_{i=1}^{d}&lt;f_{i}, g_{i}&gt;_{\mathcal{H}}\)</span> 。如果觉得上述的介绍太过抽象，可以看附录部分关于RKHS的一个<a href="#jump">toy example</a></p><h3 id="lemmas">lemmas</h3><p><strong>Stein's Identity</strong> ： <span class="math display">\[\mathbb{E}_{p}\left[\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)\right]=0\]</span> 证明的话根据<span class="math inline">\(\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)=\nabla_{x}(\boldsymbol{f}(x) p(x)) / p(x)\)</span>和分布积分法则就可以推导出来。</p><p>有了上面的引理，我们可以得到 <span class="math display">\[\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)-\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\left(\boldsymbol{s}_{q}(x)-\boldsymbol{s}_{p}(x)\right) \boldsymbol{f}(x)^{\top}\right]\]</span></p><p>也就是说<span class="math inline">\(\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]\)</span>是一个由<span class="math inline">\(f(x)\)</span>加权的期望，对于<span class="math inline">\(\left(s_{q}(x)-s_{p}(x)\right)\)</span>的期望。</p><h3 id="ksd">KSD</h3><p>借助上面的推导以及RKHS的性质，我们可以定义出核空间下的stein discrepancy： <span class="math display">\[S(p, q)=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right)\right]\]</span> （这里我们将Kernelized前后的stein discrepancy分别写为<span class="math inline">\(\mathbb{S}(p,q)\)</span>和<span class="math inline">\(S(p, q)\)</span>。另外需要注意后续部分推导是针对stein discrepancy中的期望项）<br />我们要用一个可采样的分布<span class="math inline">\(q\)</span>拟合分布<span class="math inline">\(p\)</span>，因此我们希望<span class="math inline">\(S(p, q)\)</span>[]中的式子是与<span class="math inline">\(p\)</span>无关的。把式子展开后可以发现 <span class="math display">\[\begin{aligned}S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}-s_{p}\right)\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T}\left(k(\mathbf{x}, \mathbf{y}) s_{q}+\nabla_{y} k(\mathbf{x}, \mathbf{y})-k(\mathbf{x}, \mathbf{y}) s_{p}-\nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})\right)\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T} v(\mathbf{x}, \mathbf{y})\right]\end{aligned}\]</span> 其中<span class="math inline">\(v(\mathbf{x}, \mathbf{y})=k(\mathbf{x}, \mathbf{y}) s_{q}(\mathbf{y})+\nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})=\mathcal{A}_{q} k_{\mathbf{x}}(\mathbf{y}), k_{\mathbf{x}}(\cdot)=k(\mathbf{x}, \cdot)\)</span></p><p>而对于固定的 <span class="math inline">\(\mathbf{y}\)</span>, 容易证明 <span class="math inline">\(v(\cdot, \mathbf{y})\)</span> 是Stein class of <span class="math inline">\(p(\mathbf{x})\)</span>, 即满足 <span class="math inline">\(\int_{\mathbf{x} \in \mathcal{X}} \nabla_{\mathbf{x}}(v(\mathbf{x}, \mathbf{y}) p(\mathbf{x})) d \mathbf{x}=0\)</span> 。因此就可以进一步将上式写开为 <span class="math display">\[\begin{aligned}S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})-\left(\nabla_{\mathbf{x}} \ln p(\mathbf{x})\right)^{T} v(\mathbf{x}, \mathbf{y})\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})\right]-\int d \mathbf{x} d \mathbf{y} p(\mathbf{x}) p(\mathbf{y})\left(\nabla_{\mathbf{x}} \ln p(\mathbf{x})\right)^{T} v(\mathbf{x}, \mathbf{y}) \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})\right]+\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\operatorname{tr} \nabla_{\mathbf{x}} v(\mathbf{x}, \mathbf{y})\right]\end{aligned}\]</span></p><p>其中<span class="math inline">\(\nabla_{\mathbf{x}} v(\mathbf{x}, \mathbf{y})=\nabla_{\mathbf{x}} k(\mathbf{x}, \mathbf{y}) s_{q}(\mathbf{y})^{T}+\nabla_{\mathbf{x}} \nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})\)</span>为一个矩阵。</p><p>这样就得到了前文提到了 <span class="math display">\[S(p, q)=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[u_{q}(\mathbf{x}, \mathbf{y})\right]\]</span> 其中<span class="math inline">\(u_{q}(\mathbf{x}, \mathbf{y})\)</span>仅与分布<span class="math inline">\(q\)</span>有关。</p><p>上述的推导说明了什么？说明引入核方法的可行性。而另一方面，我们要用核方法来加速内积的计算，如何体现？<br />借助RKHS的对称性和再生性，我们可以将<span class="math inline">\(S(p,q)\)</span>进行变化： <span class="math display">\[\begin{aligned}S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)^{T}&lt;k(\mathbf{x}, \cdot), k(\cdot, \mathbf{y})&gt;_{\mathcal{H}}\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)\right] \\&amp;=\sum_{i=1}^{d}&lt;\mathbb{E}_{\mathbf{x} \sim p}\left[\left(s_{q}^{i}(\mathbf{x})-s_{p}^{i}(\mathbf{x})\right) k(\mathbf{x}, \cdot)\right], \mathbb{E}_{\mathbf{y} \sim p}\left[\left(s_{q}^{i}(\mathbf{y})-s_{p}^{i}(\mathbf{y})\right) k(\cdot \mathbf{y})\right]&gt;_{\mathcal{H}} \\&amp;=\sum_{i=1}^{d}&lt;\beta_{i}, \beta_{i}&gt;_{\mathcal{H}} \\&amp;=\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}^{2}\end{aligned}\]</span> 其中<span class="math inline">\(\boldsymbol{\beta}(\mathbf{y})\)</span>是一个向量函数 <span class="math display">\[\boldsymbol{\beta}(\mathbf{y})=\mathbb{E}_{\mathbf{x} \sim p}\left[\mathcal{A}_{q} k_{\mathbf{y}}(\mathbf{x})\right]=\mathbb{E}_{\mathbf{x} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right) k_{\mathbf{y}}(\mathbf{x})\right]\]</span></p><p>在这里再展示一下最开始的stein discrepancy <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\mathcal{A}_{q} f(x)\right]\right)^{2}\]</span> 对于任意向量 <span class="math inline">\(\mathbf{f} \in \mathcal{H}^{d}\)</span> 与 <span class="math inline">\(\boldsymbol{\beta}\)</span> 的内积为 <span class="math display">\[\begin{aligned}&lt;\mathbf{f}, \boldsymbol{\beta}&gt;_{\mathcal{H}^{d}} &amp;=\sum_{i=1}^{d}&lt;f_{i}, \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x}) k(\mathbf{x}, \cdot)+\nabla_{x_{i}} k(\mathbf{x}, \cdot)\right]&gt;_{\mathcal{H}} \\&amp;=\sum_{i=1}^{d} \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x})&lt;f_{i}, k(\mathbf{x}, \cdot)&gt;_{\mathcal{H}}+&lt;f_{i}, \nabla_{x_{i}} k(\mathbf{x}, \cdot)&gt;_{\mathcal{H}}\right] \\&amp;=\sum_{i=1}^{d} \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x}) f_{i}(\mathbf{x})+\nabla_{x_{i}} f_{i}(\mathbf{x})\right] \\&amp;=\mathbb{E}_{\mathbf{x} \sim p}\left[\operatorname{tr}\left(\mathcal{A}_{q} \mathbf{f}(\mathbf{x})\right)\right] \leq\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}} (因为任意两个向量内积小于它与自身的内积)\end{aligned}\]</span> 因此我们有 <span class="math display">\[\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}=S(p, q)=\max _{f \in \mathcal{H}^{d}}\left\{\mathbb{E}_{\mathbf{x} \sim p}\left[\operatorname{tr}\left(\mathcal{A}_{q} f(\mathbf{x})\right)\right]\right \}.\]</span> 上式中<span class="math inline">\(\|f\|_{\mathcal{H}^{d}} \leq 1\)</span>，对应于最初提到的映射到希尔伯特空间中的一个球中的最优向量。这个<span class="math inline">\(S(p,q)\)</span>最大值所对应的向量为 <span class="math inline">\({f}^{*}=\boldsymbol{\beta} /\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}\)</span></p><p>简单来说，引入kernel之后，我们可以直接得到stein discrepancy定义式中的函数。也就是说，KSD非常容易就能求得。</p><h1 id="stein-variational-gradient-descent-svgd">Stein Variational Gradient Descent (SVGD)</h1><p>SVGD的另一个核心公式在于 <span class="math display">\[\left.\nabla_{\epsilon} \mathrm{KL}\left(q_{[T]} \| p\right)\right|_{\epsilon=0}=-\mathbb{E}_{x \sim q}\left[\operatorname{tr}\left(\mathcal{A}_{p} \phi(x)\right)\right]\]</span> 也就是说KL散度变分求导等于KSD（具体推导过程见附录），这意味着KL散度变化最快的方向就是KSD所对应的向量函数 <span class="math inline">\(\phi^{*}=\boldsymbol{\beta} /\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}\)</span> <img src="/images/vimcmc/2.png" title="算法伪代码" /> 具体而言粒子<span class="math inline">\(\left\{x_{i}^{l}\right\}_{i=1}^{n}\)</span> 表示第<span class="math inline">\(l\)</span>次达代的第<span class="math inline">\(i\)</span>个粒子, 一共<span class="math inline">\(n\)</span> 个。粒子最开始是从分布<span class="math inline">\(q_{0}\)</span>中采样的, 最初的分布<span class="math inline">\(q\)</span>可以是任意 的。也就是说，该算法不依赖于初始的分布。</p><p>算法中的更新项包含了两个部分 <span class="math display">\[k\left(x_{j}^{\ell}, x\right) \nabla_{x_{j}^{\ell}} \log p\left(x_{j}^{\ell}\right)+\nabla_{x_{j}^{\ell}} k\left(x_{j}^{\ell}, x\right)\]</span> 其中第一项意味着粒子会朝<span class="math inline">\(p\)</span>分布概率高的地方移动，而第二项代表着粒子将会朝着远离当前迭代轮数$l ll的粒子，从而减轻局部最优的风险。</p><p><img src="/images/vimcmc/2.gif" title="SVGD拟合一维分布" /></p><h1 id="回顾与总结">回顾与总结</h1><p>从上文繁杂的推导中，SVGD算法确保粒子的移动是朝着KL散度的减小最快方向，而这个方向可以有核化的stein discrepancy导出 我们回看一下KL divergence的定义式： <span class="math display">\[\mathrm{KL}(P \| Q)=\int P(x) \log \frac{P(x)}{Q(x)} d x\]</span></p><p>对于目标分布<span class="math inline">\(p(x)\)</span>，变分推断 (VI)目标是从一类分布族<span class="math inline">\(\mathcal{Q}\)</span>中找到最优的<span class="math inline">\(q(x)\)</span> <span class="math display">\[q^{*}=\underset{q \in \mathcal{Q}}{\arg \min }\left\{K L(q \| p)=\mathbb{E}_{q}[\log q(x)]-\mathbb{E}_{q}[\log \bar{p}(x)]+\log Z\right\}\]</span> 而SVGD在再生核希尔伯特空间下给出了使得KL散度下降最快的确定性方向，类似经典的梯度下降算法，可以理解为迭代构建增量变化的方法。</p><h1 id="appendix">Appendix</h1><p><span id="jump"> </span></p><h2 id="the-reproducing-kernel-hilbert-space">The reproducing kernel Hilbert space</h2><p>先回顾一下kernel的定义： Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}\]</span></p><p>在此基础上，我们用一个异或问题的例子来介绍RKHS。考虑特征映射</p><p><span class="math display">\[\phi: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}\]</span> <span class="math display">\[x=\left[\begin{array}{l}x_{1} \\x_{2}\end{array}\right] \quad \mapsto \quad \phi(x)=\left[\begin{array}{c}x_{1} \\x_{2} \\x_{1} x_{2}\end{array}\right]\]</span> <img src="/images/vimcmc/4.png" title="特征空间和特征映射。希尔伯特空间的元素一般是函数，而函数可以被视为无穷维的向量。因此事实上希尔伯特空间的基底是一组无限维的函数，可以参考傅立叶变化或泰勒展开" /></p><p>kernel <span class="math display">\[k(x, y)=\left[\begin{array}{c}x_{1} \\x_{2} \\x_{1} x_{2}\end{array}\right]^{\top}\left[\begin{array}{c}y_{1} \\y_{2} \\y_{1} y_{2}\end{array}\right]\]</span></p><p>接下来我们可以定义一个特征函数： <span class="math display">\[f(x)=a x_{1}+b x_{2}+c x_{1} x_{2}\]</span> 这个函数属于从<span class="math inline">\(\mathcal{X}=\mathbb{R}^{2}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>的函数空间。此时，我们也可以把函数<span class="math inline">\(f\)</span>等价表示为： <span class="math display">\[f(\cdot)=\left[\begin{array}{l}a \\b \\c\end{array}\right]\]</span> 至此，我们可以把<span class="math inline">\(f(x)\)</span>写为： <span class="math display">\[\begin{aligned}f(x) &amp;=f(\cdot)^{\top} \phi(x) \\&amp;:=\langle f(\cdot), \phi(x)\rangle_{\mathcal{H}}\end{aligned}\]</span> 也就是说，特征函数<span class="math inline">\(f\)</span>在<span class="math inline">\(x\)</span>的值可以被写为特征空间中的内积。<span class="math inline">\(\mathcal{H}\)</span>是一个将<span class="math inline">\(\mathbb{R}^{2}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>的函数空间。上面这些乱七八糟的怎么体现再生性呢？我们仔细看下面的等式 <span class="math display">\[k(\cdot, y)=\left[\begin{array}{c}y_{1} \\y_{2} \\y_{1} y_{2}\end{array}\right]=\phi(y)\]</span> 上式我们参考<span class="math inline">\(f(\cdot)\)</span>类似的定义。具体来说，如果我们令<span class="math inline">\(a=y_{1}, b=y_{2}\)</span>, and <span class="math inline">\(c=y_{1} y_{2}\)</span>，就有 <span class="math display">\[\langle k(\cdot, y), \phi(x)\rangle_{\mathcal{H}}=a x_{1}+b x_{2}+c x_{1} x_{2}\]</span></p><p>总的来说RKHS两个特性：<br />每个点的特征映射在特征空间中 <span class="math display">\[\forall x \in \mathcal{X}, \quad k(\cdot, x) \in \mathcal{H}\]</span> 再生性：<br /><span class="math display">\[\forall x \in \mathcal{X}, \forall f \in \mathcal{H},\langle f, k(\cdot, x)\rangle_{\mathcal{H}}=f(x)\]</span> <span class="math display">\[k(x, y)=\langle k(\cdot, x), k(\cdot, y)\rangle_{\mathcal{H}}\]</span></p><h2 id="kl散度一阶导与ksd">KL散度一阶导与KSD</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;
&lt;p&gt;这一工作是清华大学liu qiang老师提出的，相关论文从2016年开始也一直在更新，分别发表在NIPS、ICLR等顶会上。&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1704.07520&quot;</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="SVGD" scheme="http://example.com/tags/SVGD/"/>
    
  </entry>
  
  <entry>
    <title>Multi-Label Image Recognition with Graph Convolutional Networks [CVPR2019]</title>
    <link href="http://example.com/2021/10/26/pd5/"/>
    <id>http://example.com/2021/10/26/pd5/</id>
    <published>2021-10-26T15:25:47.000Z</published>
    <updated>2021-10-27T16:57:37.967Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1904.03582">Multi-Label Image Recognition with Graph Convolutional Networks</a><br />Code: <a href="https://github.com/chenzhaomin123/ML_GCN">link</a></p><p>针对多标签图像识别 (multi-label image recognition) 问题，旷视研究院提出一种基于图卷积网络的模型取得了良好的表现，该模型包含一个CNN的图像特征提取模块和一个图卷积网络进行标签间关系提取模块。</p><h2 id="intro">Intro</h2><p>对于多标签图像的识别问题，传统的方法往往是对每个标签进行孤立的二分类，即预测每个物体是否出现。基于概率图模型或RNN模型的方法则考虑显式的建模标签之间的依赖关系。也有方法将图像区域划分后考虑区域间的局部相关性，从而隐式的建模标签相关性。本文提出的基于GCN的端到端模型将标签的表示映射到相互独立的对象分类器上。</p><h2 id="related-work">Related Work</h2><p>最简单的多标签识别方法就是为每个标签独立训练一个二分类器，这种模型没有考虑标签之间的关系。当数据集中可能的标签数量增长时，可能的标签组合就会指数级增长（当一个数据集包含20个标签，则标签组合就有<span class="math inline">\(2^{20}\)</span>种。基于RNN、LSTM之类的模型将标签嵌入为向量，从而发掘标签间的相关性。<br />本文提出的模型将多标签构建为有向图，借助GCN在标签间的信息传播来学习图像标签间依赖、共现关系，并实现端到端训练。</p><h2 id="framework">Framework</h2><p><img src="/images/pd5/2.png" title="模型框架" /></p><h3 id="图像特征提取">图像特征提取</h3><p>论文用CNN进行图像特征提取，具体为ResNet-101的网络结构，输入图像<span class="math inline">\(I\)</span>，经过cnn和global max-pooling后得到2048维图像特征。 <span class="math display">\[\boldsymbol{x}=f_{\mathrm{GMP}}\left(f_{\mathrm{cnn}}\left(\boldsymbol{I} ; \theta_{\mathrm{cnn}}\right)\right) \in \mathbb{R}^{D}\]</span></p><h3 id="图卷积">图卷积</h3><p>卷积模块与最基本的卷积相同，如下式 <span class="math display">\[\boldsymbol{H}^{l+1}=h\left(\widehat{\boldsymbol{A}} \boldsymbol{H}^{l} \boldsymbol{W}^{l}\right)\]</span> 我们主要关注如何构图，在这一方面，本文的idea似乎有些超脱CV领域。模型针对图片数据集构建图，图中的节点为数据中的标签，并使用word embedding（pre-trained glove）对节点特征进行初始化。<br />而对于图的边，也对应图卷积中的矩阵<span class="math inline">\(\boldsymbol{A}\)</span>（文中称其为相关系数矩阵），模型使用条件概率<span class="math inline">\(P\left(L_{j} \mid L_{i}\right)\)</span>进行建模，已期获得标签相关性信息。 <img src="/images/pd5/3.png" /> 具体而言，论文统计了数据集中的标签对的共现次数，然后构建共现矩阵，并设定一个阈值来进行二值化处理，借此过滤噪声边。 <img src="/images/pd5/1.png" title="基于多标签构建有向图" /></p><p>借助模型框架图可以看到，模型中图卷积模块起的是类似辅助分类器的作用，图中每个标签节点就是该标签的一个二分类器，将基于整个数据集训练的分类器<span class="math inline">\(\boldsymbol{W} \in \mathbb{R}^{C \times D}\)</span>与图像的特征<span class="math inline">\(x \in \mathbb{R}^{D}\)</span>进行点积，得到<span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^{C}\)</span>（C表示标签的总数）。图卷积利用的信息也只有图的边，也就是标签的共现，而后借助图卷积与图像特征提取进行共同训练，得到标签之间关系的隐式表示，最终推动更准确的多标签识别。</p><h2 id="实验">实验</h2><p><img src="/images/pd5/4.png" title="实验结果—非常不错 XD" /> 不过在尝试复现该模型时，本人试验了几个数据集似乎始终无法到达论文中的结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.03582&quot;&gt;Multi-Label Image Recognition with Graph Convolutional Networks&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;https</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="CV-GNN" scheme="http://example.com/tags/CV-GNN/"/>
    
  </entry>
  
  <entry>
    <title>Multi-hop Question Generation with Graph Convolutional Network [Arxiv]</title>
    <link href="http://example.com/2021/10/24/pd6/"/>
    <id>http://example.com/2021/10/24/pd6/</id>
    <published>2021-10-24T15:56:42.000Z</published>
    <updated>2021-10-27T17:48:33.014Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2010.09240.pdf">Multi-hop Question Generation with Graph Convolutional Network</a><br />Code: <a href="https://github.com/HLTCHKUST/MulQG">link</a></p><h2 id="background">Background</h2><p>问题生成(QG)是一个从给定的上下文自动生成问题或答案的任务，而多跳问题生成 (Multi-hop Question Generation) 需要从多个不同的段落中推理生成与答案相关的问题。QG可以应用于教育系统，也可以结合QA模型作为双重任务来增强QA系统的推理能力。对于多跳问题生成，核心问题在于如何连接多个段落间的零散的信息以及答案。</p><p><img src="/images/pd6/1.png" title="多跳问题生成" /></p><h2 id="模型">模型</h2><p><img src="/images/pd6/2.png" title="模型框架" /></p><h3 id="multi-hop-encoder">Multi-hop Encoder</h3><p>对于输入的文本段落和答案，先分割成word-level的token，并分别用pre-trained Glove进行embedding，并在文本的token embedding中加入答案 embedding tag。对于得到的token embedding 输入到LSTM-RNN中学习初步的上下文相关的representation，再输入到Encoder中，模型的Encoder包括三个部分:</p><ul><li><p>Answer-aware context encoder 这一部分参考了阅读理解中的co-attention reasoning机制: <span class="math display">\[\begin{aligned}S &amp;=C_{0}^{T} A_{0} \in R^{n \times m} \\S^{\prime} &amp;=\operatorname{softmax}(S) \in R^{n \times m} \\S^{\prime \prime} &amp;=\operatorname{softmax}\left(S^{T}\right) \in R^{m \times n} \\A_{0}^{\prime} &amp;=C_{0} \cdot S^{\prime} \in R^{d \times m} \\\tilde{C}_{1} &amp;=\left[A_{0} ; A_{0}^{\prime}\right] \cdot S^{\prime \prime} \in R^{2 d \times n}\\C_{1} &amp;=\operatorname{BiLSTM}\left(\left[\tilde{C}_{1} ; C_{0}\right]\right) \in R^{d \times n}\end{aligned}\]</span> 相关性矩阵S表示答案与上下文的相关性，整个过程比较复杂，这一模块的有效性在阅读理解任务中被验证，大致操作即将答案与文本计算attention后生成新的“答案”而后同样进行一遍相关性计算，最后输入Bi-LSTM中。</p></li><li><p>GCN-based entity-aware answer encoder 将上述encoder得到的embedding输入到GCN中进行多跳信息的嵌入。 <img src="/images/pd6/3.png" title="GCN-based entity-aware answer encoder" /> 图中的节点为文本中的命名实体（由BERT自动化提取），如果实体对在同一句子中，则为它们创建边。将上面Answer-aware context encoder 的结果结合到多跳图卷积中，并最终和图的结果结合，输入到Bi-attention模型，进一步得到token的representations <span class="math display">\[A_{1}=\text { BiAttention }\left(A_{0}, E_{M}\right)\]</span></p></li><li><p>Gated encoder reasoning layer 将前面得到的结果输入到门控网络进行特征融合，进行特征保留或遗忘，得到最终的Encoder结果。</p></li></ul><h3 id="maxout-pointer-decoder">Maxout Pointer Decoder</h3><p>模型采用单向LSTM作为解码器，而Maxout pointer这一模块也并不是由作者提出的，而是参考了他人的模型，用这一模块减少生成结果中的重复项。</p><h2 id="实验">实验</h2><p>实验部分，作者分别做了与现有multi-hop QG模型对比以及消融实验，取得了SOTA结果，并且证明了框架中每个模块的意义。 <img src="/images/pd6/4.png" title="纵向对比" /> <img src="/images/pd6/5.png" title="消融实验" /></p><h2 id="总结">总结</h2><p>本文提出的框架总体来说比较复杂。往牛了说可以理解为整个框架模拟了人类的问题生成的过程，包括整体文本和答案的阅读，进行大概了解，而后对文本和答案中的实体进行关注，并寻找他们的联系，最后在生成问题时确定核心和次要信息，生成相关的问题。不过事实上整个框架就是对几个现有模型中的部分模块进行组装，类似“搭积木”的过程。而新加入的multi-hop图卷积部分整体方法也不具有很亮点的想法，从消融实验结果中也可以看出这一模块对最终结果的提升也并不是很明显。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.09240.pdf&quot;&gt;Multi-hop Question Generation with Graph Convolutional Network&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;ht</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="QA" scheme="http://example.com/tags/QA/"/>
    
  </entry>
  
</feed>
