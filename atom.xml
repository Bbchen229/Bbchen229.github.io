<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chen&#39;s Homepage</title>
  
  <subtitle>Hello AI</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-10-13T11:13:23.803Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Chen jiayuan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Vocabulary Learning via Optimal Transport for Neural Machine Translation</title>
    <link href="http://example.com/2021/10/10/pd2/"/>
    <id>http://example.com/2021/10/10/pd2/</id>
    <published>2021-10-10T10:22:13.000Z</published>
    <updated>2021-10-13T11:13:23.803Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2012.15671.pdf">Vocabulary Learning via Optimal Transport for Neural Machine Translation</a><br />ACL2021 Best paper Code: <a href="https://github.com/Jingjing-NLP/VOLT">link</a></p><p>这篇也就是被ICLR2021拒了后被评为ACL2021 best paper的文章，来自字节跳动的AI Lab。</p><h2 id="related-work">Related Work</h2><h3 id="subword-model">Subword model</h3><p>英文中传统分词方法基于空格进行tokenization。但这一方法面临OOV (Out Of Vocabulary)问题和同一单词的不同形态造成的冗余。因此如今BERT等模型多使用Subword模型，它的划分粒度介于词与字符之间。主流的(指某些中文网站上有博客介绍的）Subword model有Byte Pair Encoding (BPE), WordPiece和Unigram Language Model。</p><h3 id="byte-pair-encodingbpe">Byte-Pair-Encoding(BPE)</h3><p>&quot;Neural machine translation of rare words with subword units.&quot;arXiv preprint arXiv:1508.07909(2015).<br />BPE算法被用于处理NMT (Neural Machine Translation)任务中的OOV问题。<br />BPE是一种自下而上的压缩算法。将单词作为单词片段处理（word pieces），以便于处理未出现单词。</p><blockquote><p>we adopt BPE generated tokens as the token candidates.</p></blockquote><p>论文提出的算法要先用BPE...</p><h2 id="概要">概要</h2><p>机器翻译中，token vocabulary对最终结果会产生很大的影响。论文研究了词表的评价指标以及如何不通过训练直接找到最优的词表。文章的主要内容包括 1. 从信息论角度分析词表的作用 2.借助Optimal transport来找到最佳token词典 3. 更小的词表but更高的BLEU。</p><h2 id="intro">Intro</h2><p>词汇量（vocabulary size）会影响机器翻译任务的绩效，而通过遍历搜索来寻找最优的词汇量需要极高的计算开销，因此现有的研究大多采用统一的大小，如30k-40k。BPE通过选择频率最高的sub-words做为词典的token以进行数据压缩，以此减少熵。</p><p>语料熵随着词汇量的增加而减少，有利于模型学习。另一方面，过多的字符会导致字符稀疏化，这会损害模型学习。本文通过同时考虑熵和词汇量大小来探索自动词汇化，需要找到一个合适的目标函数来同时优化它们。其次，假设给出了适当的度量，由于指数搜索空间（<span class="math inline">\(2^N\)</span>)，解决这种离散优化问题仍然具有挑战性。</p><p>针对上述问题，论文提出VOcabulary Learning approach via optimal Transport, VOLT——最优传输的词汇学习方法</p><p>总的来说，论文的目标是1.得到“简洁而不臃肿”的词汇表 —— entropy-size trade off 2. 优化搜索过程。</p><h2 id="marginal-utility-of-vocabularization-muv">Marginal Utility of Vocabularization (MUV)</h2><p>借用经济学中的边际效应的概念，以词汇的边际效应（MUV）作为衡量标准， 然后将目标转向在可处理的时间复杂度中最大化 MUV。 <img src="/images/pd2_3.png" title="vocabulary的边际效益（没有显示给出MUV）" /></p><p>在经济学中，边际效应用于平衡收益和成本，因此论文使用 MUV 来平衡熵（收益）和词汇量（成本）。也就是从成本（大小）的增加中获得多大的收益（熵）。</p><p><img src="/images/pd2_1.png" title="MUV 与三分之二翻译任务的下游性能相关" /></p><h3 id="definition-of-muv">Definition of MUV</h3><p>MUV 表示熵对大小的负导数 <span class="math display">\[\mathcal{M}_{v(k+m)}=\frac{-\left(\mathcal{H}_{v(k+m)}-\mathcal{H}_{v(k)}\right)}{m}\]</span> 其中 <span class="math inline">\(v(k), v(k+m)\)</span> 是两个分别带有 <span class="math inline">\(k\)</span> 和 <span class="math inline">\(k+m\)</span> 个字符的词汇。<span class="math inline">\(\mathcal{H}_{v}\)</span> 表示词汇表 <span class="math inline">\(v\)</span> 语料库的樀，它由字符樀的总和定义。用字符的平均长度对熵进行归一化来避免字符长度的影响。最终的熵定义为： <span class="math display">\[\mathcal{H}_{v}=-\frac{1}{l_{v}} \sum_{j \in v} P(j) \log P(j)\]</span> <span class="math inline">\(P(i)\)</span> 是训练语料库中token <span class="math inline">\(i\)</span> 的相对频率, <span class="math inline">\(l_{v}\)</span> 是词汇表 <span class="math inline">\(v\)</span> 中token的平均长度。</p><h3 id="preliminary-results">Preliminary Results</h3><p>为了验证 MUV 作为词汇化衡量标准的有效性，作者对来自 TED 的 45 个语言对进行了实验，并计算了 MUV 和 BLEU 分数之间的Spearman相关系数(<span class="math inline">\(\rho\)</span>)。Spearman 得分为 0.4。</p><blockquote><p>We believe that it is a good signal to show MUV matters</p></blockquote><p>有了MUV作为评价指标，我们有两个选择来获得最终词表：搜索和学习。作者认为基于学习是更高效的，因此进一步探索了一种基于学习的解决方案 VOLT。（当然最终借助实验比较了 MUV-Search 和 VOLT的性能。）</p><h2 id="maximizing-muv-via-optimal-transport">Maximizing MUV via Optimal Transport</h2><h3 id="优化问题">优化问题</h3><p>首先引入一个辅助变量<span class="math inline">\(S\)</span>，<span class="math inline">\(\boldsymbol{S}=\{k, 2 \cdot k, \ldots,(t-1) \cdot k, \cdots\}\)</span>。 <span class="math inline">\(S\)</span>是一个递增序列，对于每个时间戳t，<span class="math inline">\(S[t]\)</span>代表<strong>不多于<span class="math inline">\(S[t]\)</span>个词条的词表集合</strong>。引入这一变量，根据递推关系来计算任意一个词表的MUV（借助前一个时间戳s[t-1]上的词表递进计算）</p><p><span class="math inline">\(k\)</span>代表前后两个词表<span class="math inline">\(v(t)\)</span>和<span class="math inline">\(v(t-1)\)</span>之间的大小差（size gap）。我们的目标是找到MUV最高的<span class="math inline">\(v[t]\)</span> <span class="math display">\[\begin{array}{l}\underset{t}{\arg \max } \underset{v(t-1) \in \mathbb{V}_{\boldsymbol{S}[t-1]}, v(t) \in \mathbb{V}_{S[t]}}{\arg \max } \mathcal{M}_{v(t)}= \\\underset{t}{\arg \max } \underset{v(t-1) \in \mathbb{V}_{\boldsymbol{S}[t-1]}, v(t) \in \mathbb{V}_{S[t]}}{\arg \max }-\frac{1}{k}\left[\mathcal{H}_{v(t)}-\mathcal{H}_{v(t-1)}\right]\end{array}\]</span> <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t-1]}\)</span>和 <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t]}\)</span>表示两个词表的集合，其中每个词表大小的上界为<span class="math inline">\(s[t-1]\)</span>和<span class="math inline">\(s[t]\)</span></p><blockquote><p>The inner arg max represents that the target is to find the vocabulary from <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t]}\)</span> with the maximum MUV scores. The outer arg max means that the target is to enumerate all timesteps and find the vocabulary with the maximum MUV scores.</p></blockquote><p>遍历t，遍历<span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t-1]}\)</span>。<br />（词表越大熵越小）上述公式意味着从v(t-1)这个词表，增加i个词/tokens之后，期望新得到的v(t)词表的熵降低的最多。即两个词表对应的熵的差值越大越好。</p><blockquote><p>Due to exponential search space, we propose to optimize its upper bound: <span class="math display">\[\underset{t}{\arg \max } \frac{1}{k}\left[\underset{v(t) \in \mathbb{V}_{S[t]}}{\arg \max } \mathcal{H}_{v(t)}-\underset{v(t-1) \in \mathbb{V}_{S[t-11}}{\arg \max } \mathcal{H}_{v(t-1)}\right]\]</span></p></blockquote><p>(论文ArXiv上的前一版本中写的还是lower bound...而最新版放的是upper bound...)<br />anway至此整个方法可以分成两个步骤：</p><ul><li>每个时间步t上，寻找最优的词表（按照最大化熵来寻找）</li><li>枚举每个时间步t，并输出满足上一个公式的词表（对应的就是时间步t的”最优词表“）</li></ul><p>step1的目标就是最大化： <span class="math display">\[\underset{v(t) \in \mathbb{V}_{\boldsymbol{S}[t]}}{\arg \max }-\frac{1}{l_{v(t)}} \sum_{j \in v(t)} P(j) \log P(j)\]</span> <span class="math inline">\(l_{v}\)</span>是每个token的平均字符长度，<span class="math inline">\(P(j)\)</span>是token j的概率（频率）</p><blockquote><p>However, notice that this problem is in general intractable due to the extensive vocabulary size. Therefore, we instead propose a relaxation in the formulation of discrete optimal transport, which can then be solved efficiently via the Sinkhorn algorithm</p></blockquote><p>借助最优传输OT的思想，松弛原优化问题，进而用信息论中的Sinkhorn algorithm求解。</p><h3 id="optimal-transport-不太懂">Optimal Transport （不太懂）</h3><p><img src="/images/pd2_2.png" title="寻找一个从“character分布、单字分布”到“词表词条分布”的一个最优的运输矩阵的过程" /></p><ul><li>每个transport matrix对应一个词表；</li><li>transport matrix决定有多少chars被“运输”到token候选（词条候选）；</li><li>长度为0的tokens（包含0个chars)，不会被增加到词表。</li></ul><p>不同的”运输矩阵“会带来不同的”运输开销”。而最优化运输（路径）问题的目标就是寻找一个“运输矩阵“，使得”运输开销“(即，负熵)最小化。</p><p>目标函数： <span class="math display">\[\begin{array}{c}\min _{v \in \mathbb{V}_{S[t]}} \frac{1}{l_{v}} \sum_{j \in v} P(j) \log P(j) \\\text { s.t. } \quad P(j)=\frac{\operatorname{Token}(j)}{\sum_{j \in v} \operatorname{Token}(j)}, l_{v}=\frac{\sum_{j \in v} \operatorname{len}(j)}{|v|}\end{array}\]</span></p><p>近似(obtain a tractable lower bound of entropy) - 启发式规则(最长词条匹配原则) - 变换为两部分损失</p><p>复杂的推导后得到： <span class="math display">\[\min _{\boldsymbol{P} \in \mathbb{R}^{m \times n}}\langle\boldsymbol{P}, \boldsymbol{D}\rangle-\gamma H(\boldsymbol{P})\]</span></p><p><span class="math display">\[\boldsymbol{D}(j, i)=\left\{\begin{array}{ll}-\log P(i \mid j)=+\infty, &amp; \text { if } i \notin j \\-\log P(i \mid j)=-\log \frac{1}{\operatorname{len}(j)}, &amp; \text { otherwise }\end{array}\right.\]</span></p><p><img src="/images/pd2_4.png" title="算法（不太复杂）" /></p><h2 id="实验">实验</h2><p>3个数据集上NMT任务的BLEU比较（双语语料、多语语料等）<br />VOLT对比BPE、MUV search更加高效</p><p>最后，全论文的核心应该是MUV的提出以及用OT进行优化这一trick，实验结果也比较solid，虽然最终的算法并不复杂，但是OT部分Sinkhorn算法需要较强的信息论背景，本CS专业看了半年仍是一脸懵。<br />指路一篇介绍Sinkhorn算法的链接： <a href="https://arxiv.org/pdf/1803.00567.pdf" class="uri">https://arxiv.org/pdf/1803.00567.pdf</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.15671.pdf&quot;&gt;Vocabulary Learning via Optimal Transport for Neural Machine Translation&lt;/a&gt;&lt;br /&gt;
ACL2021</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="ACL2021" scheme="http://example.com/tags/ACL2021/"/>
    
  </entry>
  
  <entry>
    <title>Encoding Word Order in Complex Embeddings</title>
    <link href="http://example.com/2021/10/09/p1-position-encoding/"/>
    <id>http://example.com/2021/10/09/p1-position-encoding/</id>
    <published>2021-10-09T13:29:54.000Z</published>
    <updated>2021-10-10T12:08:37.446Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1912.12333.pdf">Encoding Word Order in Complex Embeddings</a><br />ICLR 2020<br />Code: <a href="https://github.com/iclr-complex-order/complex-order">link</a></p><h2 id="概要">概要</h2><p>针对位置编码提出的改进，切入点新颖高效且为位置编码带来了一定的具体意义和可解释性。传统的位置嵌入捕获单个单词的位置，而不是单个单词位置之间的有序关系(例如邻接关系或优先级)。本文提出的方法建模单词的全局绝对位置和它们的顺序关系，将以前定义为独立向量的词嵌入推广到变量(位置)上的连续词函数。每个单词的表示会随着位置的增加而移动。因此，在连续函数中，不同位置的词表示可以相互关联。将这些函数的通解推广到复值域，得到了更丰富的表示。作者在文本分类、机器翻译和语言模型方面进行实验，取得了良好的表现。</p><h2 id="positional-encoding">Positional encoding</h2><p>Positional encoding 位置编码在transformer中用于存储位置信息（由于self-attention没法获取序列位置的信息），此外BERT中encoding部分也包含了位置编码。对于位置编码，本能的想法是针对序列中的每个位置必须是独一无二的，且不受序列长度的影响。常见的positional encoding的方法有:</p><ul><li>绝对（正弦）位置编码（Sinusoidal Position Encoding）</li><li>相对位置编码（Relative Position Representations）</li><li>可学习位置编码</li></ul><h3 id="正弦位置编码">正弦位置编码</h3><p>Transformer中使用的就是这种编码，实际上具体编码过程使用了正弦和余弦。具体公式为： <span class="math display">\[\begin{aligned}P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}} \right) \\P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}} \right)\end{aligned}\]</span> 其中<span class="math inline">\(d_{model}\)</span>为输入词向量的维度。如d(model)=128,那么位置3对应的位置向量为 <span class="math display">\[\left[\sin \left(3 / 10000^{0 / 128}\right), \cos \left(3 / 10000^{1 / 128}\right), \sin \left(3 / 10000^{2 / 28}\right), \cos \left(3 / 10000^{3 / 28}\right), \ldots\right]\]</span> 在具体的应用时可能前一部分用正弦后一部分用余弦。</p><h3 id="相对位置编码">相对位置编码</h3><p>Todo<br /><a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a></p><h3 id="可学习位置编码">可学习位置编码</h3><p>Todo</p><h2 id="intro">Intro</h2><p>本文的重点在于建模文本信息中额外的词的内部顺序和相邻关系，对比原本位置编码方式仅编码词的位置。模型将之前定义为独立向量的词嵌入扩展为位置自变量上的连续函数。在一个连续函数中，不同位置的词表示可以相互关联。</p><p><img src="/images/pe1.png" /></p><h2 id="methodology">Methodology</h2><p>类似于Word Embedding，位置编码（PE）定义了一个映射关系，将词的序列索引映射为一个向量。<span class="math inline">\(f_{n e}: \mathbb{N} \rightarrow \mathbb{R}^{D}\)</span>。最终某个词的embedding通常表示为为词向量和位置向量的和： <span class="math display">\[f(j, p o s)=f_{w e}(j)+f_{p e}(p o s)\]</span></p><p>论文中提出了一个位置独立问题（position independence problem），即位置编码无法捕获相邻词以及其顺序之间的潜在关系。而当后续用于特征处理的网络对这类信息不敏感时，这一问题就会限制整个模型的表达能力。相对位置编码针对这一问题进行了一定的研究，但其无法涵盖整个序列域。</p><h3 id="性质">性质</h3><p>论文指出了在位置编码中建立词序模型所必需的性质。<br />由于位置向量中每个维度的值都是根据离散的位置index得到的，这使得位置间有序关系建模变得困难，因此需要根据位置索引构建一个连续的函数（以在每个维度中表示一个特定的单词？） <span class="math display">\[f(j, \text { pos })=\boldsymbol{g}_{j}(\text { pos }) \in \mathbb{R}^{D}\]</span> <span class="math inline">\(g_j\)</span>即<span class="math inline">\(\boldsymbol{g}_{w e}(j) \in(\mathcal{F})^{D}\)</span>，词<span class="math inline">\(w_j\)</span>在pos位置可以表示为 <span class="math display">\[\left[g_{j, 1}(\operatorname{pos}), g_{j, 2}(\operatorname{pos}), \ldots, g_{j, D}(\text { pos })\right] \in \mathbb{R}^{D}\]</span> 当词<span class="math inline">\(w_j\)</span>从pos位置转到pos’位置时，只需要改变自变量的值而不需要改变映射函数<span class="math inline">\(g_j\)</span>。</p><h3 id="函数">函数</h3><p>由于实数也被囊括在复数域中，且前人有相关工作（详见论文原文Section2.2）验证了复数域所具有的更强大的表达能力，作者将模型拓展到了复数域。对于理想的映射函数，论文中提出了两条性质，即:</p><ul><li>Position-free offset transformation</li><li>Boundedness</li></ul><p>变换函数<span class="math inline">\(Transform\)</span>需满足对于任何pos，有 <span class="math display">\[g(p o s+n)=\operatorname{Transform}_{n}(g(p o s))\]</span> 满足等式的变换函数被称为witness，而满足这一条件的映射函数<span class="math inline">\(g_j\)</span>则被称为<em>linearly witnessed</em>。规定Transform <span class="math inline">\((n\)</span>, pos <span class="math inline">\()=\)</span> Transform <span class="math inline">\(_{n}(\)</span> pos <span class="math inline">\()=w(n)\)</span>。另外，映射函数<span class="math inline">\(g_j\)</span>需要有界。</p><p>而后作者证明了满足上述性质的映射函数唯一解为 <span class="math display">\[g(p o s)=z_{2} z_{1}^{p o s} \text { for } z_{1}, z_{2} \in \mathbb{C} \text { with }\left|z_{1}\right| \leq 1\]</span> 对于任意的 <span class="math inline">\(z \in \mathbb{C}\)</span>, 我们可以写成 <span class="math inline">\(z=r e^{i \theta}=r(\cos \theta+i \sin \theta)\)</span>，因此上式可写为： <span class="math display">\[g(p o s)=z_{2} z_{1}^{p o s}=r_{2} e^{i \theta_{2}}\left(r_{1} e^{i \theta_{1}}\right)^{p o s}=r_{2} r_{1}^{p o s} e^{i\left(\theta_{2}+\theta_{1} p o s\right)} \quad$ subject to $\left|r_{1}\right| \leq 1\]</span></p><p>(...跳过证明和优化过程)</p><p>最终的位置编码函数<span class="math inline">\(f(j\)</span>, pos <span class="math inline">\()\)</span>为 <img src="/images/pe2.png" /> <span class="math inline">\(j\)</span>代表单词（索引），<span class="math inline">\(pos\)</span>表示位置索引。<br />对于embedding中的每一维度，都有各自的参数，振幅r、频率p、初相<span class="math inline">\(\theta\)</span>，这些参数是trainable的。此外，周期/频率决定了单词对位置的敏感程度。当周期很短，则说明嵌入将对position高度敏感。注意，振幅、频率是与postion（自变量）无关的，与单词和维度有关。此时，word embedding可以用这些参数来表示（维度与positional embedding维度相同）。</p><h2 id="实验">实验</h2><p>作者在文本分类、机器翻译和语言模型几个任务上进行了实验，分别用Fasttext、LSTM、CNN、Transformer作为模型的backbone，而后使用不同的位置编码方法以及本文的Complex-order编码方法进行embedding，对比几个实验结果均取得了可观的提升。而计算开销（时间）上并没有显著的增加。 <img src="/images/pe3.png" title="部分实验结果" /> 实验基于tensorflow，目前没有pytorch版本，笔者将会尝试将其迁移到pytorch框架下并开源。</p><h2 id="相关工作">相关工作</h2><p>Vanilla Position Embeddings<br />Trigonometric Position Embeddings<br />Todo</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1912.12333.pdf&quot;&gt;Encoding Word Order in Complex Embeddings&lt;/a&gt;&lt;br /&gt;
ICLR 2020&lt;br /&gt;
Code: &lt;a href=&quot;https:/</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="Positional_encoding" scheme="http://example.com/tags/Positional-encoding/"/>
    
  </entry>
  
  <entry>
    <title>Improved GCN for Text Classification [GNN]</title>
    <link href="http://example.com/2021/09/29/textGCN/"/>
    <id>http://example.com/2021/09/29/textGCN/</id>
    <published>2021-09-29T14:24:53.000Z</published>
    <updated>2021-09-30T16:45:05.204Z</updated>
    
    <content type="html"><![CDATA[<center><font size = 4> <strong>TextRGNN: Residual Graph Neural Networks for Text Classification</strong></font></center>]]></content>
    
    
      
      
    <summary type="html">&lt;center&gt;
&lt;font size = 4&gt; &lt;strong&gt;TextRGNN: Residual Graph Neural Networks for Text Classification&lt;/strong&gt;&lt;/font&gt;
&lt;/center&gt;
</summary>
      
    
    
    
    <category term="Research" scheme="http://example.com/categories/Research/"/>
    
    
    <category term="TextGCN" scheme="http://example.com/tags/TextGCN/"/>
    
  </entry>
  
  <entry>
    <title>word2vec</title>
    <link href="http://example.com/2021/09/29/word2vec/"/>
    <id>http://example.com/2021/09/29/word2vec/</id>
    <published>2021-09-28T16:10:00.000Z</published>
    <updated>2021-09-28T16:10:08.351Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>sort algorithm 1</title>
    <link href="http://example.com/2021/09/20/sort-1/"/>
    <id>http://example.com/2021/09/20/sort-1/</id>
    <published>2021-09-20T15:01:57.000Z</published>
    <updated>2021-09-28T16:05:07.653Z</updated>
    
    <content type="html"><![CDATA[<h1 id="sorti">Sort(i)</h1><p>三种复杂度为<span class="math inline">\(O(n^2)\)</span>的排序算法:</p><ul><li>冒泡排序</li><li>选择排序</li><li>插入排序</li></ul><h2 id="bubble-sort">Bubble sort</h2><p>“一趟一趟来” <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span>(<span class="params">ls</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-i-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> ls[j] &gt; ls[j+<span class="number">1</span>]:</span><br><span class="line">                ls[j+<span class="number">1</span>],ls[j]=ls[j],ls[j+<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span>(ls)</span><br></pre></td></tr></table></figure></p><h2 id="select-sort">Select sort</h2><p>&quot;一个一个排&quot; <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_sort</span>(<span class="params">ls</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-<span class="number">1</span>):</span><br><span class="line">        min_loc = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-<span class="number">1</span>-i):</span><br><span class="line">            <span class="keyword">if</span> ls[i+j]&lt;ls[min_loc]:</span><br><span class="line">                min_loc =i+j</span><br><span class="line">        ls[min_loc],ls[i] = ls[i],ls[min_loc]</span><br><span class="line">    <span class="built_in">print</span>(ls)</span><br></pre></td></tr></table></figure></p><h2 id="insert-sort">Insert sort</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_select</span>(<span class="params">ls</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(ls)):</span><br><span class="line">        j = i-<span class="number">1</span></span><br><span class="line">        tmp = ls[i]</span><br><span class="line">        <span class="keyword">while</span> j&gt;=<span class="number">0</span> <span class="keyword">and</span> ls[j]&gt;tmp:</span><br><span class="line">            ls[j+<span class="number">1</span>]=ls[j]</span><br><span class="line">            j-=<span class="number">1</span></span><br><span class="line">        ls[j+<span class="number">1</span>] = tmp</span><br><span class="line">    <span class="built_in">print</span>(ls)</span><br></pre></td></tr></table></figure><h1 id="sortii">Sort(ii)</h1><p>三种复杂度为<span class="math inline">\(O(nlogn)\)</span>的排序算法:</p><ul><li>快速排序</li><li>归并排序</li><li>堆排序</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;sorti&quot;&gt;Sort(i)&lt;/h1&gt;
&lt;p&gt;三种复杂度为&lt;span class=&quot;math inline&quot;&gt;\(O(n^2)\)&lt;/span&gt;的排序算法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;冒泡排序&lt;/li&gt;
&lt;li&gt;选择排序&lt;/li&gt;
&lt;li&gt;插入排序&lt;/li&gt;
&lt;/</summary>
      
    
    
    
    <category term="Leetcode" scheme="http://example.com/categories/Leetcode/"/>
    
    
    <category term="Data Structure" scheme="http://example.com/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression (python &amp; pytorch)</title>
    <link href="http://example.com/2021/08/25/dae%E7%9A%84%E5%89%AF%E6%9C%AC/"/>
    <id>http://example.com/2021/08/25/dae%E7%9A%84%E5%89%AF%E6%9C%AC/</id>
    <published>2021-08-25T15:21:47.000Z</published>
    <updated>2021-09-28T16:09:28.897Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="ML" scheme="http://example.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>search algorithm</title>
    <link href="http://example.com/2021/08/25/test/"/>
    <id>http://example.com/2021/08/25/test/</id>
    <published>2021-08-25T14:42:55.000Z</published>
    <updated>2021-09-28T15:52:31.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="linear-search-and-binary-search">Linear Search and Binary Search</h1><h2 id="linear-search">Linear search</h2><p>Time complexity：<span class="math inline">\(O(n)\)</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_search</span>(<span class="params">ls,val</span>):</span></span><br><span class="line">    <span class="keyword">for</span> index,value <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        <span class="keyword">if</span> value == val:</span><br><span class="line">            <span class="keyword">return</span> index</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure></p><h2 id="binary-search">Binary search</h2><p>Time complexity：<span class="math inline">\(O(log n)\)</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span>(<span class="params">ls,val</span>):</span></span><br><span class="line">    left = <span class="number">0</span>  <span class="comment">#左指针</span></span><br><span class="line">    right = <span class="built_in">len</span>(ls)-<span class="number">1</span>  <span class="comment">#右指针</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = (left+right)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> ls[mid] == val:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="keyword">elif</span> ls[mid]&lt;val:</span><br><span class="line">            left = mid+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right = mid-<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;linear-search-and-binary-search&quot;&gt;Linear Search and Binary Search&lt;/h1&gt;
&lt;h2 id=&quot;linear-search&quot;&gt;Linear search&lt;/h2&gt;
&lt;p&gt;Time complexity：&lt;</summary>
      
    
    
    
    <category term="Leetcode" scheme="http://example.com/categories/Leetcode/"/>
    
    
    <category term="Data Structure" scheme="http://example.com/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>BERT</title>
    <link href="http://example.com/2021/08/16/test-my-site/"/>
    <id>http://example.com/2021/08/16/test-my-site/</id>
    <published>2021-08-16T14:58:35.000Z</published>
    <updated>2021-09-29T14:29:21.109Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pre-training-of-deep-bidirectional-transformers-for-language-understanding">Pre-training of Deep Bidirectional Transformers for Language Understanding</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pre-training-of-deep-bidirectional-transformers-for-language-understanding&quot;&gt;Pre-training of Deep Bidirectional Transformers for Lang</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Markdown语法</title>
    <link href="http://example.com/2021/08/16/hello-world/"/>
    <id>http://example.com/2021/08/16/hello-world/</id>
    <published>2021-08-16T14:25:40.470Z</published>
    <updated>2021-09-30T16:42:47.024Z</updated>
    
    <content type="html"><![CDATA[<p>由于Hexo博客的撰写需要用Markdown，虽然比Latex要简单点，但是平时用的比较少，这些杂七杂八的语法很难一下子全部记住，因此在这页博客中记录一下</p><h2 id="文字">文字</h2><h3 id="标题">标题</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 一级标题</span><br><span class="line">## 二级标题</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="居中">居中</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;center&gt;这一行需要居中&lt;/center&gt;</span><br></pre></td></tr></table></figure><h3 id="字体">字体</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">**（）** 加粗</span><br><span class="line">*（）* 斜体</span><br><span class="line">～～（）～～ 删除线</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;font face= “黑体” color=red size=7&gt;字体设置&lt;/font&gt; #size 1-7，浏览器默认3</span><br></pre></td></tr></table></figure><h2 id="引用">引用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;这是引用文字</span><br></pre></td></tr></table></figure><h2 id="分割线">分割线</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">***</span><br></pre></td></tr></table></figure><h2 id="图片">图片</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![图片alt](图片地址 &#x27;&#x27;图片title&#x27;&#x27;)</span><br></pre></td></tr></table></figure><p>图片alt就是显示在图片下面的文字，相当于对图片内容的解释。 图片title是图片的标题，当鼠标移到图片上时显示的内容。title可加可不加</p><h2 id="链接">链接</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[超链接名](超链接地址 &quot;超链接title&quot;)</span><br></pre></td></tr></table></figure><h2 id="列表">列表</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 列表内容</span><br><span class="line">+ 列表内容</span><br><span class="line">1. 列表内容</span><br></pre></td></tr></table></figure><h2 id="代码">代码</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">`代码内容`</span><br><span class="line">\``` (防止打不出加个\转义一下)</span><br><span class="line">  代码...</span><br><span class="line">  代码...</span><br><span class="line">  代码...</span><br><span class="line">\```</span><br></pre></td></tr></table></figure><h2 id="数学公式">数学公式</h2><p>公式、希腊字母、上标下标等基本语法与latex类似，可参考<a href="https://www.jianshu.com/p/a0aa94ef8ab2">markdown数学公式</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">正文中$...$</span><br><span class="line">单行显示$$...$$</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;由于Hexo博客的撰写需要用Markdown，虽然比Latex要简单点，但是平时用的比较少，这些杂七杂八的语法很难一下子全部记住，因此在这页博客中记录一下&lt;/p&gt;
&lt;h2 id=&quot;文字&quot;&gt;文字&lt;/h2&gt;
&lt;h3 id=&quot;标题&quot;&gt;标题&lt;/h3&gt;
&lt;figure class=&quot;</summary>
      
    
    
    
    <category term="杂七杂八" scheme="http://example.com/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>DAE for Action Quality Assessment(AQA)  [CV]</title>
    <link href="http://example.com/2020/09/25/dae/"/>
    <id>http://example.com/2020/09/25/dae/</id>
    <published>2020-09-25T15:21:47.000Z</published>
    <updated>2021-09-30T16:41:07.071Z</updated>
    
    <content type="html"><![CDATA[<p><font  size=5>Auto-Encoding Score Distribution Regression for Action Quality Assessment</font></p><p><em>Action quality assessment (AQA) from videos is a challenging vision task since the relation between videos and action scores is difficult to model. Thus, action quality assessment has been widely studied in the literature. Traditionally, AQA task is treated as a regression problem to learn the underlying mappings between videos and action scores. More recently, the method of uncertainty score distribution learning (USDL) made success due to the introduction of label distribution learning (LDL). But USDL does not apply to dataset with continuous labels and needs a fixed variance in training. In this paper, to address the above problems, we further develop Distribution Auto-Encoder (DAE). DAE takes both advantages of regression algorithms and label distribution learning (LDL). Specifically, it encodes videos into distributions and uses the reparameterization trick in variational auto-encoders (VAE) to sample scores, which establishes a more accurate mapping between videos and scores. Meanwhile, a combined loss is constructed to accelerate the training of DAE. DAE-MT is further proposed to deal with AQA on multi-task datasets. We evaluate our DAE approach on MTL-AQA and JIGSAWS datasets. Experimental results on public datasets demonstrate that our method achieves state-of- the-arts under the Spearman’s Rank Correlation: 0.9449 on MTL-AQA and 0.73 on JIGSAWS.</em></p><h2 id="aqa">AQA</h2><p>Action Quality Accessment (AQA) automatically scores the quality of actions by analyzing features extracted from videos and images. It’s different from conventional action recognition problem. In the past few years, much work has been devoted to different AQA tasks, such as healthcare, sports video analysis and many others.</p><h2 id="related-work">Related Work</h2><p>Parmar <em>et al.</em> [1] proposed C3D-SVR and C3D-LSTM to predict the score of the Olympic events. Additionally, incremental-label training method was introduced to train the LSTM model based on the hypothesis that the final score is an aggregation of the sequential sub-action scores.</p><p>Tang <em>et al.</em> noticed the underlying ambiguity of action scores and then proposed an improved approach: uncertainty-aware score distribution learning (USDL) [2] to address this problem. USDL is designed based on label distribution learning (LDL), a general learning paradigm to solve problems with uncertainty and answer how much each label describes the instance.</p><h2 id="methoddae">Method:DAE</h2><p><img src="/images/dae1.png" title="The pipeline of DAE architecture contains two segments: video features extraction network (orange) and label distribution encoding network (pink)." /></p><h3 id="video-feature-extraction">Video Feature Extraction</h3><p>The input video is divided into n small clips by down-sampling. Then the clips are sent into I3D ConvNets for extracting features. The final features are synthesized by three fully-connected layers.</p><h3 id="auto-encoder-for-distribution-learning">Auto-Encoder for Distribution Learning</h3><p>Compared with the regression-based method and the label distribution learning method, our approach combines the two methods’ characteristics comprehensively. The action features are encoded into score distribution, and the final result is sampled from the auto-encoders output. This architecture en- ables learning a continuous distribution without loss in training procedure and quantifies the uncertainty of action score with high accuracy.</p><p>The encoder uses a simple but quite an efficient neural network, namely multi-layered perceptrons (MLPs), to encode mean and variance simultaneously. The input 1024-dimensional feature vector x is encoded into the parameters <span class="math inline">\(μ(x)\)</span> and <span class="math inline">\(σ^2(x)\)</span> via a neural network.</p><p>We take the action score as a random variable. Treating the action score as a random variable, we need to learn its score distribution and then sample the predicted score from the obtained distribution. <span class="math display">\[p\left(y ; \mu(\boldsymbol{x}), \sigma^{2}(\boldsymbol{x})\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}(\boldsymbol{x})}} \exp \left(-\frac{(y-\mu(\boldsymbol{x}))^{2}}{2 \sigma^{2}(\boldsymbol{x})}\right)\]</span></p><p>To generate a sample from Gaussian distributed y as the predicted score and make full use of the two parameters in the score distribution at the same time, we invoke the reparameterization trick. According to reparameterization trick in VAE [3], assume that <span class="math inline">\(z\)</span> is a random variable, and <span class="math inline">\(z \sim q(z ; \phi), \phi\)</span> is its parameter. We can express <span class="math inline">\(z\)</span> as a deterministic variable, <span class="math inline">\(z=g(\epsilon ; \phi), \epsilon\)</span> is an auxiliary variable with independent marginal <span class="math inline">\(p(\epsilon)\)</span>, and <span class="math inline">\(g(\cdot ; \phi)\)</span> is a deterministic function parameterized by <span class="math inline">\(\phi\)</span>.<br /><span class="math display">\[y=\mu(\boldsymbol{x})+\epsilon * \sigma^{2}(\boldsymbol{x})\]</span></p><h2 id="experiments">Experiments</h2><p>We use Spearman’s rank correlation to measure the performance of our methods between the ground-truth and predicted score series. Spearman’s correlation is defined as: <span class="math display">\[\rho=\frac{\sum_{i}\left(p_{i}-\bar{p}\right)\left(q_{i}-\bar{q}\right)}{\sqrt{\sum_{i}\left(p_{i}-\bar{p}\right)^{2} \sum_{i}\left(q_{i}-\bar{q}\right)^{2}}}\]</span> <img src="/images/dae2.png" title="The results on JIGSAWS." /> <img src="/images/dae3.png" title="The results on MTL-AQA." /></p><p><img src="/images/dae4.png" title="Comparison of different distribution of different videos on MTL-AQA dataset." /></p><p>References (incomplete)<br />[1] What and How Well You Performed? A Multitask Learning Approach to Action Quality Assessment<br />[2] Uncertainty-aware score distribution learning for action quality assessment<br />[3] Auto-encoding variational bayes</p><p>Cooperate with zby (seu) supervisor: xyf (seu)<br /><a href="https://github.com/InfoX-SEU/DAE-AQA">Github link</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;font  size=5&gt;Auto-Encoding Score Distribution Regression for Action Quality Assessment&lt;/font&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Action quality assessment (AQA)</summary>
      
    
    
    
    <category term="Research" scheme="http://example.com/categories/Research/"/>
    
    
    <category term="AQA" scheme="http://example.com/tags/AQA/"/>
    
  </entry>
  
</feed>
