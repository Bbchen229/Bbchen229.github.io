<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chen&#39;s Homepage</title>
  
  <subtitle>Hello AI</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-12-14T13:04:34.134Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Chen jiayuan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks [ACL2020]</title>
    <link href="http://example.com/2021/12/08/pd8/"/>
    <id>http://example.com/2021/12/08/pd8/</id>
    <published>2021-12-08T05:49:22.000Z</published>
    <updated>2021-12-14T13:04:34.134Z</updated>
    
    <content type="html"><![CDATA[<p>æœ¬æ–‡ä»‹ç»çš„å†…å®¹åŒ…å«å¤šç¯‡åŸºäºå›¾ç¥ç»ç½‘ç»œè¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„è®ºæ–‡ï¼Œä»æå‡ºGCNåçš„ç®€å•åº”ç”¨åˆ°å»å¹´ACLä¸Šçš„TextINGä»¥åŠä»Šå¹´çš„BertGCNï¼ŒGNNåœ¨æ–‡æœ¬åˆ†ç±»ä¸Šå–å¾—äº†éå¸¸å¥½çš„æ•ˆæœã€‚</p><p><a href="https://arxiv.org/abs/1809.05679">Graph Convolutional Networks for Text Classification</a><br /><a href="https://arxiv.org/abs/1910.02356v2">Text Level Graph Neural Network for Text Classification</a><br /><a href="https://arxiv.org/abs/2004.13826v1">Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks</a><br /><a href="https://arxiv.org/abs/2105.05727">BertGCN: Transductive Text Classification by Combining GCN and BERT</a></p><h1 id="text-classification">Text Classification</h1><p>æ–‡æœ¬åˆ†ç±»çš„åº”ç”¨éå¸¸å¹¿æ³›ï¼ŒåŒ…æ‹¬Sentiment Analysisã€News Categorizationã€Topic Analysisç”šè‡³æ˜¯Question Answeringç­‰ã€‚è€Œautomatic text classificationæ–¹æ³•å¤§è‡´å¯ä»¥è¢«å½’ä¸ºä¸¤ç±»ï¼šåŸºäºè§„åˆ™çš„^rule-basedä»¥åŠåŸºäºæœºå™¨å­¦ä¹ çš„^æ•°æ®é©±åŠ¨çš„ã€‚</p><p>åŸºäºè§„åˆ™çš„æ–‡æœ¬åˆ†ç±»éœ€è¦å…ˆéªŒçŸ¥è¯†ï¼ŒåŒ…æ‹¬pre-defined rulesã€domain knowledgeç­‰ã€‚è€ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ–¹æ³•åˆ™æ˜¯å€ŸåŠ©æ ‡æ³¨æ•°æ®é›†ã€‚é€šå¸¸æˆ‘ä»¬å¯ä»¥æŠŠæœºå™¨å­¦ä¹ æ–‡æœ¬åˆ†ç±»çš„æ­¥éª¤å½’ä¸ºä¸¤æ­¥ï¼Œé¦–å…ˆæ˜¯æ–‡æœ¬ç‰¹å¾æå–ï¼Œè€Œåå°†ç‰¹å¾è¾“å…¥åˆ†ç±»å™¨è¿›è¡Œåˆ†ç±»ã€‚è€Œå‘å±•åˆ°ç°åœ¨ï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹å¤§è‡´ä¹Ÿéšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ä»å‰é¦ˆç¥ç»ç½‘ç»œåˆ°CNNã€RNNä»¥åŠattentionã€transformeråˆ°å¦‚ä»Špre-trained modelå¦‚BERTç­‰ã€‚</p><p>å‰é¦ˆç¥ç»ç½‘ç»œè¿›è¡Œæ–‡æœ¬åˆ†ç±»é€šå¸¸å°†æ–‡æœ¬ä½œä¸ºbag of wordsï¼›RNNåˆ™æŠŠæ–‡æœ¬ä½œä¸ºè¯çš„åºåˆ—ï¼›CNNç”¨äºè®­ç»ƒæå–æ–‡æœ¬ä¸­çš„å…³é”®çŸ­è¯­ã€è¯ç»„ç­‰è¿›è¡ŒåŒ¹é…åˆ†ç±»ï¼›attentionæœºåˆ¶èƒ½è¯†åˆ«æ–‡æœ¬ä¸­ç›¸äº’å…³è”çš„è¯ï¼Œå¯ä»¥åµŒå…¥åˆ°å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼›è‡³äºtransformerä»¥åŠBERTè¿™ç±»å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™æ˜¯â€œå¤§åŠ›å‡ºå¥‡è¿¹â€ã€‚</p><p>è€Œæœ¬æ–‡çš„é‡ç‚¹ï¼Œåˆ™æ˜¯åŸºäºGNN-å›¾ç¥ç»ç½‘ç»œçš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•ã€‚å€ŸåŠ©å›¾ç¥ç»ç½‘ç»œè¿›è¡Œæ–‡æœ¬ä¸­å¥æ³•ã€è¯­ä¹‰è§£ææ ‘ä¹‹ç±»çš„å›¾ç»“æ„ä¿¡æ¯æŒ–æ˜ï¼Œè¿›è€Œè¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚å¦å¤–ç»è¿‡ä¸‹æ–‡çš„ä»‹ç»æˆ‘ä»¬è¿˜èƒ½å‘ç°ï¼ŒåŸºäºGNNçš„æ¨¡å‹èƒ½ä¸å…¶ä»–æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œçº§è”å¹¶è¿›è¡Œè”åˆè®­ç»ƒï¼Œè¿›è€Œæœ‰æ•ˆæå‡åˆ†ç±»å‡†ç¡®ç‡ã€‚<br />åŸºäºå›¾ç¥ç»ç½‘ç»œçš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹çš„å·®å¼‚å¤§è‡´ä½“ç°åœ¨ä¸‰ä¸ªæ–¹é¢ï¼šå›¾çš„æ„å»ºã€èŠ‚ç‚¹åµŒå…¥çš„åˆå§‹åŒ–ã€å›¾ç¥ç»ç½‘ç»œã€‚</p><h1 id="textgcn">TextGCN</h1><p>Graph Convolutional Networks for Text Classification<br /><img src="/images/gcn/2.png" title="Framework of TextGCN" /> TextGCNä¸ºAAAI2018çš„è®ºæ–‡ï¼Œç°åœ¨å¾ˆå¤šäººçœ‹åˆ°è¿™ç¯‡æ–‡ç« çš„æ—¶å€™å¯èƒ½ä¼šæ„Ÿå¹â€œè¿™ä¹Ÿèƒ½å‘ï¼Ÿâ€ï¼Œä½†äº‹å®ä¸Šè¿™ç¯‡è®ºæ–‡æ˜¯æœ€å…ˆæ„å»ºäº†transductiiveçš„åŸºäºGNNè¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„æ¡†æ¶ï¼Œå¹¶å–å¾—äº†éå¸¸å¥½çš„è¡¨ç°ã€‚</p><p>æ¨¡å‹æ•´ä½“çš„æ¡†æ¶å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒåŒ…æ‹¬å›¾çš„æ„å»ºå’Œå›¾ç¥ç»ç½‘ç»œä¸¤ä¸ªæ¨¡å—ã€‚å…¶ä¸­å›¾ç¥ç»ç½‘ç»œç”±ç®€å•çš„ä¸¤å±‚å·ç§¯å±‚æ„æˆã€‚å¦ä¸€æ–¹é¢ï¼Œå›¾èŠ‚ç‚¹çš„ç‰¹å¾åˆå§‹åŒ–ç”¨one-hot vectorï¼Œè¾“å…¥çš„ä¿¡æ¯ä»…ä¸ºè¾¹ä¿¡æ¯å’ŒèŠ‚ç‚¹çš„ç»“æ„ä¿¡æ¯ï¼Œè€Œå…¶åœ¨åˆ†ç±»ç»“æœä¸Šå–å¾—çš„å‡†ç¡®ç‡ä¹Ÿåæ˜ äº†GNNåº”ç”¨æ–‡æœ¬åˆ†ç±»çš„åˆç†æ€§ã€‚ <span class="math display">\[Z=\operatorname{softmax}\left(\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W_{0}\right) W_{1}\right)\]</span></p><p>å¯¹äºæ„å›¾æ–¹é¢ï¼Œæ¨¡å‹åŸºäºæ•´ä¸ªè¯­æ–™åº“æ„å»ºä¸€ä¸ªå¼‚æ„å›¾ï¼Œå›¾ä¸­çš„èŠ‚ç‚¹åŒ…æ‹¬æ–‡æ¡£èŠ‚ç‚¹å’Œè¯èŠ‚ç‚¹ã€‚è€Œè¿™ä¸¤ç±»èŠ‚ç‚¹ä¹‹é—´çš„è¾¹ï¼Œword-wordã€doc-wordå®šä¹‰å¦‚ä¸‹ <span class="math display">\[A_{i j}=\left\{\begin{array}{ll}\operatorname{PMI}(i, j) &amp; i, j \text { are words, } \operatorname{PMI}(i, j)&gt;0 \\\operatorname{TF}-\operatorname{IDF}_{i j} &amp; i \text { is document, } j \text { is word } \\1 &amp; i=j \\0 &amp; \text { otherwise }\end{array}\right.\]</span> å…¶ä¸­è¯iä¸jä¹‹é—´çš„PMI (point-wise mutual information) è®¡ç®—ä¸º <span class="math display">\[\begin{aligned}\operatorname{PMI}(i, j) &amp;=\log \frac{p(i, j)}{p(i) p(j)} \\p(i, j) &amp;=\frac{\# W(i, j)}{\# W} \\p(i) &amp;=\frac{\# W(i)}{\# W}\end{aligned}\]</span> <span class="math inline">\(\#W(i,j)\)</span>ä»£è¡¨æ»‘çª—ä¸­ä¸¤ä¸ªè¯å…±ç°æ¬¡æ•°ã€‚å¦å¤–ï¼Œå›¾ä¸­çš„è¯èŠ‚ç‚¹ä¼šå°†è¯­æ–™åº“ç»Ÿè®¡åçš„ä½é¢‘è¯è¿‡æ»¤æ‰ã€‚è‡³äºä¸ºä»€ä¹ˆé€‰æ‹©PMIä»¥åŠTF-IDFè¿™ä¸¤ä¸ªæŒ‡æ ‡ä½œä¸ºè¾¹æƒé‡ï¼Œä½œè€…æåˆ°æ˜¯ä»å®éªŒçš„ç»“æœå‡ºå‘ä½œå‡ºçš„é€‰æ‹©ã€‚ <img src="/images/pd8/2.png" title="Results: TextGCN" /></p><h1 id="text-level-gnn">Text-level GNN</h1><p>Text Level Graph Neural Network for Text Classification [ENMLP2019]<br />TextGCNæ¨¡å‹è·Ÿå¤§éƒ¨åˆ†ç›´æ¨å¼GNNæ¨¡å‹ä¸€æ ·ï¼Œåº”ç”¨æ—¶å­˜åœ¨æ˜æ˜¾çš„ç¼ºé™·ï¼Œå³æ²¡æœ‰åŠæ³•è¿›è¡Œåœ¨çº¿æµ‹è¯•ã€‚å½“æˆ‘ä»¬è¦è¾“å…¥çš„æ–°æ–‡æœ¬è¿›è¡Œåˆ†ç±»æ—¶ï¼Œéœ€è¦å°†æ–‡æœ¬åŠ å…¥è¯­æ–™åº“åé‡æ–°æ„å»ºgraphè®­ç»ƒï¼Œè¿™å°±ä¼šå¸¦æ¥æå¤§çš„å¼€é”€ã€‚è€Œè¿™ç¯‡Text-level GNNåˆ™æ˜¯æ„å»ºåŸºäºæ–‡æœ¬çº§åˆ«çš„å›¾ï¼Œä½¿å¾—åŸºäºGNNçš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹æä¾›åœ¨çº¿æµ‹è¯•çš„åŠŸèƒ½ï¼Œè™½ç„¶æ¨¡å‹ä¾æ—§æ˜¯Transductiveã€‚ <img src="/images/pd8/3.png" title="Text-level GNN" /> Text-levelå›¾çš„æ„å»ºå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œå…¶ä¸­è¯ä¸è¯ä¹‹é—´è¿æ¥çš„è¾¹æƒé‡ä»¥åŠè¯çš„embeddingä¸ºæ•´ä¸ªè¯­æ–™åº“å…¨å±€å…±äº«ï¼Œä¿å­˜åœ¨å…¨å±€çŸ©é˜µä¸­ï¼ˆä¸Šå›¾çš„ä¸¤ä¸ªçŸ©é˜µï¼‰ï¼Œå¦å¤–æ–‡æ¡£ä¸­çš„æ¯ä¸ªè¯ä¸æ­¢å’Œç›¸é‚»çš„è¯å­˜åœ¨è¾¹ï¼Œè€Œç”±ä¸€ä¸ªè¶…å‚æ•°æ§åˆ¶å¤šè·³é‚»å±…ã€‚</p><p>åœ¨å›¾ç¥ç»ç½‘ç»œæ¨¡å—ï¼Œè™½ç„¶è®ºæ–‡ä¸­ä»‹ç»çš„æ˜¯non-spectral message passing mechanismï¼Œä½†äº‹å®ä¸Šä¸TextGCNæœ¬è´¨ä¸Šæ˜¯ä¸€æ ·çš„ï¼Œä¸è¿‡è¾¹çš„æƒé‡ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œæ›´æ–°ã€‚ <span class="math display">\[\begin{aligned}\mathbf{M}_{\mathbf{n}} &amp;=\max _{a \in \mathcal{N}_{n}^{p}} e_{a n} \mathbf{r}_{\mathbf{a}} \\\mathbf{r}_{\mathbf{n}}^{\prime} &amp;=\left(1-\eta_{n}\right) \mathbf{M}_{\mathbf{n}}+\eta_{n} \mathbf{r}_{\mathbf{n}}\end{aligned}\]</span> ä¸Šå¼ä¸­<span class="math inline">\(r\)</span>ä»£è¡¨èŠ‚ç‚¹ç‰¹å¾ï¼Œ<span class="math inline">\(\eta_{n}\)</span>ä¸ºå¯è®­ç»ƒçš„å‚æ•°ã€‚<br />æœ€åï¼Œä½¿ç”¨æ–‡æœ¬ä¸­çš„æ‰€æœ‰è¯çš„embeddingè¿›è¡Œç±»åˆ«çš„æ¨æ–­ï¼›è€Œåœ¨TextGCNä¸­åˆ™æ˜¯ç›´æ¥ä½¿ç”¨æ–‡æ¡£èŠ‚ç‚¹çš„embeddingè¿›è¡Œåˆ†ç±»ã€‚ <span class="math display">\[y_{i}=\operatorname{softmax}\left(\operatorname{Relu}\left(\mathbf{W} \sum_{n \in N_{i}} \mathbf{r}_{\mathbf{n}}^{\prime}+\mathbf{b}\right)\right)\]</span></p><p>åœ¨è¿™é‡Œç®€å•å¯¹Corpus-level GNNï¼ˆTextGCNï¼‰å’ŒText-level GNNè¿›è¡Œç®€å•çš„æ¯”è¾ƒã€‚é¦–å…ˆä¸¤è€…åœ¨ä¸‹æ¸¸ä»»åŠ¡çš„å‡†ç¡®ç‡ä¸Šæœ‰è¾ƒå°çš„å·®å¼‚ï¼Œå…¶ä¸­åè€…ç•¥èƒœä¸€ç­¹ï¼Œä¸è¿‡åè€…ä½¿ç”¨äº†Gloveè¯å‘é‡è¿›è¡Œåˆå§‹åŒ–ï¼Œæ‰€ä»¥äº‹å®ä¸Šå°†TextGCNä½¿ç”¨ä¸€äº›å°æŠ€å·§åä¸¤è€…çš„å‡†ç¡®ç‡æ˜¯éå¸¸æ¥è¿‘çš„ã€‚ä¸è¿‡Text-level GNNä¼˜è¶Šæ€§ä½“ç°åœ¨å…¶èƒ½å¤Ÿæä¾›åœ¨çº¿æµ‹è¯•ä¸Šï¼Œå½“è¾“å…¥æ–°æ–‡æ¡£è¿›è¡Œåˆ†ç±»æ—¶ï¼Œå®ƒçš„è®¡ç®—å¼€é”€ä¼šè¿œå°äºTextGCNã€‚è€Œå¯¹äºå®ƒä½¿ç”¨çš„MPMç¥ç»ç½‘ç»œè€Œä¸æ˜¯GCNï¼Œæ˜¯å› ä¸ºMPMæ›´é€‚åˆå®ƒçš„æ„å›¾æ¨¡å¼ï¼Œè€Œä¸æ˜¯MPMæ¯”å¸¸è§„çš„GCNæ›´å¼ºçš„ä¿¡æ¯æå–èƒ½åŠ›ã€‚Text-levelä½¿å¾—å…¨å±€çš„è¾¹æƒé‡å¿…é¡»æˆä¸ºå¯è®­ç»ƒçš„å‚æ•°ï¼Œè€ŒMPMä¸­çš„å¦ä¸€ä¸ªå¯è®­ç»ƒçš„å‚æ•°<span class="math inline">\(\eta_{n}\)</span>å®è´¨ä¸Šä¸GCNä¸­ç»“åˆ<span class="math inline">\(I\)</span>çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ<span class="math inline">\(L\)</span>æ˜¯ä¸€è‡´çš„ã€‚</p><h1 id="texting">TextING</h1><p>Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks<br />ä¸Šæ–‡çš„ä¸¤ä¸ªæ¨¡å‹ä»¥åŠåç»­çš„BertGCNéƒ½æ˜¯transductiveï¼Œè€Œæœ¬æ–‡çš„TextINGåˆ™æ˜¯induciveæ¨¡å‹ã€‚è®ºæ–‡å‘è¡¨åœ¨ACL2020ä¸Šï¼Œä¹Ÿå°±æ˜¯åšå®¢æ ‡é¢˜ã€‚ <img src="/images/pd8/4.png" title="TextING" /> æ¨¡å‹é’ˆå¯¹æ¯ç¯‡æ–‡æ¡£æ„å»ºä¸€ä¸ªå›¾ï¼Œä»¥è¯å…±ç°ä½œä¸ºè¾¹èŠ‚ç‚¹ï¼Œå€ŸåŠ©æ»‘çª—ï¼ˆsize 3ï¼‰æ„å»ºå›¾ã€‚èŠ‚ç‚¹åµŒå…¥ç”¨Gloveè¿›è¡Œåˆå§‹åŒ–ã€‚ æ¨¡å‹çš„å›¾ç¥ç»ç½‘ç»œæ¨¡å—ä½¿ç”¨äº†Gated Graph Neural Networks(GGNN)å’Œå›¾æ³¨æ„åŠ›(GAT)ã€‚ <span class="math display">\[\begin{aligned}\mathbf{a}^{t} &amp;=\mathbf{A h}^{t-1} \mathbf{W}_{a} \\\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z} \mathbf{a}^{t}+\mathbf{U}_{z} \mathbf{h}^{t-1}+\mathbf{b}_{z}\right) \\\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r} \mathbf{a}^{t}+\mathbf{U}_{r} \mathbf{h}^{t-1}+\mathbf{b}_{r}\right) \\\tilde{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W}_{h} \mathbf{a}^{t}+\mathbf{U}_{h}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)+\mathbf{b}_{h}\right) \\\mathbf{h}^{t} &amp;=\tilde{\mathbf{h}}^{t} \odot \mathbf{z}^{t}+\mathbf{h}^{t-1} \odot\left(1-\mathbf{z}^{t}\right)\end{aligned}\]</span></p><p>ä¸Šå¼ä¸­<span class="math inline">\(h\)</span>è¡¨ç¤ºèŠ‚ç‚¹embeddingï¼Œ<span class="math inline">\(a\)</span>ä»£è¡¨æ¥å—çš„ä¿¡æ¯ï¼Œ<span class="math inline">\(z\)</span>å’Œ<span class="math inline">\(r\)</span>åˆ†åˆ«ä»£è¡¨æ›´æ–°å’Œé—å¿˜ã€‚è€Œåå°†èŠ‚ç‚¹å€ŸåŠ©readoutæ¨¡å—è¾“å‡ºä¸ºgraph-levelçš„embeddingï¼š <span class="math display">\[\begin{array}{l}\mathbf{h}_{v}=\sigma\left(f_{1}\left(\mathbf{h}_{v}^{t}\right)\right) \odot \tanh \left(f_{2}\left(\mathbf{h}_{v}^{t}\right)\right) \\\mathbf{h}_{\mathcal{G}}=\frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \mathbf{h}_{v}+\text { Maxpooling }\left(\mathbf{h}_{1} \ldots \mathbf{h}_{\mathcal{V}}\right)\end{array}\]</span></p><p>ä¸Šæ–‡æåˆ°çš„ä¸¤ä¸ªæ¨¡å‹ä¸­GNNå¹¶æ²¡æœ‰attentionæ¨¡å—ï¼Œè¿™æ˜¯ç”±äºTextGCNçš„PMIã€TF-IDFä¿¡æ¯ä¼šæŸå¤±ï¼Œå¦ä¸€æ–¹é¢Text-level GNNå…¨å±€çš„è¾¹æƒé‡ä¹Ÿä¸åº”è¯¥å¼•å…¥æ–‡æœ¬å›¾ä¸­çš„attentionæœºåˆ¶è¿›è¡Œæ›´æ–°ã€‚</p><p>å°†ä¸Šæ–‡çš„ä¸‰ä¸ªæ¨¡å‹è¿›è¡Œç®€å•çš„å¯¹æ¯”ï¼Œå¯ä»¥éšçº¦æ„Ÿåˆ°å­˜åœ¨ä¸€ä¸ªå›¾çš„æ„å»ºä¸å›¾ç¥ç»ç½‘ç»œä¹‹é—´çš„trade-offã€‚å‰ä¸¤ä¸ªæ¨¡å‹çš„å›¾æ„å»ºè¿‡ç¨‹éƒ½åµŒå…¥äº†å¤§é‡çš„ä¿¡æ¯ï¼ˆå…ˆéªŒä¿¡æ¯ã€å…¨å±€ä¿¡æ¯ï¼‰ï¼Œè€Œä»–ä»¬çš„å›¾ç¥ç»ç½‘ç»œéƒ½éå¸¸ç®€å•ã€‚äº‹å®ä¸Šæˆ‘æ›¾åœ¨TextGCNä¸Šåšè¿‡ä¸€äº›å®éªŒï¼Œå°è¯•æŠŠGATèå…¥åˆ°ä¿¡æ¯ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œå‘ç°å‡†ç¡®ç‡ä¼šæœ‰æ˜æ˜¾ä¸‹é™ã€‚è€ŒTextINGçš„æ„å›¾è¿‡ç¨‹ä¸­çš„ä¿¡æ¯ä»…æ˜¯è¯èŠ‚ç‚¹çš„å…±ç°æ‰€åŒ…å«çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å’Œç»“æ„ä¿¡æ¯ï¼Œå› æ­¤å®ƒå¯ä»¥æ¥å—æ›´å¤æ‚çš„ä¿¡æ¯ä¼ æ’­å’Œèšåˆè¿‡ç¨‹ã€‚è¿™ä¹Ÿä½¿å¾—TextINGå¯ä»¥é¢å¯¹æ–°è¯å’Œæ–°è¾“å…¥çš„æ–‡æœ¬ç›´æ¥è¿›è¡Œåˆ†ç±»ã€‚</p><h1 id="bertgcn">BertGCN</h1><p>BertGCN: Transductive Text Classification by Combining GCN and BERT<br />BertGCNæ˜¯ç”±é¦™ä¾¬ç§‘æŠ€æå‡ºï¼Œå‘è¡¨åœ¨ACL2021ä¸Šçš„æ–‡ç« ï¼Œä¹Ÿæ˜¯ç›®å‰æ–‡æœ¬åˆ†ç±»çš„SOTAæ¨¡å‹ã€‚ä¸è¿‡è¿™ç¯‡æ–‡ç« çš„è´¡çŒ®ä¸»è¦æ˜¯å·¥ç¨‹ä¸Šçš„ã€‚å¦å¤–ï¼Œä¸å¦¨æ’åˆ—ç»„åˆä¸€ä¸‹å°†Bertä¸TextINGç»“åˆï¼Œåº”è¯¥èƒ½å–å¾—æ›´å¥½çš„ç»“æœXDï¼ˆè™½ç„¶åœ¨R8ä¸Šçš„ç»“æœå·²ç»éå¸¸æ¥è¿‘100äº†ï¼‰ã€‚<br />å›é¡¾TextGCNï¼Œæ¨¡å‹ä¸­å›¾èŠ‚ç‚¹åˆå§‹åŒ–ç”¨çš„æ˜¯one-hotå‘é‡ï¼Œè€ŒBertGCNåˆ™æ˜¯ç”¨Bertè¿›è¡Œembeddingçš„åˆå§‹åŒ–ï¼Œå¦å¤–å°†Bertä¸GNNä¸¤ä¸ªæ¨¡å—è¿›è¡Œè”åˆè®­ç»ƒï¼Œå–å¾—äº†å¾ˆå¥½çš„è¡¨ç°ã€‚</p><p>ä¸¤è€…çš„ç»“åˆå­˜åœ¨ä¸¤ä¸ªé—®é¢˜ï¼Œä¸€æ˜¯éš¾æ”¶æ•›ï¼šBERTä¸GCNå¤„ç†æ•°æ®çš„æ–¹å¼ä¸åŒã€æ¨¡å‹å¤§å°ä¸åŒï¼›äºŒæ˜¯GCNæ˜¯åœ¨æ•´ä¸ªå›¾ä¸Šè¿ç®—ï¼Œè€ŒBERTè¿‡å¤§çš„æ¨¡å‹æ— æ³•ä¸€æ¬¡å…¨éƒ¨åŠ è½½å›¾ä¸­æ‰€æœ‰ç»“ç‚¹ï¼Œè¿™å°±ç»™BertGCNçš„è®­ç»ƒå¸¦æ¥é˜»ç¢ã€‚ é’ˆå¯¹ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹ä½¿ç”¨äº†InterpolatingæŸå¤±ã€‚ <span class="math display">\[Z=\lambda Z_{\mathrm{GCN}}+(1-\lambda) Z_{\mathrm{BERT}}, Z_{\mathrm{BERT}}=\operatorname{softmax}(W X)\]</span> å½“<span class="math inline">\(\lambda=1\)</span>æ—¶ï¼ŒBERTæ¨¡å—æ²¡æœ‰æ›´æ–°ï¼›å½“<span class="math inline">\(\lambda=0\)</span>æ—¶ï¼ŒGCNæ¨¡å—æ²¡æœ‰æ›´æ–°ï¼›å½“<span class="math inline">\(\lambda \in (0,1)\)</span>æ—¶ï¼Œä¸¤ä¸ªæ¨¡å—éƒ½èƒ½å¾—åˆ°æ›´æ–°ï¼Œå¹¶ä¸”é€šè¿‡è°ƒèŠ‚<span class="math inline">\(\lambda\)</span>å®ç°BertGCNæ•´ä½“æ¨¡å—çš„å¿«é€Ÿæ”¶æ•›ã€‚<br />å¯¹äºæ— æ³•æ•´å›¾è®­ç»ƒè¿™ä¸€é—®é¢˜ï¼ŒBertGNNæå‡ºäº†ä¸€ä¸ªMemory Bankç”¨äºä¿å­˜æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾ï¼Œæ¯æ¬¡ä»ä¸­åŠ è½½batchè¿›è¡Œè®­ç»ƒå¹¶æ›´æ–°ï¼Œå…¶ä»–ä¿æŒä¸å˜ã€‚å°†æ•´ä¸ªè¯­æ–™åº“ä¸­çš„æ–‡æ¡£ç‰¹å¾åˆ†æ‰¹æ›´æ–°ï¼Œä¸ºäº†é˜²æ­¢å¼‚æ­¥æ›´æ–°å¸¦æ¥çš„ä¸ä¸€è‡´æ€§ï¼Œæ¨¡å‹åœ¨è®­ç»ƒBertæ¨¡å‹æ—¶é‡‡ç”¨äº†å°å­¦ä¹ ç‡ã€‚</p><p><img src="/images/pd8/1.png" title="Results: BertGCN" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;æœ¬æ–‡ä»‹ç»çš„å†…å®¹åŒ…å«å¤šç¯‡åŸºäºå›¾ç¥ç»ç½‘ç»œè¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„è®ºæ–‡ï¼Œä»æå‡ºGCNåçš„ç®€å•åº”ç”¨åˆ°å»å¹´ACLä¸Šçš„TextINGä»¥åŠä»Šå¹´çš„BertGCNï¼ŒGNNåœ¨æ–‡æœ¬åˆ†ç±»ä¸Šå–å¾—äº†éå¸¸å¥½çš„æ•ˆæœã€‚&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.05679&quot;</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="Text Classification" scheme="http://example.com/tags/Text-Classification/"/>
    
  </entry>
  
  <entry>
    <title>å°±å«â€œæ·±å…¥æµ…å‡ºå›¾å·ç§¯(GCN)â€å§</title>
    <link href="http://example.com/2021/11/28/gcn/"/>
    <id>http://example.com/2021/11/28/gcn/</id>
    <published>2021-11-28T10:35:25.000Z</published>
    <updated>2021-12-02T01:53:08.111Z</updated>
    
    <content type="html"><![CDATA[<p>å›¾å·ç§¯ç½‘ç»œä½œä¸ºå›¾ç¥ç»ç½‘ç»œä¹‹ä¸€ï¼Œå…·æœ‰éå¸¸å¹¿æ³›çš„åº”ç”¨ã€‚å› æ­¤ç½‘ä¸Šä¹Ÿæœ‰éå¸¸å¤šçš„å…³äºGCNçš„ä»‹ç»ï¼Œä½†æ˜¯å„ç§åšå®¢çœ‹çš„å¤šäº†æå¾—æˆ‘è„‘å£³å—¡å—¡çš„ï¼Œè€Œåˆšå¥½æœ€è¿‘çš„ä¸€ä¸ªå·¥ä½œæ¶‰åŠåˆ°GCNçš„å†…æ ¸ï¼Œå› æ­¤å€ŸåŠ©è¿™ç¯‡åšå®¢æ•´ç†å¯¹GCNè¿›è¡Œæ•´ç†ã€‚</p><p>åœ¨ä»‹ç»GCNå…ˆä»‹ç»å‡ ç¯‡æ–‡çŒ®ï¼š<br />Semi-Supervised Classification with Graph Convolutional Networks-é¦–æ¬¡æå‡ºGCNçš„è®ºæ–‡ï¼ˆICLR2017)<br />Data Analytics on Graphs-å›¾æœºå™¨å­¦ä¹ çš„æ•™æ<br /><a href="https://www.zhihu.com/question/54504471" class="uri">https://www.zhihu.com/question/54504471</a>-ä»‹ç»GCNçš„åšå®¢</p><h2 id="gnn-message-passing">GNN &amp; Message Passing</h2><p>GNNä½œä¸ºä¸€ç§graph embeddingçš„æ‰‹æ®µï¼Œå¯ä»¥å€ŸåŠ©èŠ‚ç‚¹ç‰¹å¾çš„message passingæå–å›¾ç»“æ„ä¿¡æ¯ <span class="math display">\[\begin{aligned} \mathbf{h}_{u}^{(k+1)} &amp;=\operatorname{UPDATE}^{(k)}\left(\mathbf{h}_{u}^{(k)}, \text { AGGREGATE }^{(k)}\left(\left\{\mathbf{h}_{v}^{(k)}, \forall v \in \mathcal{N}(u)\right\}\right)\right) \\ &amp;=\operatorname{UPDATE}^{(k)}\left(\mathbf{h}_{u}^{(k)}, \mathbf{m}_{\mathcal{N}(u)}^{(k)}\right) \end{aligned}\]</span></p><p><img src="/images/gcn/1.png" title="GNN Framework" /> GNNè¿›è¡Œkè½®è¿­ä»£ï¼Œæ¯è½®åŒ…æ‹¬ä¸€ä¸ªèšåˆï¼ˆaggregateï¼‰å’Œæ›´æ–°ï¼ˆupdateï¼‰æ“ä½œã€‚èšåˆæ¥è·å–é‚»èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œè€Œåæ›´æ–°èŠ‚ç‚¹çš„è‡ªèº«ç‰¹å¾ã€‚è¿™ç§å¤šè½®message passagingçš„æœºåˆ¶ä½¿å¾—å›¾çš„ç»“æ„ä¿¡æ¯ä»¥åŠé‚»èŠ‚ç‚¹ç‰¹å¾è¢«æå–åˆ°èŠ‚ç‚¹çš„ç‰¹å¾ä¸­ã€‚å¹¿ä¹‰ä¸Šæ¥è¯´ï¼ŒGCNä¹Ÿæ˜¯GNNä¸­çš„ä¸€ç§ã€‚GCNçš„message passagingéå¸¸çš„ç®€å•ã€‚ä»ä¸‹å¼ï¼ˆæœ€åŸºç¡€å½¢å¼çš„GCNï¼‰å¯ä»¥çœ‹å‡ºï¼ŒGCNå€ŸåŠ©è¾¹ä¿¡æ¯å¯¹èŠ‚ç‚¹ä¿¡æ¯è¿›è¡Œèšåˆã€‚ <span class="math display">\[f\left(H^{(l)}, A\right)=\sigma\left(A H^{(l)} W^{(l)}\right)\]</span></p><h2 id="gnn-cnn">GNN &amp; CNN</h2><p>å¯¹äºå›¾åƒæ¥è¯´ï¼Œnxnçš„å·ç§¯æ ¸å¯ä»¥ä½œä¸ºå›¾åƒä¸­çš„ç‰¹å¾æå–å™¨ï¼ˆä¸ºäº†é˜²æ­¢å›¾å’Œå›¾åƒè¿™ä¸¤ä¸ªè¯çš„å¤ªè¿‡æ¥è¿‘å¯¼è‡´å¯èƒ½å‡ºç°çš„é—®é¢˜ï¼Œä¸‹æ–‡æåˆ°å›¾æ—¶å°†ç”¨graphï¼‰ã€‚ä½†æ˜¯è¿™ç§å·ç§¯æ“ä½œæ— æ³•ç›´æ¥ç”¨åœ¨Graphä¸Šï¼Œä¸ºä»€ä¹ˆï¼Ÿ<br />ç”±äºå›¾åƒä¸graphçš„æ•°æ®ç‰¹æ€§å’Œå·ç§¯æ“ä½œçš„ç‰¹æ€§ã€‚é¦–å…ˆï¼Œå›¾åƒå…·æœ‰å±€éƒ¨å¹³ç§»ä¸å˜æ€§(local translational invariance)ï¼Œä½¿å¾—å·ç§¯æ ¸èƒ½å¤Ÿå¯¹å›¾åƒçŸ©é˜µè¿›è¡Œæ‰«æå·ç§¯ï¼›è€Œgraphä½œä¸ºéæ¬§ç©ºé—´çš„æ•°æ® (Non Euclidean Structure)ï¼Œæ¯ä¸ªèŠ‚ç‚¹é‚»æ¥ç‚¹çš„å„å¼‚å¯¼è‡´ä¼ ç»ŸCNNæ“ä½œæ— æ³•åº”ç”¨ã€‚ç¬¬äºŒï¼Œå·ç§¯æ ¸æ˜¯å‚æ•°å…±äº«çš„ï¼Œä¸”å¯ä»¥å®ç°å±‚æ¬¡åŒ–ç‰¹å¾æå–-å·ç§¯å±‚å¯ä»¥åœ¨å‰ä¸€å±‚çš„åŸºç¡€ä¸Šæå–æ›´é«˜é˜¶çš„ç‰¹å¾ï¼›è€Œgraphçš„å±‚æ•°åŠ æ·±åˆ™æ˜¯ä½¿èŠ‚ç‚¹è·å–æ›´å¹¿çš„æ„Ÿå—é‡ã€‚</p><h2 id="spectrum">Spectrum</h2><p>å‚è€ƒ<a href="https://zhuanlan.zhihu.com/p/120311352" class="uri">https://zhuanlan.zhihu.com/p/120311352</a><br />åœ¨ç©ºé—´åŸŸä¸Šçš„å›¾å·ç§¯ç¢°å£å¹¶ä¸æ„å‘³ç€åœ¨å›¾ä¸Šæ²¡æ³•è¿›è¡Œæ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥ä»é¢‘åŸŸä¸­è¿›è¡Œåˆ†æã€‚</p><h3 id="å›¾çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ">å›¾çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ</h3><p>é¦–å…ˆå®šä¹‰å‡ ä¸ªæ¦‚å¿µï¼š<br />åœ¨å›¾ä¸Šæœ€åŸºæœ¬çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µLaplacian matrixä¸ºï¼š <span class="math display">\[\mathbf{L}=\mathbf{D}-\mathbf{A}\]</span> å…¶ä¸­<span class="math inline">\(\mathbf{D}\)</span>ä¸ºåº¦çŸ©é˜µï¼Œ<span class="math inline">\(\mathbf{A}\)</span>ä¸ºé‚»æ¥çŸ©é˜µã€‚æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µæœ‰ä¸€äº›åŸºæœ¬çš„æ€§è´¨ï¼šå¯¹ç§° (<span class="math inline">\(\mathbf{L}^{T}=\mathbf{L}\)</span>)ï¼›åŠæ­£å®š (<span class="math inline">\(\mathbf{x}^{\top} \mathbf{L} \mathbf{x} \geq 0, \forall \mathbf{x} \in \mathbb{R}^{|\mathcal{V}|}\)</span>)ï¼Œè¿™ä¹Ÿæ„å‘³ç€æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å€¼éƒ½æ˜¯éè´Ÿçš„ï¼š<span class="math inline">\(0=\lambda_{|\mathcal{V}|} \leq \lambda_{|\mathcal{V}|-1} \leq \ldots \leq \lambda_{1}\)</span>ã€‚ <span class="math display">\[\begin{aligned}\mathbf{x}^{\top} \mathbf{L} \mathbf{x} &amp;=\frac{1}{2} \sum_{u \in \mathcal{V}} \sum_{v \in \mathcal{V}} \mathbf{A}[u, v](\mathbf{x}[u]-\mathbf{x}[v])^{2} \\&amp;=\sum_{(u, v) \in \mathcal{E}}(\mathbf{x}[u]-\mathbf{x}[v])^{2}\end{aligned}\]</span> å¦å¤–ï¼Œå¯¹ç§°è§„èŒƒåŒ–æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µsymmetric normalized Laplacianå®šä¹‰å¦‚ä¸‹ï¼Œè¿™æ˜¯GCNç›¸å…³å·¥ä½œä¸­æ¯”è¾ƒå¸¸ç”¨çš„ã€‚ <span class="math display">\[\mathbf{L}_{\mathrm{sym}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}}\]</span></p><h3 id="æ‹‰æ™®æ‹‰æ–¯ç®—å­">æ‹‰æ™®æ‹‰æ–¯ç®—å­</h3><p>æ¥ä¸‹æ¥æˆ‘ä»¬æ¥ä¸€æ­¥æ­¥ç†è§£ä¸ºä»€ä¹ˆè¦è¿™æ ·å®šä¹‰å›¾çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µã€‚å¯¹äºç©ºé—´ä¸­çš„ä»»æ„å‡½æ•°<span class="math inline">\(f\)</span>æ¥è¯´ï¼Œ <span class="math display">\[\Delta f=\nabla^{2} f=\nabla \cdot \nabla f =\sum_{i=1}^{n} \frac{\partial^{2} f}{\partial x_{i}^{2}}\]</span> æ‹‰æ™®æ‹‰æ–¯ç®—å­ (Laplacian)æ˜¯æ¬§å¼ç©ºé—´ä¸­çš„å‡½æ•°æ¢¯åº¦çš„æ•£åº¦ (Divergence)å¯¹åº”çš„å¾®åˆ†ç®—å­ã€‚åœ¨nç»´ç©ºé—´ä¸­è®¡ç®—çš„æ˜¯å‡½æ•°å„ä¸ªç»´åº¦äºŒé˜¶åå¯¼çš„å’Œã€‚åœ¨äºŒç»´ç©ºé—´ä¸­ï¼Œå¯ä»¥<strong>è¿‘ä¼¼</strong>ä¸ºå·®åˆ†çš„è®¡ç®— <span class="math display">\[\begin{aligned}\Delta f(x, y) &amp;=\frac{\partial^{2} f}{\partial x^{2}}+\frac{\partial^{2} f}{\partial y^{2}} \\&amp;=[f(x+1, y)+f(x-1, y))-2 f(x, y)]+[f(x, y+1)+f(x, y-1))-2 f(x, y)] \\&amp;=f(x+1, y)+f(x-1, y))+f(x, y+1)+f(x, y-1))-4 f(x, y)\end{aligned}\]</span> ä¸Šå¼äº‹å®ä¸Šå°±æ˜¯åœ¨å›¾åƒä¸Šä½œç”¨æ‹‰æ™®æ‹‰æ–¯å·ç§¯æ ¸ <span class="math display">\[\begin{array}{|r|r|r|}\hline 0 &amp; 1 &amp; 0 \\\hline 1 &amp; -4 &amp; 1 \\\hline 0 &amp; 1 &amp; 0 \\\hline\end{array}\]</span> å› æ­¤æ‹‰æ™®æ‹‰æ–¯ç®—å­å¯ä»¥ç†è§£ä¸ºâ€”â€”åœ¨æ‰€æœ‰è‡ªç”±åº¦ä¸Šè¿›è¡Œå¾®å°å˜åŒ–åæ‰€è·å¾—çš„å¢ç›Šã€‚<br />è€Œå°†å…¶æ¨å¹¿åˆ°æœ‰NèŠ‚ç‚¹çš„graphä¸Šæ—¶ï¼Œ<span class="math inline">\(f\)</span>ç»´åº¦æœ€é«˜ä¸ºNï¼Œ<span class="math inline">\(f=\left(f_{1}, \ldots, f_{N}\right)\)</span>ã€‚å…¶ä¸­<span class="math inline">\(f_{i}\)</span> è¡¨ç¤ºå‡½æ•°<span class="math inline">\(f\)</span>åœ¨ç½‘ç»œå›¾ä¸­èŠ‚ç‚¹iå¤„çš„å‡½æ•°å€¼, ç±»æ¯”<span class="math inline">\(f(x, y)\)</span>ä¸ºå‡½æ•°<span class="math inline">\(f\)</span>åœ¨ <span class="math inline">\((\mathrm{x}, \mathrm{y})\)</span>çš„å‡½æ•°å€¼ã€‚<br />å› æ­¤å½“æ‹‰æ™®æ‹‰æ–¯ç®—å­ä½œç”¨åœ¨åŠ æƒgraphï¼ˆ<strong>è¾¹æƒé‡ä¸º</strong><span class="math inline">\(w_{i j}\)</span>ï¼‰ä¸Šæ—¶ï¼Œå€ŸåŠ©å·®åˆ†è¿‘ä¼¼åæœ‰ï¼š <span class="math display">\[\begin{aligned}\Delta \boldsymbol{f}_{i} &amp;=\sum_{j \in N_{i}} \frac{\partial f_{i}}{\partial j^{2}} \\&amp; \approx \sum_{j} w_{i j}\left(f_{i}-f_{j}\right) \\&amp;=\sum_{j} w_{i j}\left(f_{i}-f_{j}\right) \\&amp;=\left(\sum_{j} w_{i j}\right) f_{i}-\sum_{j} w_{i j} f_{j} \\&amp;=d_{i} f_{i}-w_{i:} f\end{aligned}\]</span> å¯¹äºä»»æ„<span class="math inline">\(i \in N\)</span>éƒ½æˆç«‹ï¼Œæ‰€ä»¥å°±å¾—åˆ°äº†ï¼š <span class="math display">\[\begin{aligned}\Delta f=\left(\begin{array}{c}\Delta f_{1} \\\vdots \\\Delta f_{N}\end{array}\right) &amp;=\left(\begin{array}{cc}d_{1} f_{1}-w_{1:} f \\\vdots \\d_{N} f_{N}-w_{N:} f\end{array}\right) \\&amp;=\left(\begin{array}{ccc}d_{1} &amp; \cdots &amp; 0 \\\vdots &amp; \ddots &amp; \vdots \\0 &amp; \cdots &amp; d_{N}\end{array}\right) f-\left(\begin{array}{c}w_{1:} \\\vdots \\w_{N:}\end{array}\right) f \\&amp;=\operatorname{diag}\left(d_{i}\right) f-\mathbf{W} f \\&amp;=(\mathbf{D}-\mathbf{W}) f \\&amp;=\mathbf{L} f\end{aligned}\]</span> è¿™å°±æ„å‘³ç€ï¼Œå¯¹ç”±å›¾èŠ‚ç‚¹ç‰¹å¾æ„æˆçš„å‘é‡<span class="math inline">\(f\)</span>åšæ‹‰æ™®æ‹‰æ–¯ç­‰ä»·äºå›¾æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µä¸å‘é‡<span class="math inline">\(f\)</span>è¿›è¡Œç‚¹ç§¯ã€‚</p><h3 id="graph-fourier-transformer">Graph Fourier Transformer</h3><p>æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾åˆ†è§£ <span class="math display">\[\mathbf{L} \mathbf{u}_{\mathbf{k}}=\lambda_{k} \mathbf{u}_{\mathbf{k}}\]</span> ç»§è€Œè¿›è¡Œæ­£äº¤ç›¸ä¼¼å¯¹è§’åŒ–åå°±å¾—åˆ° <span class="math display">\[\mathbf{L}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{-1}=\mathbf{U}\left(\begin{array}{ccc}\lambda_{1} &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \\&amp; &amp; \lambda_{n}\end{array}\right) \mathbf{U^{-1}}=\mathbf{U}\boldsymbol{\Lambda} \mathbf{U}^{T}\]</span> å…¶ä¸­<span class="math inline">\(\boldsymbol{\Lambda}\)</span> ä¸ºç‰¹å¾å€¼æ„æˆå¯¹è§’çŸ©é˜µ, <span class="math inline">\(\mathbf{U}\)</span> ä¸ºç‰¹å¾å‘é‡æ„æˆçš„æ­£äº¤çŸ©é˜µã€‚</p><p>åœ¨è¿™é‡Œè¡¥å……ä¸€æ¡æ€§è´¨ <span class="math display">\[\Delta e^{-i \omega t} =\frac{\partial^{2} e^{-i \omega t}}{\partial t^{2}}= -\omega^{2} e^{-i \omega t}\]</span> ä»å¹¿ä¹‰ä¸Šæ¥çœ‹ï¼Œè¿™ç¬¦åˆç‰¹å¾æ–¹ç¨‹<span class="math inline">\(AV=\lambda V\)</span>çš„å®šä¹‰ï¼Œä¹Ÿå°±æ˜¯è¯´<span class="math inline">\(e^{-i \omega t}\)</span>æ˜¯æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å‡½æ•°</p><p>æŠŠä¼ ç»Ÿçš„å‚…é‡Œå¶å˜æ¢ä»¥åŠå·ç§¯è¿ç§»åˆ°Graphä¸Šæ¥, æ ¸å¿ƒå·¥ä½œå…¶å®å°±æ˜¯æŠŠ<strong>æ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å‡½æ•°<span class="math inline">\(e^{-i \omega t}\)</span></strong> å˜ä¸ºGraphå¯¹åº”çš„<strong>æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡</strong>ã€‚ å‚…ç«‹å¶å˜åŒ–æ˜¯ä¿¡å·å‡½æ•°<span class="math inline">\(f(t)\)</span>ä¸åŸºå‡½æ•°<span class="math inline">\(e^{-i \omega t}\)</span>çš„å†…ç§¯ <span class="math display">\[\mathcal{F}_{T}(\omega)=\int_{-\infty}^{+\infty} f(t) e^{-i \omega t} d t\]</span> å› æ­¤å¯¹äºGraphæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰ <span class="math display">\[F\left(\lambda_{l}\right)=\hat{f}\left(\lambda_{l}\right)=\sum_{i=1}^{N} f(i) u_{l}^{*}(i)\]</span> å…¶ä¸­<span class="math inline">\(f\)</span>æ˜¯graphä¸Šçš„Nç»´å‘é‡ï¼Œ<span class="math inline">\(f(i)\)</span>å¯¹åº”äºgraphä¸Šçš„ç¬¬iä¸ªé¡¶ç‚¹ï¼Œ<span class="math inline">\(u_{l}(i)\)</span>è¡¨ç¤ºç¬¬lä¸ªç‰¹å¾å‘é‡çš„ç¬¬iä¸ªåˆ†é‡ã€‚ç‰¹å¾å€¼<span class="math inline">\(\lambda_l\)</span>ï¼ˆé¢‘ç‡ï¼‰ä¸‹çš„<span class="math inline">\(f\)</span>çš„graphå‚…ç«‹å¶å˜æ¢å°±æ˜¯ä¸<span class="math inline">\(\lambda_l\)</span>å¯¹åº”çš„ç‰¹å¾å‘é‡<span class="math inline">\(u_{l}\)</span>è¿›è¡Œå†…ç§¯è¿ç®—ã€‚å°†ä¸Šå¼æ¨å¹¿åˆ°çŸ©é˜µå½¢å¼ï¼Œå°±æœ‰ <span class="math display">\[\left(\begin{array}{c}\hat{f}\left(\lambda_{1}\right) \\\hat{f}\left(\lambda_{2}\right) \\\vdots \\\hat{f}\left(\lambda_{N}\right)\end{array}\right)=\left(\begin{array}{cccc}u_{1}(1) &amp; u_{1}(2) &amp; \ldots &amp; u_{1}(N) \\u_{2}(1) &amp; u_{2}(2) &amp; \ldots &amp; u_{2}(N) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\u_{N}(1) &amp; u_{N}(2) &amp; \ldots &amp; u_{N}(N)\end{array}\right)\left(\begin{array}{c}f(1) \\f(2) \\\vdots \\f(N)\end{array}\right)\]</span> <span class="math display">\[\hat{f} = U^T f\]</span></p><p>å›¾ä¸Šçš„å‚…ç«‹å¶é€†å˜æ¢ç±»ä¼¼äºä¼ ç»Ÿå‚…ç«‹å¶é€†å˜æ¢çš„å¯¹é¢‘ç‡æ±‚ç§¯åˆ†ï¼š <span class="math display">\[f(i)=\sum_{l=1}^{N} \hat{f}\left(\lambda_{l}\right) u_{l}(i)\]</span> <span class="math display">\[f=U \hat{f}\]</span></p><h2 id="gcn">GCN</h2><p>ä¸Šæ–‡ä»ç™¾è‰å›­è®²åˆ°ä¸‰å‘³ä¹¦å±‹ï¼Œç»ˆäºè¦è®²åˆ°äº†æœ¬æ–‡çš„ä¸»è§’-GCNã€‚åœ¨ä¸Šé¢graphå‚…ç«‹å¶å˜æ¢çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†å·ç§¯æ¨å¹¿åˆ°graphä¸Šã€‚ <span class="math display">\[f * h=\mathcal{F}^{-1}[\hat{f}(\omega) \hat{h}(\omega)]=\frac{1}{2 \Pi} \int \hat{f}(\omega) \hat{h}(\omega) e^{i \omega t} d \omega\]</span> ç±»æ¯”åˆ°graphä¸Šï¼Œå‡½æ•°<span class="math inline">\(f\)</span>ä¸å·ç§¯æ ¸<span class="math inline">\(h\)</span>åœ¨graphä¸Šçš„å·ç§¯ <span class="math display">\[(f * h)_{G}=U\left(\begin{array}{lll}\hat{h}\left(\lambda_{1}\right) &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \hat{h}\left(\lambda_{n}\right)\end{array}\right) U^{T} f\]</span> å¼ä¸­<span class="math inline">\(\hat{h}\left(\lambda_{l}\right)=\sum_{i=1}^{N} h(i) u_{l}^{*}(i)\)</span>æ˜¯æ ¹æ®éœ€è¦è®¾è®¡çš„å·ç§¯æ ¸<span class="math inline">\(h\)</span>åœ¨graphä¸Šçš„å‚…ç«‹å¶å˜æ¢ã€‚ å›¾è¡¨ç¤ºå­¦ä¹ ä¸­ç”¨çš„å®šä¹‰ä¸º <span class="math display">\[(f * h)_{G}=U\left(\left(U^{T} h\right) \odot\left(U^{T} f\right)\right)\]</span> <span class="math inline">\(\odot\)</span> è¡¨ç¤ºHadamard productï¼Œå¯¹ä¸¤ä¸ªç»´åº¦ç›¸åŒçš„å‘é‡ã€çŸ©é˜µè¿›è¡Œå¯¹åº”ä½ç½®çš„é€å…ƒç´ ä¹˜ç§¯ã€‚</p><p>å°†ç¥ç»ç½‘ç»œä¸graphå·ç§¯ç»“åˆï¼Œåªéœ€ä»¤å·ç§¯æ ¸å˜ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œä¹Ÿå°±å¾—åˆ° <span class="math display">\[y_{\text {output }}=\sigma\left(U \left(\begin{array}{lll}\theta_{1} &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \theta_{n}\end{array}\right) U^{T} x\right)\]</span> æˆ‘ä»¬å°†å·ç§¯æ ¸è®°ä¸º<span class="math inline">\(g(\Lambda)\)</span> (<span class="math inline">\(\Lambda\)</span>å°±æ˜¯å¤§å†™çš„<span class="math inline">\(\lambda\)</span>)ã€‚<br />è¿™ç§å›¾å·ç§¯è¢«ç§°ä¸º<a href="https://arxiv.org/abs/1312.6203">ç¬¬ä¸€ä»£å›¾å·ç§¯</a>ï¼Œä½†è¿™ç±»å›¾å·ç§¯è®¡ç®—å¼€é”€éå¸¸å¤§ï¼Œä¸”å·ç§¯æ ¸æœ‰nä¸ªå‚æ•°ï¼Œè¿™ç§å›¾å·ç§¯å¾ˆéš¾å¤„ç†å·¥ä¸šçº§çš„æ•°æ®ã€‚ <a href="https://arxiv.org/pdf/1606.09375.pdf">ç¬¬äºŒä»£å›¾å·ç§¯</a>ä½¿ç”¨äº†polynomial filter <span class="math display">\[g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} \Lambda^{k}=\left(\begin{array}{ccc}\sum_{j=0}^{K} \alpha_{j} \lambda_{1}^{j} &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \\&amp; &amp; &amp; \sum_{j=0}^{K} \alpha_{j} \lambda_{n}^{j}\end{array}\right)\]</span> å…¶ä¸­<span class="math inline">\(\theta \in \mathbb{R}^{K}\)</span>æ˜¯å¤šé¡¹å¼ç³»æ•°å‘é‡ã€‚è¿›è€Œå¯ä»¥æ¨å‡º <span class="math display">\[U \sum_{j=0}^{K} \alpha_{j} \Lambda^{j} U^{T}=\sum_{j=0}^{K} \alpha_{j} U \Lambda^{j} U^{T}=\sum_{j=0}^{K} \alpha_{j} L^{j}\]</span> <span class="math display">\[y_{\text {output }}=\sigma\left(\sum_{j=0}^{K-1} \alpha_{j} L^{j} x\right)\]</span> æ­¤æ—¶ï¼Œæˆ‘ä»¬è®¡ç®—å›¾å·ç§¯è¿ç®—å°±ä¸éœ€è¦å†ä¹˜ä¸Šç‰¹å¾å‘é‡çŸ©é˜µ<span class="math inline">\(U\)</span>ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ<span class="math inline">\(L\)</span>çš„k æ¬¡æ–¹ï¼ˆKè¿œå°äºnï¼‰ï¼Œè¿™æ ·å°±é¿å…äº†è¿›è¡Œç‰¹å¾åˆ†è§£ã€‚è€Œæˆ‘ä»¬å¯ä»¥äº‹å…ˆè®¡ç®—å¥½<span class="math inline">\(L^K\)</span> ï¼Œè¿™æ ·å°±åªéœ€è¦è®¡ç®—çŸ©é˜µç›¸ä¹˜ã€‚ æ­¤å¤–ï¼Œé«˜é˜¶æ‹‰æ™®æ‹‰æ–¯å¯ä»¥ç”¨åˆ‡æ¯”é›ªå¤«å±•å¼€æ¥è¿‘ä¼¼ï¼Œå› æ­¤å·ç§¯æ ¸è¿˜å¯ä»¥ç”¨åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼æ¥è¡¨ç¤º <span class="math display">\[g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} T_{k}(\tilde{\Lambda})\]</span> è€Œå°†åˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼çš„é˜¶æ•°é™åˆ¶ä¸º2çš„æ—¶å€™å°±å¾—åˆ°åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å¸¸ç”¨çš„å›¾å·ç§¯å…¬å¼ï¼š <span class="math display">\[H^{(l+1)}=\sigma\left(\widetilde{D}^{-\frac{1}{2}} \widetilde{A} \widetilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)\]</span> ä¸Šå¼ä¸­ï¼Œ<span class="math inline">\(\widetilde{A}=A+I, \mathrm{~A}\)</span>ä¸ºé‚»æ¥çŸ©é˜µ, <span class="math inline">\(I\)</span>ä¸ºå•ä½çŸ©é˜µ, æ‰€ä»¥<span class="math inline">\(\widetilde{A}\)</span> ä¸ºæ·»åŠ è‡ªè¿æ¥çš„é‚»æ¥çŸ©é˜µ;<span class="math inline">\(W^{(l)}\)</span> ä¸ºç¥ç»ç½‘ç»œç¬¬<span class="math inline">\(l\)</span>å±‚çš„æƒé‡çŸ©é˜µ; <span class="math inline">\(\sigma(\cdot)\)</span>æ˜¯æ¿€æ´»å‡½æ•°</p><h2 id="gcn-self-attention">GCN &amp; Self-attention</h2><p>ä¸Šé¢ä»é¢‘åŸŸåˆ†ægraph convolutionï¼Œå½“æˆ‘ä»¬ä»ç©ºé—´åŸŸä¸Šåˆ†æå¾—åˆ°çš„å·ç§¯å…¬å¼æ—¶ï¼Œå¯ä»¥çœ‹å‡ºå®ƒä»æ˜¯ä¸€ç§message passingæœºåˆ¶ã€‚ <span class="math display">\[A  H^{(l)}  W^{(l)}\]</span> ä¸Šå¼å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼Œé¦–å…ˆæ˜¯<span class="math inline">\(H\)</span>ä¸å‚æ•°çŸ©é˜µ<span class="math inline">\(W\)</span>åšä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼Œè€Œåä¸é‚»èŠ‚ç‚¹åŠå…¶è¾¹ä¿¡æ¯è¿›è¡Œèšåˆæ±‡æ€»ã€‚è¿™ä¸€æ¡†æ¶åœ¨æŸç§æ„ä¹‰ä¸Šè¯´ä¸self- attentionæ˜¯éå¸¸ç±»ä¼¼çš„ã€‚</p><p>self- attentionåŒ…å«queryã€keyã€valueï¼›å…¶ä¸­è¾“å…¥çš„queryä¸æ¯ä¸ªkeyè®¡ç®—ç›¸ä¼¼åº¦ï¼Œè€Œåå¾—åˆ°ä¸€ä¸ªæ³¨æ„åŠ›ç³»æ•°<span class="math inline">\(\alpha\)</span>ï¼Œå†ç”±æ³¨æ„åŠ›ç³»æ•°å¯¹valueè¿›è¡ŒåŠ æƒæ±‚å’Œè¾“å‡ºæœ€ç»ˆçš„ç»“æœã€‚æŠ›å¼€é‚»èŠ‚ç‚¹åï¼Œè¿™ä¸¤è€…çš„è®¡ç®—æœºåˆ¶å¯ä»¥è¯´éå¸¸ç±»ä¼¼ï¼Œéƒ½å¯ä»¥è¢«å›Šæ‹¬åœ¨message passingè¿™ä¸€æ¡†æ¶ä¸‹ï¼Œè€Œself-attentionä¹Ÿå¯ä»¥ç†è§£ä¸ºåœ¨å®Œæˆå›¾ï¼ˆæ‰€æœ‰èŠ‚ç‚¹éƒ½ç›¸è¿ï¼‰ä¸Šçš„GCNã€‚è€ŒTransformerä»¥åŠGNNåœ¨NLPä¸­çš„å¹¿æ³›åº”ç”¨ä¹Ÿä»æŸç§æ„ä¹‰ä¸Šè¯´æ˜äº†ä¸¤è€…ä¹‹é—´å­˜åœ¨æŸç§ç›¸ä¼¼æ€§ã€‚</p><h2 id="gcnåº”ç”¨">GCNåº”ç”¨</h2><h3 id="textgcn">TextGCN</h3><p><img src="/images/gcn/2.png" title="TextGCN" /> è®ºæ–‡<a href="https://arxiv.org/abs/1809.05679v3">Graph Convolutional Networks for Text Classification</a>æ‰€æ„å»ºçš„TextGCNå°†GCNç”¨äºæ–‡æœ¬åˆ†ç±»ä¸­ï¼Œåœ¨ç”µå½±è¯„ä»·ã€æ–°é—»ç­‰æ•°æ®é›†ä¸Šéƒ½å–å¾—äº†ä¸é”™çš„è¡¨ç°ã€‚<br />å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæ¨¡å‹å°†è¯­æ–™åº“ä¸­çš„æ¯ä¸€ç¯‡æ–‡æ¡£å’Œè¯­æ–™åº“ä¸­è¯ä½œä¸ºèŠ‚ç‚¹ï¼Œè”åˆæ„å»ºäº†ä¸€ä¸ªå¼‚æ„å›¾ï¼Œå†å€ŸåŠ©GCNè¿›è¡Œç‰¹å¾ä¼ æ’­ï¼Œå¾—åˆ°æ¯ä¸ªæ–‡æ¡£èŠ‚ç‚¹çš„embeddingåè¿›è¡Œsoftmaxåˆ†ç±»ã€‚<br />æ–‡ç« ä¸­èŠ‚ç‚¹éƒ½ä½¿ç”¨one-hot vectorè¿›è¡Œåˆå§‹åŒ–ï¼Œè€Œæ–‡æ¡£-è¯è¾¹ã€è¯-è¯è¾¹åˆ†åˆ«ç”¨TF-IDFã€PMIèµ‹ä»¥ä¸åŒçš„æƒé‡ï¼Œè€Œæœ€ç»ˆå¾—åˆ°çš„åˆ†ç±»å‡†ç¡®ç‡æ¯”ä¼ ç»Ÿçš„CNNã€LSTMç­‰ç½‘ç»œæ•ˆæœè¦é«˜ï¼Œè¶³ä»¥è¯æ˜GCNåœ¨NLPä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚æ­¤å¤–ï¼Œåç»­ç”¨BERTè¿›è¡ŒèŠ‚ç‚¹åˆå§‹åŒ–çš„BERTGCNä¹Ÿæ˜¯ç›®å‰çš„æ–‡æœ¬åˆ†ç±»çš„SOTAæ¨¡å‹ã€‚</p><h3 id="st-gcn">ST-GCN</h3><p><img src="/images/gcn/4.png" title="ST-GCN" /> <a href="https://arxiv.org/pdf/1801.07455.pdf">ST-GCN</a>å¯ä»¥è¯´æ˜¯GCNåœ¨éª¨éª¼è¡Œä¸ºè¯†åˆ«é‡Œé¢çš„å¼€å±±ä¹‹ä½œã€‚</p><h3 id="sgc">SGC</h3><p><img src="/images/gcn/3.png" title="Simplifying Graph Convolutional Networks (SGC)" /> ä½œè€…å°†å›¾å·ç§¯å±‚ä¸­çš„æ¿€æ´»å‡½æ•°å»æ‰ï¼Œå¾—åˆ°äº†SGCåœ¨è®¸å¤šNLPä»»åŠ¡ä¸Šæ›´ä¼˜çš„ç»“æœï¼Œä¸”æ¨¡å‹é€Ÿåº¦æœ‰äº†æå¤§çš„æå‡ã€‚</p><h2 id="å†å›é¦–">å†å›é¦–</h2><p>å›é¡¾ä¸€ä¸‹ä¸Šæ–‡çš„å†…å®¹ï¼Œé¦–å…ˆGCNæ˜¯GNNçš„ä¸€ç§ï¼Œä»å…¬å¼ä¸Šçœ‹èšåˆå‡½æ•°é‡‡ç”¨çš„æ˜¯å›¾çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µã€‚å½“æˆ‘ä»¬ä»é¢‘åŸŸä¸Šåˆ†æå›¾çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µåŠå…¶ç‰¹å¾åˆ†è§£ä¹‹åå¯ä»¥å‘ç°ï¼Œæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡å¯ä»¥ä½œä¸ºå‚…é‡Œå¶å˜æ¢çš„åŸºã€ç‰¹å¾å€¼è¡¨ç¤ºé¢‘ç‡ï¼Œä»è€Œå°±å¯ä»¥å®šä¹‰å›¾ä¸Šçš„å‚…ç«‹å¶å˜æ¢ï¼Œè¿›è€Œæ‰©å±•åˆ°å·ç§¯æ“ä½œã€‚è€Œå°†å›¾å·ç§¯ä¸ç¥ç»ç½‘ç»œç»“åˆåï¼Œå€ŸåŠ©å¤šé¡¹å¼ä¼˜åŒ–åå°±å¾—åˆ°äº†ç°åœ¨å¸¸ç”¨çš„å·ç§¯å…¬å¼ã€‚è€Œä»ç©ºé—´åŸŸä¸­çœ‹ï¼Œå›¾å·ç§¯æœ¬è´¨ä¸Šä¹Ÿå°±æ˜¯ä¸€ç§ä¿¡æ¯ä¼ æ’­æœºåˆ¶ï¼Œå€ŸåŠ©è¾¹çš„æƒé‡ä¿¡æ¯å¯¹é‚»èŠ‚ç‚¹çš„ç‰¹å¾åšé™åˆ¶åä¼ æ’­ã€èšåˆã€æ›´æ–°èŠ‚ç‚¹åŸæœ¬çš„ç‰¹å¾ã€‚</p><p>åœ¨é¢‘åŸŸåˆ†æè¿‡ç¨‹ä¸­æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼Œåœ¨ç”±Graphç¡®å®šçš„<span class="math inline">\(n\)</span>ç»´ç©ºé—´ä¸­ï¼Œè¶Šå°çš„ç‰¹å¾å€¼ <span class="math inline">\(\lambda_{l}\)</span> è¡¨æ˜ï¼šæ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ <span class="math inline">\(L\)</span> å…¶æ‰€å¯¹åº”çš„åŸº <span class="math inline">\(u_{l}\)</span> ä¸Šçš„åˆ†é‡ã€&quot;ä¿¡æ¯&quot;è¶Šå°‘ã€é«˜é¢‘éƒ¨åˆ†ã€‚æ‰€ä»¥å›¾å·ç§¯æœ‰æ—¶å€™ä¹Ÿè¢«è®¤çŸ¥ä¸ºæ˜¯å›¾ä¸Šçš„é«˜æ–¯å¹³æ»‘ï¼Œä¸€ç§æ»¤æ³¢çš„è¿‡ç¨‹ï¼Œè¿™ä¹Ÿå¯¼å‡ºäº†å›¾å·ç§¯ä¸­çš„ä¸€å¤§é—®é¢˜ï¼šover-smooshingã€‚å½“å›¾å·ç§¯å±‚æ•°åŠ æ·±æ—¶ï¼Œå›¾ä¸ŠèŠ‚ç‚¹è‡ªèº«çš„ç‰¹å¾ä¼šå› ä¸ºä¸æ–­çš„ä¼ æ’­åå¯¼è‡´è‡ªèº«çš„ç‰¹å¾æ¶ˆå¤±ï¼Œæ‰€æœ‰èŠ‚ç‚¹çš„ç‰¹å¾ä¼šè¶Šæ¥è¶Šæ¥è¿‘ï¼Œè¿›è€Œä½¿å¾—ä¸‹æ¸¸ä»»åŠ¡å‡†ç¡®ç‡ä¸‹é™ã€‚</p><h3 id="gcn-vs-cnn">GCN vs CNN</h3><p>GCNå¯ä»¥é€€åŒ–ä¸ºCNN...TODO</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;å›¾å·ç§¯ç½‘ç»œä½œä¸ºå›¾ç¥ç»ç½‘ç»œä¹‹ä¸€ï¼Œå…·æœ‰éå¸¸å¹¿æ³›çš„åº”ç”¨ã€‚å› æ­¤ç½‘ä¸Šä¹Ÿæœ‰éå¸¸å¤šçš„å…³äºGCNçš„ä»‹ç»ï¼Œä½†æ˜¯å„ç§åšå®¢çœ‹çš„å¤šäº†æå¾—æˆ‘è„‘å£³å—¡å—¡çš„ï¼Œè€Œåˆšå¥½æœ€è¿‘çš„ä¸€ä¸ªå·¥ä½œæ¶‰åŠåˆ°GCNçš„å†…æ ¸ï¼Œå› æ­¤å€ŸåŠ©è¿™ç¯‡åšå®¢æ•´ç†å¯¹GCNè¿›è¡Œæ•´ç†ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ä»‹ç»GCNå…ˆä»‹ç»å‡ ç¯‡æ–‡çŒ®ï¼š&lt;br /&gt;
Semi-S</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Two are Better than One â€” Joint Entity and Relation Extraction with Table-Sequence Encoders [EMNLP2020]</title>
    <link href="http://example.com/2021/11/23/pd7/"/>
    <id>http://example.com/2021/11/23/pd7/</id>
    <published>2021-11-23T14:09:00.000Z</published>
    <updated>2021-12-08T05:52:50.695Z</updated>
    
    <content type="html"><![CDATA[<h1 id="å‰è¨€">å‰è¨€</h1><p>æœ€è¿‘ä¼¼ä¹åƒåˆ°äº†ä¸€ä¸ªç“œ</p><blockquote><p>å¦‚ä½•çœ‹å¾…EMNLP'21çš„æ–‡ç« æ¶‰å«ŒæŠ„è¢­EMNLP'20ä¸Šçš„æ–‡ç« ?<br />æœ¬äººåœ¨é˜…è¯»EMNLP2021çš„æ–‡ç« æ—¶ï¼Œå¶ç„¶å‘ç°ä¸€ç¯‡åä¸ºSeeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Modelçš„æ–‡ç« ï¼Œè¿™ç¯‡æ–‡ç« çš„å†…å®¹ä¸EMNLP2020ä¸Šçš„ä¸€ç¯‡æ–‡ç« Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encodersé«˜åº¦ç›¸ä¼¼ã€‚</p></blockquote><p>å…ˆä¸è°ˆåˆ°åº•æœ‰æ²¡æœ‰æŠ„è¢­äº‹ä»¶ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€çœ‹20å¹´å’Œ21è¿™ä¸¤ç¯‡æ–‡ç« çš„æ‰€åšçš„å·¥ä½œ</p><h1 id="emnlp20">EMNLP'20</h1><h2 id="intro">Intro</h2><h1 id="emnlp21">EMNLP'21</h1><h1 id="p.s.">p.s.</h1><p>æœ€è¿‘å°è¯•å¤ç°æ—·è§†çš„<a href="/2021/10/26/pd5/" title="ML-GCN">ML-GCN</a>ä¹Ÿé‡åˆ°äº†å¤§æ— è¯­äº‹ä»¶ï¼Œå…¶ä¸€æ˜¯ç”¨äº†å¥½å‡ ä¸ªç‰ˆæœ¬çš„Pytorchéƒ½æ— æ³•è¾ƒå¥½çš„æ”¶æ•›ï¼Œçœ‹ç½‘å‹çš„ä¸€äº›è®¨è®ºè¯´å±…ç„¶è¦ç”¨19å¹´ç‰¹å®šçš„0.3.1ç‰ˆæœ¬ï¼›å…¶äºŒå°±æ˜¯ML-GCNä¸­æœ€ä¸»è¦çš„æ¨¡å—GCNå¯¹æ¨¡å‹çš„å‡†ç¡®ç‡æ²¡æœ‰ä»»ä½•è´¡çŒ®ï¼Œå»æ‰è¯¥æ¨¡å—åè€Œæœ‰æ›´å¥½çš„æ•ˆæœã€‚ğŸ˜…<br />æ€ªä¸å¾—ä¼šæœ‰äººç”¨â€œé­”æ”¹â€è¿™ç§ç•¥å¸¦ä¾®è¾±æ€§çš„è¯æ¥å½¢å®¹AIçš„ç§‘ç ”ã€‚å¸Œæœ›èƒ½æœ‰è¶Šæ¥è¶Šå¤šçš„äººåšæœ‰æ„ä¹‰çš„ç§‘ç ”ï¼Œä¹Ÿå¸Œæœ›â€œç‹¬è§’å…½â€ä»¬å’Œâ€œmasterâ€ä»¬æŠŠæŒ£é’±å’Œç§‘ç ”åˆ†å¼€ã€‚â€œæŒ£é’±å˜›ï¼Œä¸å¯’ç¢œâ€œ ï¼ˆè®©å­å¼¹é£èµ¶ç´§ç”³é—ï¼‰ï¼Œä½†æ—¢ç„¶æƒ³æŒ£é’±å°±åˆ«å¾€ç§‘ç ”é‡Œé’»ï¼Œæ•´äº›æ— æ„ä¹‰çŒæ°´ç”šè‡³æ˜¯å‰½çªƒä»–äººçš„æˆæœã€‚<br />å½“ç„¶ï¼ŒæŠ›å¼€ç¯å¢ƒç°çŠ¶ä¸è°ˆæ¥æ‰¹åˆ¤æŸäº›ä¸ªä½“çš„è¡Œä¸ºä¼¼ä¹æœ‰äº›è€æµæ°“ã€‚è¿˜è®°å¾—å¤§äºŒæ—¶å€™å¯¼å¸ˆè·Ÿæˆ‘å’Œzbè¯´è¦åšä¸€ä¸ªä¸€èº«æ­£æ°”çš„ç§‘ç ”å·¥ä½œè€…ã€‚ï¼ˆè™½ç„¶ç°åœ¨ä¹Ÿè¯´ä¸å‡†ä»¥åæ˜¯è¿›å­¦æœ¯ç•Œè¿˜æ˜¯å·¥ä¸šç•Œï¼‰å›½å®¶å¿«é€Ÿå‘å±•æ€»ä¼šç•™ä¸‹ä¸€äº›å¼Šç—…ï¼Œå¸Œæœ›èƒ½ä»é«˜æ ¡è€å¸ˆå’Œå­¦ç”Ÿå¼€å§‹ï¼Œé€æ¸æ„å»ºä¸€ç‰‡å‡€åœŸï¼Œè®©çƒ­çˆ±å­¦æœ¯çš„äººèƒ½å¤Ÿåšå®šä¸ç§»çš„èµ°ä¸‹å»ã€‚è‡³å°‘ä»¥åå­¦æœ¯é€ å‡ç­‰äº‹ä»¶æ›å…‰æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„ä¸»è§’ä¸åº”è¯¥æ˜¯ä¸­å›½äººçš„åå­—ã€‚</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;å‰è¨€&quot;&gt;å‰è¨€&lt;/h1&gt;
&lt;p&gt;æœ€è¿‘ä¼¼ä¹åƒåˆ°äº†ä¸€ä¸ªç“œ&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;å¦‚ä½•çœ‹å¾…EMNLP&#39;21çš„æ–‡ç« æ¶‰å«ŒæŠ„è¢­EMNLP&#39;20ä¸Šçš„æ–‡ç« ?&lt;br /&gt;
æœ¬äººåœ¨é˜…è¯»EMNLP2021çš„æ–‡ç« æ—¶ï¼Œå¶ç„¶å‘ç°ä¸€ç¯‡åä¸ºSeeking Common but D</summary>
      
    
    
    
    
    <category term="NER" scheme="http://example.com/tags/NER/"/>
    
  </entry>
  
  <entry>
    <title>SVGDè¡¥å……</title>
    <link href="http://example.com/2021/11/18/svgd-2/"/>
    <id>http://example.com/2021/11/18/svgd-2/</id>
    <published>2021-11-18T02:03:58.000Z</published>
    <updated>2021-11-18T05:41:21.352Z</updated>
    
    <content type="html"><![CDATA[<p>å…³äº<a href="/2021/11/09/svgd/" title="SVGD">SVGD</a>çš„å†…å®¹ï¼Œè¿™ä¸€ç¯‡åšå®¢ä¸­å¤§è‡´åšäº†ç®€å•çš„ä»‹ç»ï¼ŒåŒ…æ‹¬ä»ä¸Šåˆ°ä¸‹çš„æ•°å­¦æ¨å¯¼å’Œç®€å•çš„é€»è¾‘é“¾ã€‚ä½†æ•´ä¸ªè´å¶æ–¯æ¨æ–­èƒŒåè¿˜æœ‰è®¸å¤šæ€æƒ³ï¼Œä¸”SVGDèƒŒåä¹Ÿéšè—çš„ä¸€äº›æ€ç»´è¿‡ç¨‹ã€‚è¿™ç¯‡Blogå°†ä½œä¸ºSVGDç›¸å…³å†…å®¹çš„è¡¥å……ä»¥åŠæ¶‰åŠåˆ°çš„çŸ¥è¯†ä½“ç³»çš„ç®€è¦å½’çº³ã€‚</p><h1 id="svgdå®éªŒ">SVGDå®éªŒ</h1><h2 id="æ‹Ÿåˆé«˜æ–¯åˆ†å¸ƒ">æ‹Ÿåˆé«˜æ–¯åˆ†å¸ƒ</h2><h2 id="è´å¶æ–¯ç¥ç»ç½‘ç»œ">è´å¶æ–¯ç¥ç»ç½‘ç»œ</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;å…³äº&lt;a href=&quot;/2021/11/09/svgd/&quot; title=&quot;SVGD&quot;&gt;SVGD&lt;/a&gt;çš„å†…å®¹ï¼Œè¿™ä¸€ç¯‡åšå®¢ä¸­å¤§è‡´åšäº†ç®€å•çš„ä»‹ç»ï¼ŒåŒ…æ‹¬ä»ä¸Šåˆ°ä¸‹çš„æ•°å­¦æ¨å¯¼å’Œç®€å•çš„é€»è¾‘é“¾ã€‚ä½†æ•´ä¸ªè´å¶æ–¯æ¨æ–­èƒŒåè¿˜æœ‰è®¸å¤šæ€æƒ³ï¼Œä¸”SVGDèƒŒåä¹Ÿéšè—çš„ä¸€äº›æ€ç»´è¿‡ç¨‹ã€‚è¿™ç¯‡Blogå°†ä½œä¸ºSVG</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="SVGD" scheme="http://example.com/tags/SVGD/"/>
    
  </entry>
  
  <entry>
    <title>Kernels and Hilbert Space</title>
    <link href="http://example.com/2021/11/16/kernel/"/>
    <id>http://example.com/2021/11/16/kernel/</id>
    <published>2021-11-16T11:43:16.000Z</published>
    <updated>2021-11-17T12:21:34.962Z</updated>
    
    <content type="html"><![CDATA[<p>Referenceï¼š<br />â€˜Introduction to Hilbert Spaces with Application.â€™<br />â€˜Introduction to RKHS, and some simple kernel algorithms.â€™</p><p>Since Kernel trick is one of the core methods in SVM and SVGD also involves expertise related to RKHS. I looked up several books on Kernel method, trying to get a systematic understanding of Kernel and Hilbert space. This blog can also be regarded as a summary and summary of the book â€˜Introduction to Hilbert Spaces with Application â€™.</p><h1 id="introduction">Introduction</h1><p><img src="/images/kernel/1.png" title="XOR example" /> <img src="/images/kernel/2.png" title="Document classification example" /></p><h2 id="kernel">Kernel</h2><p>Definition: Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}\]</span></p><h1 id="normed-vector-spaces">Normed Vector Spaces</h1><p>First, the space defined in mathematics can be divided from simple to complex as:</p><ul><li><p>Vector Space<br />a nonempty set <span class="math inline">\(E\)</span> with two operations: <em>addition</em> and <em>multiplication by scalars</em>.<br />e.g. <span class="math inline">\(\mathbb{R}^{N}\)</span> <span class="math inline">\(\mathbb{C}^{N}\)</span></p></li><li><p>Normed Space<br />norm is an abstract generalization of the length of a vector:<br />function <span class="math inline">\(x \mapsto\|x\|\)</span> from a vector space <span class="math inline">\(E\)</span> into <span class="math inline">\(\mathbb{R}\)</span></p></li><li><p>Banach Space: complete normed space<br />A normed space is complete if and only if every absolutely convergent series converges. (The contents of Cauchy sequence and Cauchy series are put in the appendix)<br />Actually, Banach space introduces the concept of Limits</p></li><li><p>Inner Product Spaces<br />The space that defines the <a href="#jump">inner product</a>.</p></li><li><p>Hilbert Spaces: A complete inner product space</p></li></ul><h1 id="hilbert-spaces">Hilbert Spaces</h1><h1 id="appendix">Appendix</h1><h2 id="cauchy-sequence-and-cauchy-series">Cauchy sequence and Cauchy series</h2><p>Definition of <strong><em>Cauchy sequence</em></strong>. A sequence <span class="math inline">\(\left\{f_{n}\right\}_{n=1}^{\infty}\)</span> of elements in a normed space <span class="math inline">\(\mathcal{H}\)</span> is said to be a Cauchy sequence if for every <span class="math inline">\(\epsilon&gt;0\)</span>, there exists <span class="math inline">\(N=N(\varepsilon) \in \mathbb{N}\)</span>, such that for all <span class="math inline">\(n, m \geq N,\left\|f_{n}-f_{m}\right\|_{\mathcal{H}}&lt;\epsilon\)</span></p><h2 id="inner-product">Inner product</h2><p><span id="jump"> </span> Definition of <strong><em>Inner product</em></strong>. Let <span class="math inline">\(\mathcal{H}\)</span> be a vector space over <span class="math inline">\(\mathbb{R}\)</span>. A function <span class="math inline">\(\langle\cdot, \cdot\rangle_{\mathcal{H}}: \mathcal{H} \times \mathcal{H} \rightarrow \mathbb{R}\)</span> is said to be an inner product on <span class="math inline">\(\mathcal{H}\)</span> if:</p><ul><li><p><span class="math inline">\(\left\langle\alpha_{1} f_{1}+\alpha_{2} f_{2}, g\right\rangle_{\mathcal{H}}=\alpha_{1}\left\langle f_{1}, g\right\rangle_{\mathcal{H}}+\alpha_{2}\left\langle f_{2}, g\right\rangle_{\mathcal{H}}\)</span></p></li><li><p><span class="math inline">\(\langle f, g\rangle_{\mathcal{H}}=\langle g, f\rangle_{\mathcal{H}}{ }^{1}\)</span></p></li><li><p><span class="math inline">\(\langle f, f\rangle_{\mathcal{H}} \geq 0\)</span> and <span class="math inline">\(\langle f, f\rangle_{\mathcal{H}}=0\)</span> if and only if <span class="math inline">\(f=0\)</span>.</p></li></ul><p>the inner product between matrices <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(B \in\)</span> <span class="math inline">\(\mathbb{R}^{m \times n}\)</span> is <span class="math display">\[\langle A, B\rangle=\operatorname{trace}\left(A^{\top} B\right)\]</span></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Referenceï¼š&lt;br /&gt;
â€˜Introduction to Hilbert Spaces with Application.â€™&lt;br /&gt;
â€˜Introduction to RKHS, and some simple kernel algorithms.â€™&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="Kernels" scheme="http://example.com/tags/Kernels/"/>
    
  </entry>
  
  <entry>
    <title>Stein variational gradient descent (NIPS2018)</title>
    <link href="http://example.com/2021/11/09/svgd/"/>
    <id>http://example.com/2021/11/09/svgd/</id>
    <published>2021-11-09T12:28:58.000Z</published>
    <updated>2021-11-18T05:48:02.952Z</updated>
    
    <content type="html"><![CDATA[<h1 id="intro">Intro</h1><p>è¿™ä¸€å·¥ä½œæ˜¯æ¸…åå¤§å­¦liu qiangè€å¸ˆæå‡ºçš„ï¼Œç›¸å…³è®ºæ–‡ä»2016å¹´å¼€å§‹ä¹Ÿä¸€ç›´åœ¨æ›´æ–°ï¼Œåˆ†åˆ«å‘è¡¨åœ¨NIPSã€ICLRç­‰é¡¶ä¼šä¸Šã€‚<br /><a href="https://arxiv.org/abs/1704.07520">Stein Variational Gradient Descent as Gradient Flow</a><br /><a href="https://arxiv.org/abs/1810.11693">Stein Variational Gradient Descent as Moment Matching</a><br /><a href="https://arxiv.org/pdf/1608.04471.pdf">Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a></p><p>ä»å®šä¹‰ä¸Šæ¥è¯´ï¼ŒSVGDæ˜¯ä¸€ç§ç¡®å®šæ€§çš„é‡‡æ ·ç®—æ³•ï¼Œç”¨ä¸€ç»„ç²’å­æ¥è¿‘ä¼¼ç»™å®šçš„åˆ†å¸ƒã€‚åŸºäºè¿™ä¸¤ç‚¹ï¼Œå®ƒå’ŒMCMCä»¥åŠVIéƒ½æœ‰å…±é€šä¹‹å¤„ã€‚ä½†SVGDå³ä¿è¯äº†åœ¨å¤§é‡æ•°æ®ä¸‹çš„è®¡ç®—é€Ÿåº¦ï¼Œä¹Ÿæ¯”å˜åˆ†æ¨æ–­å…·æœ‰æ›´é«˜çš„å‡†ç¡®æ€§ã€‚</p><p>ä»æ•´ä½“ä¸Šæ¥çœ‹ï¼Œè¿™ä¸€å·¥ä½œé€šè¿‡å¼•å…¥Stein discrepancyæ¥åº¦é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œå†å€ŸåŠ©RKHSä½¿å…¶å®¹æ˜“è®¡ç®—ï¼Œæœ€åå€ŸåŠ©gradient descentè¿›è¡Œä¼˜åŒ–ã€‚å› æ­¤ä¸‹æ–‡ä¹Ÿå°±ä»è¿™ä¸‰éƒ¨åˆ†ä¸€ä¸€ä»‹ç»ã€‚</p><h1 id="background">Background</h1><h2 id="steins-method">Stein's method</h2><p>é¦–å…ˆæˆ‘ä»¬éœ€è¦å¼•å…¥å‡ ä¸ªå®šä¹‰ï¼š</p><ul><li><p><em>Stein score function</em> <span class="math display">\[\boldsymbol{s}_{p}=\nabla_{x} \log p(x)=\frac{\nabla_{x} p(x)}{p(x)}\]</span> è¿™ä¸€å‡½æ•°è¢«ç§°ä¸º<span class="math inline">\(q(x)\)</span>çš„Stein score function</p></li><li><p><em>Stein class</em><br />å½“å‡½æ•°<span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}\)</span>æ»¡è¶³ä¸‹å¼æ—¶åˆ™ç§°å…¶åœ¨stein classä¸­ <span class="math display">\[\int_{x \in \mathcal{X}} \nabla_{x}(f(x) p(x)) d x=0\]</span> å…¶ä¸­<span class="math inline">\(\mathcal{X}\)</span> æ˜¯<span class="math inline">\(\mathbb{R}^{d}\)</span>ä¸‹çš„å­é›†ï¼Œè€Œ<span class="math inline">\(p(x)\)</span>åˆ™æ˜¯åœ¨<span class="math inline">\(\mathcal{X}\)</span> ä¸‹è¿ç»­å¯å¾®çš„åˆ†å¸ƒã€‚</p></li><li><p><em>Stein's operator</em>ï¼šä½œç”¨åœ¨<span class="math inline">\(p\)</span>ä¸Šçš„çº¿æ€§æ“ä½œ <span class="math display">\[\mathcal{A}_{p} f(x)=\boldsymbol{s}_{p}(x) f(x)+\nabla_{x} f(x)\]</span> å…¶ä¸­<span class="math inline">\(s_{p}\)</span>å’Œ<span class="math inline">\(\mathcal{A}_{p} f\)</span> éƒ½æ˜¯<span class="math inline">\(d \times 1\)</span> å‡½æ•°(mapping from <span class="math inline">\(\mathcal{X}\)</span> to <span class="math inline">\(\mathbb{R}^{d}.\)</span>)</p></li></ul><p>æœ‰äº†ä»¥ä¸Šä¸‰ä¸ªå®šä¹‰åï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•å¾—åˆ°stein discrepancyã€‚é¦–å…ˆä½œä¸ºä¸€ä¸ªåº¦é‡æ‰‹æ®µï¼Œå¿…ç„¶éœ€è¦æ»¡è¶³ä¸€äº›æ¡ä»¶ã€‚<br />å½“ä¸”ä»…å½“ <span class="math display">\[\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]=0   \qquad   (1) \]</span> <span class="math inline">\(p(x)\)</span> å’Œ <span class="math inline">\(q(x)\)</span>æ˜¯ç›¸ç­‰çš„ã€‚è€Œå½“ä¸¤ä¸ªåˆ†å¸ƒ<span class="math inline">\(p=q\)</span>æ—¶åˆè¢«ç§°ä¸ºstein identityã€‚<br />å€ŸåŠ© (1) å¼ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰Stein discrepancyæ¥åº¦é‡ä¸¤ä¸ªåˆ†å¸ƒ<span class="math inline">\(p\)</span>å’Œ<span class="math inline">\(q\)</span>ä¹‹é—´çš„å·®å¼‚ï¼š <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]\right)^{2}\]</span> å€ŸåŠ©ä¹‹å‰å®šä¹‰çš„stein operatorï¼Œä¹Ÿå¯ä»¥æŠŠä¸Šå¼å†™ä¸º <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\mathcal{A}_{q} f(x)\right]\right)^{2}\]</span></p><p><span class="math inline">\(\mathcal{F}\)</span>æ˜¯ä¸€ç³»åˆ—è¿ç»­å¯å¾®çš„ä¸”æ»¡è¶³<span class="math inline">\(\mathbb{S}(p, q)\)</span>ä¸ä¸º0(<span class="math inline">\(p \neq q\)</span>æ—¶)å‡½æ•°é›†åˆã€‚å½“<span class="math inline">\(p \neq q\)</span>æ—¶ï¼Œ<span class="math inline">\(\mathbb{S}(p,q)&gt;0\)</span>ï¼Œè€Œ<span class="math inline">\(max\)</span>åˆ™æ˜¯å› ä¸ºæˆ‘ä»¬å¸Œæœ›è·ç¦»å°½å¯èƒ½æ˜æ˜¾ã€‚</p><p><span class="math inline">\(\mathbb{S}(p, q)\)</span>å¹¶æ²¡æœ‰è¢«å¹¿æ³›åº”ç”¨åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå› ä¸ºå…¶è®¡ç®—å’Œä¼˜åŒ–çš„å¤æ‚æ€§: <span class="math inline">\(q(x)=f(x) / Z\)</span> è€Œ<span class="math inline">\(Z=\int f(x) d x\)</span>çš„è®¡ç®—å¾€å¾€è®¾è®¡é«˜ç»´ç§¯åˆ†ã€‚<br />ä½†æ˜¯è®ºæ–‡æå‡ºäº†å°†å‡½æ•°<span class="math inline">\(\mathcal{F}\)</span>ç”¨æ ¸å‡½æ•°ä»£æ›¿æ—¶ï¼Œä¼šå¾—åˆ°æ˜“äºè®¡ç®—çš„Stein discrepancy <span class="math inline">\(\mathbb{S}(p, q)\)</span>ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ä»¤<span class="math inline">\(\mathcal{F}\)</span>æ¥æºäºå¸Œå°”ä¼¯ç‰¹å†ç”Ÿæ ¸ç©ºé—´çš„ä¸€ä¸ªçƒ (reproducing kernel Hilbert space (RKHS))ã€‚</p><h2 id="kernelized-stein-discrepancy">Kernelized Stein Discrepancy</h2><p>å¯¹äºæ˜ å°„åçš„å‡½æ•°ï¼Œå¯¹åº”çš„æ­£å®šæ ¸<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>ï¼Œæˆ‘ä»¬æœ‰ <span class="math display">\[\mathbb{S}(p, q)=\mathbb{E}_{x, x^{\prime} \sim p}\left[u_{q}\left(x, x^{\prime}\right)\right]\]</span> å…¶ä¸­<span class="math inline">\(x, x^{\prime}\)</span>æ˜¯<span class="math inline">\(p\)</span>ä¸­ç‹¬ç«‹åŒåˆ†å¸ƒçš„ä¸¤ä¸ªå˜é‡ï¼Œå‡½æ•°<span class="math inline">\(u_{q}\left(x, x^{\prime}\right)\)</span>ç”±<span class="math inline">\(q\)</span>ç¡®å®šï¼Œå¦‚æœå±•å¼€çš„è¯å®é™…ä¸Šæ˜¯ï¼š <span class="math display">\[u_{q}\left(x, x^{\prime}\right)= \boldsymbol{s}_{q}(x)^{\top} k\left(x, x^{\prime}\right) \boldsymbol{s}_{q}\left(x^{\prime}\right)+\boldsymbol{s}_{q}(x)^{\top} \nabla_{x^{\prime}} k\left(x, x^{\prime}\right)+\nabla_{x} k\left(x, x^{\prime}\right)^{\top} \boldsymbol{s}_{q}\left(x^{\prime}\right)+\operatorname{trace}\left(\nabla_{x, x^{\prime}} k\left(x, x^{\prime}\right)\right)\]</span></p><p>å½“æˆ‘ä»¬ä»æœªçŸ¥åˆ†å¸ƒ<span class="math inline">\(p(x)\)</span>é‡‡æ ·å‡ºä¸€ä¸ªæ ·æœ¬<span class="math inline">\({x_i}\)</span>æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œè¿‘ä¼¼è®¡ç®— <span class="math display">\[\hat{\mathbb{S}}(p, q)=\frac{1}{n(n-1)} \sum_{i \neq j} u_{q}\left(x_{i}, x_{j}\right)\]</span></p><p>æ¥ä¸‹æ¥æˆ‘ä»¬è¯¦ç»†ä»‹ç»ä¸Šè¿°çš„è¿‡ç¨‹ã€‚</p><h3 id="kernels-and-reproducing-kernel-hilbert-spaces">Kernels and Reproducing Kernel Hilbert Spaces</h3><a href="/2021/11/16/kernel/" title="Kernel and Hilbert Spacesä»‹ç» (æœªå®Œå¾…ç»­)">Kernel and Hilbert Spacesä»‹ç» (æœªå®Œå¾…ç»­)</a><p>ä»¤<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>ä¸ºä¸€ä¸ªæ­£å®šæ ¸ï¼Œæ ¹æ®Mercerâ€™s theoremæˆ‘ä»¬å¯¹å…¶è¿›è¡Œè°±åˆ†è§£ï¼š <span class="math display">\[k\left(x, x^{\prime}\right)=\sum_{j} \lambda_{j} e_{j}(x) e_{j}\left(x^{\prime}\right)\]</span> å…¶ä¸­<span class="math inline">\(\left\{e_{j}\right\},\left\{\lambda_{j}\right\}\)</span>åˆ†åˆ«æ˜¯æ­£äº¤ç‰¹å¾å‡½æ•°å’Œæ­£ç‰¹å¾å€¼ï¼Œæ»¡è¶³<span class="math inline">\(\int e_{i}(x) e_{j}(x) d x=\mathbb{I}[i=j]\)</span>, for <span class="math inline">\(\forall i, j\)</span></p><p>å¯¹äºä¸€ä¸ªæ­£å®šæ ¸ï¼Œå®ƒå¯ä»¥åˆ†è§£ä¸ºRKHSä¸­ç‰¹å¾å‡½æ•°çš„çº¿æ€§ç»„åˆï¼ˆç©ºé—´ä¸­ä»»ä½•ä¸€ä¸ªå‡½æ•°å¯ä»¥ç”¨è¿™ç»„åŸºçš„çº¿æ€§ç»„åˆæ¥è¡¨ç¤ºï¼‰ã€‚ç”±ä¸€ä¸ªç‰¹å®šçš„æ ¸å‡½æ•°èƒ½äº§ç”Ÿä¸€ä¸ªå”¯ä¸€çš„Hilbertç©ºé—´ï¼Œæœ‰æ€§è´¨ <span class="math display">\[f(x)=\langle f, k(\cdot, x)\rangle_{\mathcal{H}}, \quad k\left(x, x^{\prime}\right)=\left\langle k(\cdot, x), k\left(\cdot, x^{\prime}\right)\right\rangle_{\mathcal{H}}\]</span> å½“æˆ‘ä»¬å®šä¹‰ <span class="math inline">\(\mathcal{H}^{d}=\mathcal{H} \times \mathcal{H} \times \cdots \mathcal{H}\)</span> ä¸º <span class="math inline">\(d\)</span> ç»´å‘é‡å‡½æ•° <span class="math inline">\(\mathbf{f}=\left\{f_{i}: f_{i} \in \mathcal{H} \quad i=1, \cdots, d\right\}\)</span> ç»„æˆçš„ Hilbertç©ºé—´, <span class="math inline">\(\mathcal{H}^{d}\)</span> ä¸Šçš„å†…ç§¯å®šä¹‰ä¸º <span class="math inline">\(&lt;\mathbf{f}, \mathbf{g}&gt;_{\mathcal{H}^{d}}=\sum_{i=1}^{d}&lt;f_{i}, g_{i}&gt;_{\mathcal{H}}\)</span> ã€‚å¦‚æœè§‰å¾—ä¸Šè¿°çš„ä»‹ç»å¤ªè¿‡æŠ½è±¡ï¼Œå¯ä»¥çœ‹é™„å½•éƒ¨åˆ†å…³äºRKHSçš„ä¸€ä¸ª<a href="#jump">toy example</a></p><h3 id="lemmas">lemmas</h3><p><strong>Stein's Identity</strong> ï¼š <span class="math display">\[\mathbb{E}_{p}\left[\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)\right]=0\]</span> è¯æ˜çš„è¯æ ¹æ®<span class="math inline">\(\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)=\nabla_{x}(\boldsymbol{f}(x) p(x)) / p(x)\)</span>å’Œåˆ†å¸ƒç§¯åˆ†æ³•åˆ™å°±å¯ä»¥æ¨å¯¼å‡ºæ¥ã€‚</p><p>æœ‰äº†ä¸Šé¢çš„å¼•ç†ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ° <span class="math display">\[\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)-\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\left(\boldsymbol{s}_{q}(x)-\boldsymbol{s}_{p}(x)\right) \boldsymbol{f}(x)^{\top}\right]\]</span></p><p>ä¹Ÿå°±æ˜¯è¯´<span class="math inline">\(\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]\)</span>æ˜¯ä¸€ä¸ªç”±<span class="math inline">\(f(x)\)</span>åŠ æƒçš„æœŸæœ›ï¼Œå¯¹äº<span class="math inline">\(\left(s_{q}(x)-s_{p}(x)\right)\)</span>çš„æœŸæœ›ã€‚</p><h3 id="ksd">KSD</h3><p>å€ŸåŠ©ä¸Šé¢çš„æ¨å¯¼ä»¥åŠRKHSçš„æ€§è´¨ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰å‡ºæ ¸ç©ºé—´ä¸‹çš„stein discrepancyï¼š <span class="math display">\[S(p, q)=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right)\right]\]</span> ï¼ˆè¿™é‡Œæˆ‘ä»¬å°†Kernelizedå‰åçš„stein discrepancyåˆ†åˆ«å†™ä¸º<span class="math inline">\(\mathbb{S}(p,q)\)</span>å’Œ<span class="math inline">\(S(p, q)\)</span>ã€‚å¦å¤–éœ€è¦æ³¨æ„åç»­éƒ¨åˆ†æ¨å¯¼æ˜¯é’ˆå¯¹stein discrepancyä¸­çš„æœŸæœ›é¡¹ï¼‰<br />æˆ‘ä»¬è¦ç”¨ä¸€ä¸ªå¯é‡‡æ ·çš„åˆ†å¸ƒ<span class="math inline">\(q\)</span>æ‹Ÿåˆåˆ†å¸ƒ<span class="math inline">\(p\)</span>ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›<span class="math inline">\(S(p, q)\)</span>[]ä¸­çš„å¼å­æ˜¯ä¸<span class="math inline">\(p\)</span>æ— å…³çš„ã€‚æŠŠå¼å­å±•å¼€åå¯ä»¥å‘ç° <span class="math display">\[\begin{aligned}S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}-s_{p}\right)\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T}\left(k(\mathbf{x}, \mathbf{y}) s_{q}+\nabla_{y} k(\mathbf{x}, \mathbf{y})-k(\mathbf{x}, \mathbf{y}) s_{p}-\nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})\right)\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T} v(\mathbf{x}, \mathbf{y})\right]\end{aligned}\]</span> å…¶ä¸­<span class="math inline">\(v(\mathbf{x}, \mathbf{y})=k(\mathbf{x}, \mathbf{y}) s_{q}(\mathbf{y})+\nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})=\mathcal{A}_{q} k_{\mathbf{x}}(\mathbf{y}), k_{\mathbf{x}}(\cdot)=k(\mathbf{x}, \cdot)\)</span></p><p>è€Œå¯¹äºå›ºå®šçš„ <span class="math inline">\(\mathbf{y}\)</span>, å®¹æ˜“è¯æ˜ <span class="math inline">\(v(\cdot, \mathbf{y})\)</span> æ˜¯Stein class of <span class="math inline">\(p(\mathbf{x})\)</span>, å³æ»¡è¶³ <span class="math inline">\(\int_{\mathbf{x} \in \mathcal{X}} \nabla_{\mathbf{x}}(v(\mathbf{x}, \mathbf{y}) p(\mathbf{x})) d \mathbf{x}=0\)</span> ã€‚å› æ­¤å°±å¯ä»¥è¿›ä¸€æ­¥å°†ä¸Šå¼å†™å¼€ä¸º <span class="math display">\[\begin{aligned}S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})-\left(\nabla_{\mathbf{x}} \ln p(\mathbf{x})\right)^{T} v(\mathbf{x}, \mathbf{y})\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})\right]-\int d \mathbf{x} d \mathbf{y} p(\mathbf{x}) p(\mathbf{y})\left(\nabla_{\mathbf{x}} \ln p(\mathbf{x})\right)^{T} v(\mathbf{x}, \mathbf{y}) \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})\right]+\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\operatorname{tr} \nabla_{\mathbf{x}} v(\mathbf{x}, \mathbf{y})\right]\end{aligned}\]</span></p><p>å…¶ä¸­<span class="math inline">\(\nabla_{\mathbf{x}} v(\mathbf{x}, \mathbf{y})=\nabla_{\mathbf{x}} k(\mathbf{x}, \mathbf{y}) s_{q}(\mathbf{y})^{T}+\nabla_{\mathbf{x}} \nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})\)</span>ä¸ºä¸€ä¸ªçŸ©é˜µã€‚</p><p>è¿™æ ·å°±å¾—åˆ°äº†å‰æ–‡æåˆ°äº† <span class="math display">\[S(p, q)=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[u_{q}(\mathbf{x}, \mathbf{y})\right]\]</span> å…¶ä¸­<span class="math inline">\(u_{q}(\mathbf{x}, \mathbf{y})\)</span>ä»…ä¸åˆ†å¸ƒ<span class="math inline">\(q\)</span>æœ‰å…³ã€‚</p><p>ä¸Šè¿°çš„æ¨å¯¼è¯´æ˜äº†ä»€ä¹ˆï¼Ÿè¯´æ˜å¼•å…¥æ ¸æ–¹æ³•çš„å¯è¡Œæ€§ã€‚è€Œå¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è¦ç”¨æ ¸æ–¹æ³•æ¥åŠ é€Ÿå†…ç§¯çš„è®¡ç®—ï¼Œå¦‚ä½•ä½“ç°ï¼Ÿ<br />å€ŸåŠ©RKHSçš„å¯¹ç§°æ€§å’Œå†ç”Ÿæ€§ï¼Œæˆ‘ä»¬å¯ä»¥å°†<span class="math inline">\(S(p,q)\)</span>è¿›è¡Œå˜åŒ–ï¼š <span class="math display">\[\begin{aligned}S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)^{T}&lt;k(\mathbf{x}, \cdot), k(\cdot, \mathbf{y})&gt;_{\mathcal{H}}\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)\right] \\&amp;=\sum_{i=1}^{d}&lt;\mathbb{E}_{\mathbf{x} \sim p}\left[\left(s_{q}^{i}(\mathbf{x})-s_{p}^{i}(\mathbf{x})\right) k(\mathbf{x}, \cdot)\right], \mathbb{E}_{\mathbf{y} \sim p}\left[\left(s_{q}^{i}(\mathbf{y})-s_{p}^{i}(\mathbf{y})\right) k(\cdot \mathbf{y})\right]&gt;_{\mathcal{H}} \\&amp;=\sum_{i=1}^{d}&lt;\beta_{i}, \beta_{i}&gt;_{\mathcal{H}} \\&amp;=\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}^{2}\end{aligned}\]</span> å…¶ä¸­<span class="math inline">\(\boldsymbol{\beta}(\mathbf{y})\)</span>æ˜¯ä¸€ä¸ªå‘é‡å‡½æ•° <span class="math display">\[\boldsymbol{\beta}(\mathbf{y})=\mathbb{E}_{\mathbf{x} \sim p}\left[\mathcal{A}_{q} k_{\mathbf{y}}(\mathbf{x})\right]=\mathbb{E}_{\mathbf{x} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right) k_{\mathbf{y}}(\mathbf{x})\right]\]</span></p><p>åœ¨è¿™é‡Œå†å±•ç¤ºä¸€ä¸‹æœ€å¼€å§‹çš„stein discrepancy <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\mathcal{A}_{q} f(x)\right]\right)^{2}\]</span> å¯¹äºä»»æ„å‘é‡ <span class="math inline">\(\mathbf{f} \in \mathcal{H}^{d}\)</span> ä¸ <span class="math inline">\(\boldsymbol{\beta}\)</span> çš„å†…ç§¯ä¸º <span class="math display">\[\begin{aligned}&lt;\mathbf{f}, \boldsymbol{\beta}&gt;_{\mathcal{H}^{d}} &amp;=\sum_{i=1}^{d}&lt;f_{i}, \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x}) k(\mathbf{x}, \cdot)+\nabla_{x_{i}} k(\mathbf{x}, \cdot)\right]&gt;_{\mathcal{H}} \\&amp;=\sum_{i=1}^{d} \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x})&lt;f_{i}, k(\mathbf{x}, \cdot)&gt;_{\mathcal{H}}+&lt;f_{i}, \nabla_{x_{i}} k(\mathbf{x}, \cdot)&gt;_{\mathcal{H}}\right] \\&amp;=\sum_{i=1}^{d} \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x}) f_{i}(\mathbf{x})+\nabla_{x_{i}} f_{i}(\mathbf{x})\right] \\&amp;=\mathbb{E}_{\mathbf{x} \sim p}\left[\operatorname{tr}\left(\mathcal{A}_{q} \mathbf{f}(\mathbf{x})\right)\right] \leq\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}} (å› ä¸ºä»»æ„ä¸¤ä¸ªå‘é‡å†…ç§¯å°äºå®ƒä¸è‡ªèº«çš„å†…ç§¯)\end{aligned}\]</span> å› æ­¤æˆ‘ä»¬æœ‰ <span class="math display">\[\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}=S(p, q)=\max _{f \in \mathcal{H}^{d}}\left\{\mathbb{E}_{\mathbf{x} \sim p}\left[\operatorname{tr}\left(\mathcal{A}_{q} f(\mathbf{x})\right)\right]\right \}.\]</span> ä¸Šå¼ä¸­<span class="math inline">\(\|f\|_{\mathcal{H}^{d}} \leq 1\)</span>ï¼Œå¯¹åº”äºæœ€åˆæåˆ°çš„æ˜ å°„åˆ°å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸­çš„ä¸€ä¸ªçƒä¸­çš„æœ€ä¼˜å‘é‡ã€‚è¿™ä¸ª<span class="math inline">\(S(p,q)\)</span>æœ€å¤§å€¼æ‰€å¯¹åº”çš„å‘é‡ä¸º <span class="math inline">\({f}^{*}=\boldsymbol{\beta} /\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}\)</span></p><p>ç®€å•æ¥è¯´ï¼Œå¼•å…¥kernelä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¾—åˆ°stein discrepancyå®šä¹‰å¼ä¸­çš„å‡½æ•°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒKSDéå¸¸å®¹æ˜“å°±èƒ½æ±‚å¾—ã€‚</p><h1 id="stein-variational-gradient-descent-svgd">Stein Variational Gradient Descent (SVGD)</h1><p>SVGDçš„å¦ä¸€ä¸ªæ ¸å¿ƒå…¬å¼åœ¨äº <span class="math display">\[\left.\nabla_{\epsilon} \mathrm{KL}\left(q_{[T]} \| p\right)\right|_{\epsilon=0}=-\mathbb{E}_{x \sim q}\left[\operatorname{tr}\left(\mathcal{A}_{p} \phi(x)\right)\right]\]</span> ä¹Ÿå°±æ˜¯è¯´KLæ•£åº¦å˜åˆ†æ±‚å¯¼ç­‰äºKSDï¼ˆå…·ä½“æ¨å¯¼è¿‡ç¨‹è§é™„å½•ï¼‰ï¼Œè¿™æ„å‘³ç€KLæ•£åº¦å˜åŒ–æœ€å¿«çš„æ–¹å‘å°±æ˜¯KSDæ‰€å¯¹åº”çš„å‘é‡å‡½æ•° <span class="math inline">\(\phi^{*}=\boldsymbol{\beta} /\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}\)</span> <img src="/images/vimcmc/2.png" title="ç®—æ³•ä¼ªä»£ç " /> å…·ä½“è€Œè¨€ç²’å­<span class="math inline">\(\left\{x_{i}^{l}\right\}_{i=1}^{n}\)</span> è¡¨ç¤ºç¬¬<span class="math inline">\(l\)</span>æ¬¡è¾¾ä»£çš„ç¬¬<span class="math inline">\(i\)</span>ä¸ªç²’å­, ä¸€å…±<span class="math inline">\(n\)</span> ä¸ªã€‚ç²’å­æœ€å¼€å§‹æ˜¯ä»åˆ†å¸ƒ<span class="math inline">\(q_{0}\)</span>ä¸­é‡‡æ ·çš„, æœ€åˆçš„åˆ†å¸ƒ<span class="math inline">\(q\)</span>å¯ä»¥æ˜¯ä»»æ„ çš„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¯¥ç®—æ³•ä¸ä¾èµ–äºåˆå§‹çš„åˆ†å¸ƒã€‚</p><p>ç®—æ³•ä¸­çš„æ›´æ–°é¡¹åŒ…å«äº†ä¸¤ä¸ªéƒ¨åˆ† <span class="math display">\[k\left(x_{j}^{\ell}, x\right) \nabla_{x_{j}^{\ell}} \log p\left(x_{j}^{\ell}\right)+\nabla_{x_{j}^{\ell}} k\left(x_{j}^{\ell}, x\right)\]</span> å…¶ä¸­ç¬¬ä¸€é¡¹æ„å‘³ç€ç²’å­ä¼šæœ<span class="math inline">\(p\)</span>åˆ†å¸ƒæ¦‚ç‡é«˜çš„åœ°æ–¹ç§»åŠ¨ï¼Œè€Œç¬¬äºŒé¡¹ä»£è¡¨ç€ç²’å­å°†ä¼šæœç€è¿œç¦»å½“å‰è¿­ä»£è½®æ•°$l llçš„ç²’å­ï¼Œä»è€Œå‡è½»å±€éƒ¨æœ€ä¼˜çš„é£é™©ã€‚</p><p><img src="/images/vimcmc/2.gif" title="SVGDæ‹Ÿåˆä¸€ç»´åˆ†å¸ƒ" /></p><h1 id="å›é¡¾ä¸æ€»ç»“">å›é¡¾ä¸æ€»ç»“</h1><p>ä»ä¸Šæ–‡ç¹æ‚çš„æ¨å¯¼ä¸­ï¼ŒSVGDç®—æ³•ç¡®ä¿ç²’å­çš„ç§»åŠ¨æ˜¯æœç€KLæ•£åº¦çš„å‡å°æœ€å¿«æ–¹å‘ï¼Œè€Œè¿™ä¸ªæ–¹å‘å¯ä»¥æœ‰æ ¸åŒ–çš„stein discrepancyå¯¼å‡º æˆ‘ä»¬å›çœ‹ä¸€ä¸‹KL divergenceçš„å®šä¹‰å¼ï¼š <span class="math display">\[\mathrm{KL}(P \| Q)=\int P(x) \log \frac{P(x)}{Q(x)} d x\]</span></p><p>å¯¹äºç›®æ ‡åˆ†å¸ƒ<span class="math inline">\(p(x)\)</span>ï¼Œå˜åˆ†æ¨æ–­ (VI)ç›®æ ‡æ˜¯ä»ä¸€ç±»åˆ†å¸ƒæ—<span class="math inline">\(\mathcal{Q}\)</span>ä¸­æ‰¾åˆ°æœ€ä¼˜çš„<span class="math inline">\(q(x)\)</span> <span class="math display">\[q^{*}=\underset{q \in \mathcal{Q}}{\arg \min }\left\{K L(q \| p)=\mathbb{E}_{q}[\log q(x)]-\mathbb{E}_{q}[\log \bar{p}(x)]+\log Z\right\}\]</span> è€ŒSVGDåœ¨å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ä¸‹ç»™å‡ºäº†ä½¿å¾—KLæ•£åº¦ä¸‹é™æœ€å¿«çš„ç¡®å®šæ€§æ–¹å‘ï¼Œç±»ä¼¼ç»å…¸çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œå¯ä»¥ç†è§£ä¸ºè¿­ä»£æ„å»ºå¢é‡å˜åŒ–çš„æ–¹æ³•ã€‚</p><h1 id="appendix">Appendix</h1><p><span id="jump"> </span></p><h2 id="the-reproducing-kernel-hilbert-space">The reproducing kernel Hilbert space</h2><p>å…ˆå›é¡¾ä¸€ä¸‹kernelçš„å®šä¹‰ï¼š Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}\]</span></p><p>åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªå¼‚æˆ–é—®é¢˜çš„ä¾‹å­æ¥ä»‹ç»RKHSã€‚è€ƒè™‘ç‰¹å¾æ˜ å°„</p><p><span class="math display">\[\phi: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}\]</span> <span class="math display">\[x=\left[\begin{array}{l}x_{1} \\x_{2}\end{array}\right] \quad \mapsto \quad \phi(x)=\left[\begin{array}{c}x_{1} \\x_{2} \\x_{1} x_{2}\end{array}\right]\]</span> <img src="/images/vimcmc/4.png" title="ç‰¹å¾ç©ºé—´å’Œç‰¹å¾æ˜ å°„ã€‚å¸Œå°”ä¼¯ç‰¹ç©ºé—´çš„å…ƒç´ ä¸€èˆ¬æ˜¯å‡½æ•°ï¼Œè€Œå‡½æ•°å¯ä»¥è¢«è§†ä¸ºæ— ç©·ç»´çš„å‘é‡ã€‚å› æ­¤äº‹å®ä¸Šå¸Œå°”ä¼¯ç‰¹ç©ºé—´çš„åŸºåº•æ˜¯ä¸€ç»„æ— é™ç»´çš„å‡½æ•°ï¼Œå¯ä»¥å‚è€ƒå‚…ç«‹å¶å˜åŒ–æˆ–æ³°å‹’å±•å¼€" /></p><p>kernel <span class="math display">\[k(x, y)=\left[\begin{array}{c}x_{1} \\x_{2} \\x_{1} x_{2}\end{array}\right]^{\top}\left[\begin{array}{c}y_{1} \\y_{2} \\y_{1} y_{2}\end{array}\right]\]</span></p><p>æ¥ä¸‹æ¥æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç‰¹å¾å‡½æ•°ï¼š <span class="math display">\[f(x)=a x_{1}+b x_{2}+c x_{1} x_{2}\]</span> è¿™ä¸ªå‡½æ•°å±äºä»<span class="math inline">\(\mathcal{X}=\mathbb{R}^{2}\)</span>æ˜ å°„åˆ°<span class="math inline">\(\mathbb{R}\)</span>çš„å‡½æ•°ç©ºé—´ã€‚æ­¤æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠå‡½æ•°<span class="math inline">\(f\)</span>ç­‰ä»·è¡¨ç¤ºä¸ºï¼š <span class="math display">\[f(\cdot)=\left[\begin{array}{l}a \\b \\c\end{array}\right]\]</span> è‡³æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠ<span class="math inline">\(f(x)\)</span>å†™ä¸ºï¼š <span class="math display">\[\begin{aligned}f(x) &amp;=f(\cdot)^{\top} \phi(x) \\&amp;:=\langle f(\cdot), \phi(x)\rangle_{\mathcal{H}}\end{aligned}\]</span> ä¹Ÿå°±æ˜¯è¯´ï¼Œç‰¹å¾å‡½æ•°<span class="math inline">\(f\)</span>åœ¨<span class="math inline">\(x\)</span>çš„å€¼å¯ä»¥è¢«å†™ä¸ºç‰¹å¾ç©ºé—´ä¸­çš„å†…ç§¯ã€‚<span class="math inline">\(\mathcal{H}\)</span>æ˜¯ä¸€ä¸ªå°†<span class="math inline">\(\mathbb{R}^{2}\)</span>æ˜ å°„åˆ°<span class="math inline">\(\mathbb{R}\)</span>çš„å‡½æ•°ç©ºé—´ã€‚ä¸Šé¢è¿™äº›ä¹±ä¸ƒå…«ç³Ÿçš„æ€ä¹ˆä½“ç°å†ç”Ÿæ€§å‘¢ï¼Ÿæˆ‘ä»¬ä»”ç»†çœ‹ä¸‹é¢çš„ç­‰å¼ <span class="math display">\[k(\cdot, y)=\left[\begin{array}{c}y_{1} \\y_{2} \\y_{1} y_{2}\end{array}\right]=\phi(y)\]</span> ä¸Šå¼æˆ‘ä»¬å‚è€ƒ<span class="math inline">\(f(\cdot)\)</span>ç±»ä¼¼çš„å®šä¹‰ã€‚å…·ä½“æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä»¤<span class="math inline">\(a=y_{1}, b=y_{2}\)</span>, and <span class="math inline">\(c=y_{1} y_{2}\)</span>ï¼Œå°±æœ‰ <span class="math display">\[\langle k(\cdot, y), \phi(x)\rangle_{\mathcal{H}}=a x_{1}+b x_{2}+c x_{1} x_{2}\]</span></p><p>æ€»çš„æ¥è¯´RKHSä¸¤ä¸ªç‰¹æ€§ï¼š<br />æ¯ä¸ªç‚¹çš„ç‰¹å¾æ˜ å°„åœ¨ç‰¹å¾ç©ºé—´ä¸­ <span class="math display">\[\forall x \in \mathcal{X}, \quad k(\cdot, x) \in \mathcal{H}\]</span> å†ç”Ÿæ€§ï¼š<br /><span class="math display">\[\forall x \in \mathcal{X}, \forall f \in \mathcal{H},\langle f, k(\cdot, x)\rangle_{\mathcal{H}}=f(x)\]</span> <span class="math display">\[k(x, y)=\langle k(\cdot, x), k(\cdot, y)\rangle_{\mathcal{H}}\]</span></p><h2 id="klæ•£åº¦ä¸€é˜¶å¯¼ä¸ksd">KLæ•£åº¦ä¸€é˜¶å¯¼ä¸KSD</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;
&lt;p&gt;è¿™ä¸€å·¥ä½œæ˜¯æ¸…åå¤§å­¦liu qiangè€å¸ˆæå‡ºçš„ï¼Œç›¸å…³è®ºæ–‡ä»2016å¹´å¼€å§‹ä¹Ÿä¸€ç›´åœ¨æ›´æ–°ï¼Œåˆ†åˆ«å‘è¡¨åœ¨NIPSã€ICLRç­‰é¡¶ä¼šä¸Šã€‚&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1704.07520&quot;</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="SVGD" scheme="http://example.com/tags/SVGD/"/>
    
  </entry>
  
  <entry>
    <title>Multi-Label Image Recognition with Graph Convolutional Networks [CVPR2019]</title>
    <link href="http://example.com/2021/10/26/pd5/"/>
    <id>http://example.com/2021/10/26/pd5/</id>
    <published>2021-10-26T15:25:47.000Z</published>
    <updated>2021-10-27T16:57:37.967Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1904.03582">Multi-Label Image Recognition with Graph Convolutional Networks</a><br />Code: <a href="https://github.com/chenzhaomin123/ML_GCN">link</a></p><p>é’ˆå¯¹å¤šæ ‡ç­¾å›¾åƒè¯†åˆ« (multi-label image recognition) é—®é¢˜ï¼Œæ—·è§†ç ”ç©¶é™¢æå‡ºä¸€ç§åŸºäºå›¾å·ç§¯ç½‘ç»œçš„æ¨¡å‹å–å¾—äº†è‰¯å¥½çš„è¡¨ç°ï¼Œè¯¥æ¨¡å‹åŒ…å«ä¸€ä¸ªCNNçš„å›¾åƒç‰¹å¾æå–æ¨¡å—å’Œä¸€ä¸ªå›¾å·ç§¯ç½‘ç»œè¿›è¡Œæ ‡ç­¾é—´å…³ç³»æå–æ¨¡å—ã€‚</p><h2 id="intro">Intro</h2><p>å¯¹äºå¤šæ ‡ç­¾å›¾åƒçš„è¯†åˆ«é—®é¢˜ï¼Œä¼ ç»Ÿçš„æ–¹æ³•å¾€å¾€æ˜¯å¯¹æ¯ä¸ªæ ‡ç­¾è¿›è¡Œå­¤ç«‹çš„äºŒåˆ†ç±»ï¼Œå³é¢„æµ‹æ¯ä¸ªç‰©ä½“æ˜¯å¦å‡ºç°ã€‚åŸºäºæ¦‚ç‡å›¾æ¨¡å‹æˆ–RNNæ¨¡å‹çš„æ–¹æ³•åˆ™è€ƒè™‘æ˜¾å¼çš„å»ºæ¨¡æ ‡ç­¾ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚ä¹Ÿæœ‰æ–¹æ³•å°†å›¾åƒåŒºåŸŸåˆ’åˆ†åè€ƒè™‘åŒºåŸŸé—´çš„å±€éƒ¨ç›¸å…³æ€§ï¼Œä»è€Œéšå¼çš„å»ºæ¨¡æ ‡ç­¾ç›¸å…³æ€§ã€‚æœ¬æ–‡æå‡ºçš„åŸºäºGCNçš„ç«¯åˆ°ç«¯æ¨¡å‹å°†æ ‡ç­¾çš„è¡¨ç¤ºæ˜ å°„åˆ°ç›¸äº’ç‹¬ç«‹çš„å¯¹è±¡åˆ†ç±»å™¨ä¸Šã€‚</p><h2 id="related-work">Related Work</h2><p>æœ€ç®€å•çš„å¤šæ ‡ç­¾è¯†åˆ«æ–¹æ³•å°±æ˜¯ä¸ºæ¯ä¸ªæ ‡ç­¾ç‹¬ç«‹è®­ç»ƒä¸€ä¸ªäºŒåˆ†ç±»å™¨ï¼Œè¿™ç§æ¨¡å‹æ²¡æœ‰è€ƒè™‘æ ‡ç­¾ä¹‹é—´çš„å…³ç³»ã€‚å½“æ•°æ®é›†ä¸­å¯èƒ½çš„æ ‡ç­¾æ•°é‡å¢é•¿æ—¶ï¼Œå¯èƒ½çš„æ ‡ç­¾ç»„åˆå°±ä¼šæŒ‡æ•°çº§å¢é•¿ï¼ˆå½“ä¸€ä¸ªæ•°æ®é›†åŒ…å«20ä¸ªæ ‡ç­¾ï¼Œåˆ™æ ‡ç­¾ç»„åˆå°±æœ‰<span class="math inline">\(2^{20}\)</span>ç§ã€‚åŸºäºRNNã€LSTMä¹‹ç±»çš„æ¨¡å‹å°†æ ‡ç­¾åµŒå…¥ä¸ºå‘é‡ï¼Œä»è€Œå‘æ˜æ ‡ç­¾é—´çš„ç›¸å…³æ€§ã€‚<br />æœ¬æ–‡æå‡ºçš„æ¨¡å‹å°†å¤šæ ‡ç­¾æ„å»ºä¸ºæœ‰å‘å›¾ï¼Œå€ŸåŠ©GCNåœ¨æ ‡ç­¾é—´çš„ä¿¡æ¯ä¼ æ’­æ¥å­¦ä¹ å›¾åƒæ ‡ç­¾é—´ä¾èµ–ã€å…±ç°å…³ç³»ï¼Œå¹¶å®ç°ç«¯åˆ°ç«¯è®­ç»ƒã€‚</p><h2 id="framework">Framework</h2><p><img src="/images/pd5/2.png" title="æ¨¡å‹æ¡†æ¶" /></p><h3 id="å›¾åƒç‰¹å¾æå–">å›¾åƒç‰¹å¾æå–</h3><p>è®ºæ–‡ç”¨CNNè¿›è¡Œå›¾åƒç‰¹å¾æå–ï¼Œå…·ä½“ä¸ºResNet-101çš„ç½‘ç»œç»“æ„ï¼Œè¾“å…¥å›¾åƒ<span class="math inline">\(I\)</span>ï¼Œç»è¿‡cnnå’Œglobal max-poolingåå¾—åˆ°2048ç»´å›¾åƒç‰¹å¾ã€‚ <span class="math display">\[\boldsymbol{x}=f_{\mathrm{GMP}}\left(f_{\mathrm{cnn}}\left(\boldsymbol{I} ; \theta_{\mathrm{cnn}}\right)\right) \in \mathbb{R}^{D}\]</span></p><h3 id="å›¾å·ç§¯">å›¾å·ç§¯</h3><p>å·ç§¯æ¨¡å—ä¸æœ€åŸºæœ¬çš„å·ç§¯ç›¸åŒï¼Œå¦‚ä¸‹å¼ <span class="math display">\[\boldsymbol{H}^{l+1}=h\left(\widehat{\boldsymbol{A}} \boldsymbol{H}^{l} \boldsymbol{W}^{l}\right)\]</span> æˆ‘ä»¬ä¸»è¦å…³æ³¨å¦‚ä½•æ„å›¾ï¼Œåœ¨è¿™ä¸€æ–¹é¢ï¼Œæœ¬æ–‡çš„ideaä¼¼ä¹æœ‰äº›è¶…è„±CVé¢†åŸŸã€‚æ¨¡å‹é’ˆå¯¹å›¾ç‰‡æ•°æ®é›†æ„å»ºå›¾ï¼Œå›¾ä¸­çš„èŠ‚ç‚¹ä¸ºæ•°æ®ä¸­çš„æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨word embeddingï¼ˆpre-trained gloveï¼‰å¯¹èŠ‚ç‚¹ç‰¹å¾è¿›è¡Œåˆå§‹åŒ–ã€‚<br />è€Œå¯¹äºå›¾çš„è¾¹ï¼Œä¹Ÿå¯¹åº”å›¾å·ç§¯ä¸­çš„çŸ©é˜µ<span class="math inline">\(\boldsymbol{A}\)</span>ï¼ˆæ–‡ä¸­ç§°å…¶ä¸ºç›¸å…³ç³»æ•°çŸ©é˜µï¼‰ï¼Œæ¨¡å‹ä½¿ç”¨æ¡ä»¶æ¦‚ç‡<span class="math inline">\(P\left(L_{j} \mid L_{i}\right)\)</span>è¿›è¡Œå»ºæ¨¡ï¼Œå·²æœŸè·å¾—æ ‡ç­¾ç›¸å…³æ€§ä¿¡æ¯ã€‚ <img src="/images/pd5/3.png" /> å…·ä½“è€Œè¨€ï¼Œè®ºæ–‡ç»Ÿè®¡äº†æ•°æ®é›†ä¸­çš„æ ‡ç­¾å¯¹çš„å…±ç°æ¬¡æ•°ï¼Œç„¶åæ„å»ºå…±ç°çŸ©é˜µï¼Œå¹¶è®¾å®šä¸€ä¸ªé˜ˆå€¼æ¥è¿›è¡ŒäºŒå€¼åŒ–å¤„ç†ï¼Œå€Ÿæ­¤è¿‡æ»¤å™ªå£°è¾¹ã€‚ <img src="/images/pd5/1.png" title="åŸºäºå¤šæ ‡ç­¾æ„å»ºæœ‰å‘å›¾" /></p><p>å€ŸåŠ©æ¨¡å‹æ¡†æ¶å›¾å¯ä»¥çœ‹åˆ°ï¼Œæ¨¡å‹ä¸­å›¾å·ç§¯æ¨¡å—èµ·çš„æ˜¯ç±»ä¼¼è¾…åŠ©åˆ†ç±»å™¨çš„ä½œç”¨ï¼Œå›¾ä¸­æ¯ä¸ªæ ‡ç­¾èŠ‚ç‚¹å°±æ˜¯è¯¥æ ‡ç­¾çš„ä¸€ä¸ªäºŒåˆ†ç±»å™¨ï¼Œå°†åŸºäºæ•´ä¸ªæ•°æ®é›†è®­ç»ƒçš„åˆ†ç±»å™¨<span class="math inline">\(\boldsymbol{W} \in \mathbb{R}^{C \times D}\)</span>ä¸å›¾åƒçš„ç‰¹å¾<span class="math inline">\(x \in \mathbb{R}^{D}\)</span>è¿›è¡Œç‚¹ç§¯ï¼Œå¾—åˆ°<span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^{C}\)</span>ï¼ˆCè¡¨ç¤ºæ ‡ç­¾çš„æ€»æ•°ï¼‰ã€‚å›¾å·ç§¯åˆ©ç”¨çš„ä¿¡æ¯ä¹Ÿåªæœ‰å›¾çš„è¾¹ï¼Œä¹Ÿå°±æ˜¯æ ‡ç­¾çš„å…±ç°ï¼Œè€Œåå€ŸåŠ©å›¾å·ç§¯ä¸å›¾åƒç‰¹å¾æå–è¿›è¡Œå…±åŒè®­ç»ƒï¼Œå¾—åˆ°æ ‡ç­¾ä¹‹é—´å…³ç³»çš„éšå¼è¡¨ç¤ºï¼Œæœ€ç»ˆæ¨åŠ¨æ›´å‡†ç¡®çš„å¤šæ ‡ç­¾è¯†åˆ«ã€‚</p><h2 id="å®éªŒ">å®éªŒ</h2><p><img src="/images/pd5/4.png" title="å®éªŒç»“æœâ€”éå¸¸ä¸é”™ XD" /> ä¸è¿‡åœ¨å°è¯•å¤ç°è¯¥æ¨¡å‹æ—¶ï¼Œæœ¬äººè¯•éªŒäº†å‡ ä¸ªæ•°æ®é›†ä¼¼ä¹å§‹ç»ˆæ— æ³•åˆ°è¾¾è®ºæ–‡ä¸­çš„ç»“æœã€‚</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.03582&quot;&gt;Multi-Label Image Recognition with Graph Convolutional Networks&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;https</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="CV-GNN" scheme="http://example.com/tags/CV-GNN/"/>
    
  </entry>
  
  <entry>
    <title>Multi-hop Question Generation with Graph Convolutional Network [Arxiv]</title>
    <link href="http://example.com/2021/10/24/pd6/"/>
    <id>http://example.com/2021/10/24/pd6/</id>
    <published>2021-10-24T15:56:42.000Z</published>
    <updated>2021-10-27T17:48:33.014Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2010.09240.pdf">Multi-hop Question Generation with Graph Convolutional Network</a><br />Code: <a href="https://github.com/HLTCHKUST/MulQG">link</a></p><h2 id="background">Background</h2><p>é—®é¢˜ç”Ÿæˆ(QG)æ˜¯ä¸€ä¸ªä»ç»™å®šçš„ä¸Šä¸‹æ–‡è‡ªåŠ¨ç”Ÿæˆé—®é¢˜æˆ–ç­”æ¡ˆçš„ä»»åŠ¡ï¼Œè€Œå¤šè·³é—®é¢˜ç”Ÿæˆ (Multi-hop Question Generation) éœ€è¦ä»å¤šä¸ªä¸åŒçš„æ®µè½ä¸­æ¨ç†ç”Ÿæˆä¸ç­”æ¡ˆç›¸å…³çš„é—®é¢˜ã€‚QGå¯ä»¥åº”ç”¨äºæ•™è‚²ç³»ç»Ÿï¼Œä¹Ÿå¯ä»¥ç»“åˆQAæ¨¡å‹ä½œä¸ºåŒé‡ä»»åŠ¡æ¥å¢å¼ºQAç³»ç»Ÿçš„æ¨ç†èƒ½åŠ›ã€‚å¯¹äºå¤šè·³é—®é¢˜ç”Ÿæˆï¼Œæ ¸å¿ƒé—®é¢˜åœ¨äºå¦‚ä½•è¿æ¥å¤šä¸ªæ®µè½é—´çš„é›¶æ•£çš„ä¿¡æ¯ä»¥åŠç­”æ¡ˆã€‚</p><p><img src="/images/pd6/1.png" title="å¤šè·³é—®é¢˜ç”Ÿæˆ" /></p><h2 id="æ¨¡å‹">æ¨¡å‹</h2><p><img src="/images/pd6/2.png" title="æ¨¡å‹æ¡†æ¶" /></p><h3 id="multi-hop-encoder">Multi-hop Encoder</h3><p>å¯¹äºè¾“å…¥çš„æ–‡æœ¬æ®µè½å’Œç­”æ¡ˆï¼Œå…ˆåˆ†å‰²æˆword-levelçš„tokenï¼Œå¹¶åˆ†åˆ«ç”¨pre-trained Gloveè¿›è¡Œembeddingï¼Œå¹¶åœ¨æ–‡æœ¬çš„token embeddingä¸­åŠ å…¥ç­”æ¡ˆ embedding tagã€‚å¯¹äºå¾—åˆ°çš„token embedding è¾“å…¥åˆ°LSTM-RNNä¸­å­¦ä¹ åˆæ­¥çš„ä¸Šä¸‹æ–‡ç›¸å…³çš„representationï¼Œå†è¾“å…¥åˆ°Encoderä¸­ï¼Œæ¨¡å‹çš„EncoderåŒ…æ‹¬ä¸‰ä¸ªéƒ¨åˆ†:</p><ul><li><p>Answer-aware context encoder è¿™ä¸€éƒ¨åˆ†å‚è€ƒäº†é˜…è¯»ç†è§£ä¸­çš„co-attention reasoningæœºåˆ¶: <span class="math display">\[\begin{aligned}S &amp;=C_{0}^{T} A_{0} \in R^{n \times m} \\S^{\prime} &amp;=\operatorname{softmax}(S) \in R^{n \times m} \\S^{\prime \prime} &amp;=\operatorname{softmax}\left(S^{T}\right) \in R^{m \times n} \\A_{0}^{\prime} &amp;=C_{0} \cdot S^{\prime} \in R^{d \times m} \\\tilde{C}_{1} &amp;=\left[A_{0} ; A_{0}^{\prime}\right] \cdot S^{\prime \prime} \in R^{2 d \times n}\\C_{1} &amp;=\operatorname{BiLSTM}\left(\left[\tilde{C}_{1} ; C_{0}\right]\right) \in R^{d \times n}\end{aligned}\]</span> ç›¸å…³æ€§çŸ©é˜µSè¡¨ç¤ºç­”æ¡ˆä¸ä¸Šä¸‹æ–‡çš„ç›¸å…³æ€§ï¼Œæ•´ä¸ªè¿‡ç¨‹æ¯”è¾ƒå¤æ‚ï¼Œè¿™ä¸€æ¨¡å—çš„æœ‰æ•ˆæ€§åœ¨é˜…è¯»ç†è§£ä»»åŠ¡ä¸­è¢«éªŒè¯ï¼Œå¤§è‡´æ“ä½œå³å°†ç­”æ¡ˆä¸æ–‡æœ¬è®¡ç®—attentionåç”Ÿæˆæ–°çš„â€œç­”æ¡ˆâ€è€ŒååŒæ ·è¿›è¡Œä¸€éç›¸å…³æ€§è®¡ç®—ï¼Œæœ€åè¾“å…¥Bi-LSTMä¸­ã€‚</p></li><li><p>GCN-based entity-aware answer encoder å°†ä¸Šè¿°encoderå¾—åˆ°çš„embeddingè¾“å…¥åˆ°GCNä¸­è¿›è¡Œå¤šè·³ä¿¡æ¯çš„åµŒå…¥ã€‚ <img src="/images/pd6/3.png" title="GCN-based entity-aware answer encoder" /> å›¾ä¸­çš„èŠ‚ç‚¹ä¸ºæ–‡æœ¬ä¸­çš„å‘½åå®ä½“ï¼ˆç”±BERTè‡ªåŠ¨åŒ–æå–ï¼‰ï¼Œå¦‚æœå®ä½“å¯¹åœ¨åŒä¸€å¥å­ä¸­ï¼Œåˆ™ä¸ºå®ƒä»¬åˆ›å»ºè¾¹ã€‚å°†ä¸Šé¢Answer-aware context encoder çš„ç»“æœç»“åˆåˆ°å¤šè·³å›¾å·ç§¯ä¸­ï¼Œå¹¶æœ€ç»ˆå’Œå›¾çš„ç»“æœç»“åˆï¼Œè¾“å…¥åˆ°Bi-attentionæ¨¡å‹ï¼Œè¿›ä¸€æ­¥å¾—åˆ°tokençš„representations <span class="math display">\[A_{1}=\text { BiAttention }\left(A_{0}, E_{M}\right)\]</span></p></li><li><p>Gated encoder reasoning layer å°†å‰é¢å¾—åˆ°çš„ç»“æœè¾“å…¥åˆ°é—¨æ§ç½‘ç»œè¿›è¡Œç‰¹å¾èåˆï¼Œè¿›è¡Œç‰¹å¾ä¿ç•™æˆ–é—å¿˜ï¼Œå¾—åˆ°æœ€ç»ˆçš„Encoderç»“æœã€‚</p></li></ul><h3 id="maxout-pointer-decoder">Maxout Pointer Decoder</h3><p>æ¨¡å‹é‡‡ç”¨å•å‘LSTMä½œä¸ºè§£ç å™¨ï¼Œè€ŒMaxout pointerè¿™ä¸€æ¨¡å—ä¹Ÿå¹¶ä¸æ˜¯ç”±ä½œè€…æå‡ºçš„ï¼Œè€Œæ˜¯å‚è€ƒäº†ä»–äººçš„æ¨¡å‹ï¼Œç”¨è¿™ä¸€æ¨¡å—å‡å°‘ç”Ÿæˆç»“æœä¸­çš„é‡å¤é¡¹ã€‚</p><h2 id="å®éªŒ">å®éªŒ</h2><p>å®éªŒéƒ¨åˆ†ï¼Œä½œè€…åˆ†åˆ«åšäº†ä¸ç°æœ‰multi-hop QGæ¨¡å‹å¯¹æ¯”ä»¥åŠæ¶ˆèå®éªŒï¼Œå–å¾—äº†SOTAç»“æœï¼Œå¹¶ä¸”è¯æ˜äº†æ¡†æ¶ä¸­æ¯ä¸ªæ¨¡å—çš„æ„ä¹‰ã€‚ <img src="/images/pd6/4.png" title="çºµå‘å¯¹æ¯”" /> <img src="/images/pd6/5.png" title="æ¶ˆèå®éªŒ" /></p><h2 id="æ€»ç»“">æ€»ç»“</h2><p>æœ¬æ–‡æå‡ºçš„æ¡†æ¶æ€»ä½“æ¥è¯´æ¯”è¾ƒå¤æ‚ã€‚å¾€ç‰›äº†è¯´å¯ä»¥ç†è§£ä¸ºæ•´ä¸ªæ¡†æ¶æ¨¡æ‹Ÿäº†äººç±»çš„é—®é¢˜ç”Ÿæˆçš„è¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ•´ä½“æ–‡æœ¬å’Œç­”æ¡ˆçš„é˜…è¯»ï¼Œè¿›è¡Œå¤§æ¦‚äº†è§£ï¼Œè€Œåå¯¹æ–‡æœ¬å’Œç­”æ¡ˆä¸­çš„å®ä½“è¿›è¡Œå…³æ³¨ï¼Œå¹¶å¯»æ‰¾ä»–ä»¬çš„è”ç³»ï¼Œæœ€ååœ¨ç”Ÿæˆé—®é¢˜æ—¶ç¡®å®šæ ¸å¿ƒå’Œæ¬¡è¦ä¿¡æ¯ï¼Œç”Ÿæˆç›¸å…³çš„é—®é¢˜ã€‚ä¸è¿‡äº‹å®ä¸Šæ•´ä¸ªæ¡†æ¶å°±æ˜¯å¯¹å‡ ä¸ªç°æœ‰æ¨¡å‹ä¸­çš„éƒ¨åˆ†æ¨¡å—è¿›è¡Œç»„è£…ï¼Œç±»ä¼¼â€œæ­ç§¯æœ¨â€çš„è¿‡ç¨‹ã€‚è€Œæ–°åŠ å…¥çš„multi-hopå›¾å·ç§¯éƒ¨åˆ†æ•´ä½“æ–¹æ³•ä¹Ÿä¸å…·æœ‰å¾ˆäº®ç‚¹çš„æƒ³æ³•ï¼Œä»æ¶ˆèå®éªŒç»“æœä¸­ä¹Ÿå¯ä»¥çœ‹å‡ºè¿™ä¸€æ¨¡å—å¯¹æœ€ç»ˆç»“æœçš„æå‡ä¹Ÿå¹¶ä¸æ˜¯å¾ˆæ˜æ˜¾ã€‚</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.09240.pdf&quot;&gt;Multi-hop Question Generation with Graph Convolutional Network&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;ht</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="QA" scheme="http://example.com/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>2021 MAXPå‘½é¢˜èµ› åŸºäºDGLçš„å›¾æœºå™¨å­¦ä¹ ä»»åŠ¡</title>
    <link href="http://example.com/2021/10/23/maxp/"/>
    <id>http://example.com/2021/10/23/maxp/</id>
    <published>2021-10-23T07:33:06.000Z</published>
    <updated>2021-10-26T12:52:51.527Z</updated>
    
    <content type="html"><![CDATA[<p>ä½¿ç”¨<a href="https://www.dgl.ai/">Deep Graph Library (DGL)</a>è¿›è¡Œå›¾èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨çš„å›¾æ•°æ®æ˜¯åŸºäºå¾®è½¯å­¦æœ¯æ–‡çŒ®ç”Ÿæˆçš„è®ºæ–‡å…³ç³»å›¾ï¼Œå…¶ä¸­çš„èŠ‚ç‚¹æ˜¯è®ºæ–‡ï¼Œè¾¹æ˜¯è®ºæ–‡é—´çš„å¼•ç”¨å…³ç³»ã€‚æ•´ä¸ªå›¾åŒ…æ‹¬çº¦150ä¸‡ä¸ªèŠ‚ç‚¹ï¼Œ2000ä¸‡æ¡è¾¹ã€‚èŠ‚ç‚¹åŒ…å«300ç»´çš„ç‰¹å¾ï¼Œæ¥è‡ªè®ºæ–‡çš„æ ‡é¢˜å’Œæ‘˜è¦ç­‰å†…å®¹ã€‚èŠ‚ç‚¹å±äºçº¦50ä¸ªç±»åˆ«ã€‚<br />æ¯”èµ›åœ°å€: <a href="https://biendata.xyz/competition/maxp_dgl/">MAXP</a></p><p><img src="/images/maxp/1.png" title="æ¯”èµ›æ•°æ®é›†" /></p><h2 id="æ•°æ®é¢„å¤„ç†">æ•°æ®é¢„å¤„ç†</h2><p>æ ¹æ®æ‰€ç»™çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦è¯»å–èŠ‚ç‚¹åŠå…¶å¯¹åº”çš„ç‰¹å¾ï¼Œä»¥åŠè¾¹ã€‚æ ¹æ®è®ºæ–‡idæ„å»ºå¯¹åº”çš„èŠ‚ç‚¹idï¼Œå¹¶åˆ†é…ä»–ä»¬çš„ç‰¹å¾å’Œç±»åˆ«ã€‚è¯»å–è¾¹ä¹‹åå‘ç°å­˜åœ¨éƒ¨åˆ†è®ºæ–‡æ²¡æœ‰å‡ºç°åœ¨è®ºæ–‡æ•°æ®ä¸­ï¼Œè¿™éƒ¨åˆ†çš„èŠ‚ç‚¹idåˆ†é…åˆ°æœ€åï¼Œè¿™ç±»è®ºæ–‡æ²¡æœ‰ç±»åˆ«å’Œç‰¹å¾ï¼Œè¿™äº›ç‚¹çš„ç‰¹å¾å¯ä»¥é€‰æ‹©ç”¨é‚»èŠ‚ç‚¹ç‰¹å¾å‡å€¼è¿›è¡Œèµ‹å€¼ã€‚èŠ‚ç‚¹ç‰¹å¾ä½¿ç”¨Numpyä¿å­˜ä¸º.npyæ ¼å¼ï¼Œæ–¹ä¾¿åç»­è¯»å–ã€‚<br />å¯¹trainæ–‡ä»¶ä¸­çš„æ•°æ®è¿›è¡ŒTrain/Validåˆ†å‰²ï¼Œç”¨äºæ¨¡å‹è¯„ä¼°ï¼ˆ9:1ï¼‰ï¼Œå°†train/valid/testèŠ‚ç‚¹idå’Œlabelsç­‰æ•°æ®ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶æ–¹ä¾¿å¿«é€Ÿè¯»å– <img src="/images/maxp/3.png" /></p><h2 id="å›¾æ„å»º">å›¾æ„å»º</h2><p>æ ¹æ®ä¸Šé¢çš„åˆ°çš„åŸè®ºæ–‡id-å¼•ç”¨è®ºæ–‡idä»¥åŠä»–ä»¬å¯¹åº”çš„èŠ‚ç‚¹idï¼Œå€ŸåŠ©DGLåŒ…æ„å»ºè®ºæ–‡å¼•ç”¨å…³ç³»å›¾ã€‚ <img src="/images/maxp/2.png" title="æ„å›¾" /> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">g = dgl.graph((u,v))</span><br><span class="line">g.nodes() <span class="comment">#è·å–èŠ‚ç‚¹id</span></span><br><span class="line">g.edges() <span class="comment">#è·å–è¾¹å¯¹åº”çš„èŠ‚ç‚¹ï¼ˆè¾“å‡ºçš„æ˜¯ä¸¤ä¸ªtensorï¼‰</span></span><br><span class="line">g.ndata(<span class="string">&#x27;feature&#x27;</span>) <span class="comment">#è®¿é—®èŠ‚ç‚¹å±æ€§</span></span><br><span class="line">g.edata <span class="comment">#è®¿é—®è¾¹å±æ€§</span></span><br></pre></td></tr></table></figure></p><h2 id="model_baseline">Model_baseline</h2><p>é¢„å¤„ç†å’Œæ„å›¾ä¹‹åï¼Œæˆ‘ä»¬æ¨¡å‹è¾“å…¥çš„æ•°æ®åŒ…æ‹¬ï¼š <img src="/images/maxp/4.png" /></p><p>ç«èµ›çš„baselineåŒ…æ‹¬ä¸‰ä¸ªæ¨¡å‹ï¼š</p><ul><li>graphsage</li><li>graphconvolution</li><li>graphattention</li></ul><p>å…¶ä¸­å„ä¸ªç½‘ç»œç”±DGL.nnæ¨¡å—è°ƒåº“æ­å»ºã€‚ç»è¿‡åˆæ­¥è°ƒå‚åå‘ç°ç½‘ç»œæ·±åº¦ä¸º3å±‚æ—¶ä¸‰ä¸ªæ¨¡å‹ç»“æœæœ€å¥½ï¼Œå…¶ä¸­graphsageè¡¨ç°æœ€å¥½ï¼Œåœ¨éªŒè¯é›†ä¸Šå‡†ç¡®ç‡æ¥è¿‘54ã€‚ <img src="/images/maxp/5.png" title="å¯è§†åŒ–æ•ˆæœ" /></p><h2 id="proposed-model">Proposed model</h2><p>æ•°æ®ä¸­çš„èŠ‚ç‚¹ç‰¹å¾å·²ç»ç»™å®šï¼Œå› æ­¤åªèƒ½æ”¹å˜æ¨¡å‹çš„ç½‘ç»œç»“æ„ï¼Œæˆ‘å‚è€ƒäº†2020 aclçš„è®ºæ–‡ï¼šEvery Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks æ‰€ç”¨çš„ç½‘ç»œæ¶æ„ï¼Œå…·ä½“é‡‡ç”¨çš„æ˜¯ä¸‰å±‚å›¾å·ç§¯åè¿æ¥ä¸€å±‚å›¾æ³¨æ„åŠ›ï¼Œå¹¶åœ¨å›¾å·ç§¯ä¸attentionç›´æ¥å¢åŠ äº†skip-connectionã€‚åœ¨éªŒè¯é›†ä¸Šçš„åˆ°å‡†ç¡®ç‡ä¸º55+ï¼Œç›®å‰æ’è¡Œæ¦œä¸Šå‰10ï¼Œåç»­å°†ä¼šåšè¿›ä¸€æ­¥è°ƒå‚æ¥å¾—åˆ°æ›´å¥½çš„ç»“æœã€‚<br />å¦å¤–ï¼Œæ ¹æ®ç»™å®šçš„ç‰¹å¾ç›´æ¥ä½¿ç”¨MLPç­‰å‰å‘ä¼ æ’­ç½‘ç»œè™½ç„¶æ— æ³•åˆ©ç”¨å¼•ç”¨ä¿¡æ¯ï¼Œä½†å¯ä»¥ç”¨æ¥è¾…åŠ©æœ€ç»ˆçš„åˆ†ç±»ã€‚</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ä½¿ç”¨&lt;a href=&quot;https://www.dgl.ai/&quot;&gt;Deep Graph Library (DGL)&lt;/a&gt;è¿›è¡Œå›¾èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ï¼Œä½¿ç”¨çš„å›¾æ•°æ®æ˜¯åŸºäºå¾®è½¯å­¦æœ¯æ–‡çŒ®ç”Ÿæˆçš„è®ºæ–‡å…³ç³»å›¾ï¼Œå…¶ä¸­çš„èŠ‚ç‚¹æ˜¯è®ºæ–‡ï¼Œè¾¹æ˜¯è®ºæ–‡é—´çš„å¼•ç”¨å…³ç³»ã€‚æ•´ä¸ªå›¾åŒ…æ‹¬çº¦150ä¸‡ä¸ªèŠ‚ç‚¹ï¼Œ2000ä¸‡æ¡è¾¹ã€‚èŠ‚</summary>
      
    
    
    
    <category term="Project" scheme="http://example.com/categories/Project/"/>
    
    
    <category term="Competition" scheme="http://example.com/tags/Competition/"/>
    
  </entry>
  
  <entry>
    <title>A Survey of Graph Neural Networks in Natural Language Processing [Arxiv]</title>
    <link href="http://example.com/2021/10/18/gnn-nlp/"/>
    <id>http://example.com/2021/10/18/gnn-nlp/</id>
    <published>2021-10-18T05:58:53.000Z</published>
    <updated>2021-10-26T15:36:48.929Z</updated>
    
    <content type="html"><![CDATA[<p>æˆªæ­¢2021å¹´åˆï¼Œå…³äºç°æœ‰çš„å›¾ç¥ç»ç½‘ç»œåº”ç”¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ç»¼è¿°<br />link: <a href="https://arxiv.org/abs/2106.06090">Graph Neural Networks for Natural Language Processing: A Survey</a></p><p>å¦å¤–ï¼Œæœ¬ç¯‡åšå®¢è¿˜åŒ…å«å¦å¤–å‡ ç¯‡å›¾ç›¸å…³çš„ç»¼è¿°æ€§æ–‡ç« å†…å®¹ï¼š<br />Graph Representation Learning<br />Geometric Deep Learning</p><p>å¯¹äºå›¾çš„æœºå™¨å­¦ä¹ ï¼Œéå¸¸å®¹æ˜“æƒ³åˆ°çš„å°±æ˜¯èŠ‚ç‚¹åˆ†ç±»ï¼Œè¾¹é¢„æµ‹ã€ä»¥åŠå›¾å±‚çº§çš„åˆ†ç±»ç­‰ã€‚å¯¹äºä¼ ç»Ÿçš„NLPé—®é¢˜ï¼Œæˆ‘ä»¬å°†è¾“å…¥çš„æ–‡æœ¬åºåˆ—è¡¨ç¤ºä¸ºå›¾ç»“æ„æ—¶ï¼Œå°±å¯ä»¥å€ŸåŠ©å›¾æ·±åº¦å­¦ä¹ æŠ€æœ¯è¿›è¡Œå¤„ç†ã€‚æ•´ç¯‡ç»¼è¿°æ ¹æ®è¿™ä¸€æ€è·¯ä»å›¾æ„é€ ã€å›¾è¡¨ç¤ºå­¦ä¹ ã€åŸºäºå›¾çš„Encoder-Decoderæ¨¡å‹ä¸‰æ–¹é¢è¿›è¡Œä»‹ç»ã€‚</p><h2 id="å›¾æ„é€ ">å›¾æ„é€ </h2><p>å¯¹äºAIå¤„ç†çš„æ•°æ®ç±»å‹ï¼Œå¤§æ¦‚å¯ä»¥åˆ†ç±»3ç±»ï¼šEuclidean Structureã€Sequence Structureã€Graph Structureã€‚<br />Eucildean dataæ¯”å¦‚å›¾ï¼ŒSequence dataå¦‚æ–‡æœ¬ï¼Œè¿™ç±»æ•°æ®éƒ½æœ‰ä¸€ä¸ªç‰¹ç‚¹ï¼šè§„åˆ™ï¼Œå³æ’åˆ—æ•´é½ï¼›è€Œå›¾ç»“æ„è¿™ç±»éæ¬§å‡ ä½•æ•°æ®ï¼Œæ ·æœ¬æ˜¯ä¸è§„åˆ™çš„ï¼Œæ¯ä¸ªæ ·æœ¬çš„é‚»å±…èŠ‚ç‚¹æ•°é‡éƒ½æ˜¯ä¸åŒçš„ï¼Œå› æ­¤å›¾åƒä¸­çš„å·ç§¯æ“ä½œå°±æ— æ³•åœ¨å›¾ç»“æ„ä¸­åº”ç”¨ã€‚</p><p>é’ˆå¯¹è¾“å…¥çš„æ•°æ®ï¼ŒåŒ…æ‹¬æ–‡æœ¬ã€æ ‘ç­‰ï¼Œæˆ‘ä»¬æ ¹æ®ä¸€å®šçš„è§„åˆ™è‡ªåŠ¨åŒ–æ„é€ æ„å»ºä¸åŒç±»å‹çš„å›¾ï¼Œå¦‚æ— å‘å›¾ã€æœ‰å‘å›¾ã€å¤šå…³ç³»å›¾ã€å¼‚æ„å›¾å¹¶ä½¿ç”¨ç‰¹ç‚¹çš„GNNç»“æ„æ¥è¿›è¡Œå­¦ä¹ ã€‚</p><h3 id="é™æ€å›¾æ„å»º">é™æ€å›¾æ„å»º</h3><p>åˆ©ç”¨è§„åˆ™æˆ–ç°æœ‰çš„å…³ç³»è§£æå·¥å…·åœ¨æ–‡æœ¬é¢„å¤„ç†æ—¶æ„é€ å›¾ç»“æ„ï¼Œå¸¸è§çš„é™æ€å›¾æ„å»ºæœ‰ï¼š</p><ul><li><p>Dependency Graph<br />ä¾èµ–å›¾å¯ä»¥ç”¨äºæ•æ‰ç»™å®šå¥å­ä¸­ä¸¤ä¸ªä¸»è¯­ä¹‹é—´çš„å…³ç³»ã€‚å¯¹äºç»™å®šçš„å¥å­ï¼Œå¯ä»¥å€ŸåŠ©ç°æœ‰çš„è§£æå·¥å…·åŒ…å¾—åˆ°dependency parsing treeï¼Œè€ŒåæŠ½å–å‡ºä¾èµ–å…³ç³»ï¼Œæ„å»ºdependency graph</p></li><li><p>Constituency Graph<br />è¯­è¨€å­¦ä¸­constituency relationæŒ‡ç¬¦åˆçŸ­è¯­ç»“æ„è¯­æ³•çš„å…³ç³»ï¼Œæ¯”å¦‚ä¸»è¯­NPå’Œè°“è¯­VPçš„å…³ç³» <img src="/images/pd3_1.png" /></p></li><li><p>Information Extraction Graph<br />IE GraphæŠ½å–å‡ºæ–‡æœ¬ä¸­è·¨è¶Šä¸åŒå¥çš„ç»“æ„åŒ–çš„ä¿¡æ¯ã€‚æ„å»ºIEå›¾é¦–å…ˆè¦æå–ä¸‰å…ƒç»„ï¼Œè€Œåé€šè¿‡ä¸åŒä¸‰å…ƒç»„é—´å…±åŒå‚æ•°æ¥ç¡®å®šç›¸åŒå«ä¹‰çš„å®ä½“è¿›è¡Œåˆå¹¶ï¼Œä»è€Œå‡å°‘èŠ‚ç‚¹çš„æ•°é‡ï¼Œæ¶ˆé™¤æ¨¡ç³Šæ€§ã€‚ <img src="/images/pd3_2.png" /></p></li><li><p>Knowledge Graph<br />çŸ¥è¯†å›¾è°±èƒ½æ•è·å®ä½“ä»¥åŠå…³ç³»ï¼Œè¢«å¹¿æ³›ç”¨äºæ¨ç†ã€å…³ç³»æŠ½å–ç­‰ä»»åŠ¡ä¸­ã€‚çŸ¥è¯†å›¾è°±å¯ä»¥ä½œä¸ºæ–‡æœ¬åˆ°embeddingä¹‹é—´çš„ä¸€ä¸ªç²¾ç»ƒä¸”å¯è§£é‡Šçš„ä¸­é—´è¡¨ç¤ºã€‚KGå¯ä»¥è¡¨ç¤ºä¸º <span class="math inline">\(\mathcal{G}(\mathcal{V}, \mathcal{E})\)</span>ï¼Œç”±ä¸‰å…ƒç»„<span class="math inline">\(\left(e_{1}, r e l, e_{2}\right)\)</span>ã€‚KGåœ¨ä¸åŒçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­èµ·ä¸åŒçš„ä½œç”¨ï¼Œå¦‚æœºå™¨ç¿»è¯‘å¯ä»¥ç”¨äºæ•°æ®å¢å¼ºï¼Œé˜…è¯»ç†è§£ä¸­ç”¨äºæ„å»ºå­å›¾ã€‚ <img src="/images/pd3_3.png" /></p></li><li><p>Co-occurrence Graph</p></li></ul><p>å…±ç°å…³ç³»æè¿°äº†ä¸¤ä¸ªè¯åœ¨å›ºå®šå¤§å°çš„ä¸Šä¸‹æ–‡çª—å£å†…å…±ç°çš„é¢‘ç‡ï¼Œè€Œåå•è¯å’Œè¯ä¸è¯é—´å…±ç°é¢‘ç‡æ„å»ºå›¾ã€‚<br />é™¤äº†ä¸Šè¿°å‡ ç±»å›¾æ„å»ºæ–¹æ³•å¤–ï¼Œé’ˆå¯¹å…·ä½“çš„ä»»åŠ¡è¿˜æœ‰å¾ˆå¤šä¸åŒçš„å›¾æ„é€ æ–¹æ³•ã€‚</p><h3 id="åŠ¨æ€å›¾æ„å»º">åŠ¨æ€å›¾æ„å»º</h3><p>é™æ€å›¾å¯ä»¥å°†æ•°æ®çš„éƒ¨åˆ†å…ˆéªŒçŸ¥è¯†ç¼–ç åˆ°å›¾ä¸­ï¼Œä½†æ˜¯è¿™éœ€è¦å¤§é‡çš„äººåŠ›è¯•éªŒä»¥åŠé¢†åŸŸä¸“ä¸šçŸ¥è¯†ï¼Œä¸”å®¹æ˜“åŒ…å«å™ªå£°ã€‚å¦å¤–ï¼Œé™æ€å›¾çš„æ„å»ºæ˜¯åŸºäºæ„å»ºè€…è‡ªèº«çš„ç»éªŒï¼Œå¾—åˆ°çš„å¹¶ä¸ä¸€å®šæ˜¯å¯¹äºæŸä¸€ä¸‹æ¸¸ä»»åŠ¡æœ€ä¼˜çš„å›¾ã€‚<br />è€ŒåŠ¨æ€å›¾åˆ™æ˜¯åŠ¨æ€å­¦ä¹ å›¾ç»“æ„ï¼ˆåŠ æƒé‚»æ¥çŸ©é˜µï¼‰ï¼Œå›¾æ„é€ æ¨¡å—å’Œåç»­å›¾è¡¨ç¤ºå­¦ä¹ ä¸€èµ·é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡è”åˆä¼˜åŒ–ã€‚å›¾ç»“æ„å­¦ä¹ ä¹Ÿæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸç ”ç©¶çš„çƒ­ç‚¹é—®é¢˜ <img src="/images/pd3_4.png" alt="åŠ¨æ€å›¾æ„å»ºæ–¹æ³•" /></p><p><strong>Graph similarity metric learning</strong><br />å›¾ç»“æ„å­¦ä¹ å¯ä»¥è½¬åŒ–ä¸ºèŠ‚ç‚¹ç›¸ä¼¼åº¦åº¦é‡é—®é¢˜ (ç›¸ä¼¼åº¦çŸ©é˜µ<span class="math inline">\(S\)</span>)ï¼Œå¯¹äºç›¸ä¼¼åº¦åº¦é‡å‡½æ•°ï¼Œå¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼š</p><ul><li>åŸºäºèŠ‚ç‚¹åµŒå…¥çš„ç›¸ä¼¼åº¦åº¦é‡å­¦ä¹ </li><li>åŸºäºç»“æ„æ„ŸçŸ¥çš„ç›¸ä¼¼åº¦åº¦é‡å­¦ä¹ </li></ul><p><em>åŸºäºèŠ‚ç‚¹åµŒå…¥çš„ç›¸ä¼¼åº¦å‡½æ•°</em>é€šè¿‡è®¡ç®—åµŒå…¥ç©ºé—´ä¸­èŠ‚ç‚¹çš„æˆå¯¹ç›¸ä¼¼åº¦æ¥å­¦ä¹ åŠ æƒé‚»æ¥çŸ©é˜µã€‚å¸¸è§çš„åº¦é‡å‡½æ•°åŒ…æ‹¬åŸºäºæ³¨æ„åŠ›çš„åº¦é‡å‡½æ•°å’ŒåŸºäºä½™å¼¦çš„åº¦é‡å‡½æ•°ã€‚ <span class="math display">\[S_{i, j}=\operatorname{ReLU}\left(\vec{W} \vec{v}_{i}\right)^{T} \operatorname{ReLU}\left(\vec{W} \vec{v}_{j}\right)\]</span> ä¸Šå¼ä¸ºåŸºäºæ³¨æ„åŠ›çš„åº¦é‡å‡½æ•°ï¼Œ<span class="math inline">\(\vec{W}\)</span>ä¸ºå¯å­¦ä¹ çš„æƒé‡ã€‚ç±»ä¼¼çš„ï¼ŒåŸºäºcosineçš„åº¦é‡å‡½æ•°ä¸ºï¼š <span class="math display">\[\begin{aligned}S_{i, j}^{p} &amp;=\cos \left(\vec{w}_{p} \odot \vec{v}_{i}, \vec{w}_{p} \odot \vec{v}_{j}\right) \\S_{i, j} &amp;=\frac{1}{m} \sum_{p=1}^{m} S_{i j}^{p}\end{aligned}\]</span></p><p><em>åŸºäºç»“æ„æ„ŸçŸ¥çš„ç›¸ä¼¼æ€§å‡½æ•°</em>åœ¨èŠ‚ç‚¹ä¿¡æ¯ä¹‹å¤–è¿˜è€ƒè™‘äº†è¾¹çš„ä¿¡æ¯ï¼Œå¦‚ <span class="math display">\[S_{i, j}^{l}=\operatorname{softmax}\left(\vec{u}^{T} \tanh \left(\vec{W}\left[\vec{h}_{i}^{l}, \vec{h}_{j}^{l}, \vec{v}_{i}, \vec{v}_{j}, \vec{e}_{i, j}\right]\right)\right)\]</span> å…¶ä¸­<span class="math inline">\(\vec{v}_{i}\)</span> ä»£è¡¨èŠ‚ç‚¹içš„embeddingï¼Œ <span class="math inline">\(i\vec{e}_{i, j}\)</span> ä»£è¡¨è¾¹çš„embedding $ _{i}^{l}$ ä»£è¡¨èŠ‚ç‚¹iåœ¨GNNä¸­ç¬¬iå±‚çš„embeddingï¼Œ <span class="math inline">\(\vec{u}\)</span>å’Œ<span class="math inline">\(\vec{W}\)</span> æ˜¯å¯è®­ç»ƒçš„æƒé‡ã€‚</p><p><strong>Graph sparsification</strong><br />ç°å®ä¸–ç•Œä¸­å¤§å¤šæ•°çš„å›¾éƒ½æ˜¯ç¨€ç–å›¾ï¼Œè€Œé€šè¿‡ç›¸ä¼¼åº¦åº¦é‡å‡½æ•°ä¼šå¾—åˆ°ä»»æ„ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„è¾¹ï¼Œæœ€ç»ˆç”Ÿæˆä¸€ä¸ªå…¨è¿é€šå›¾ï¼Œè¿™ä¼šæå¤§å¢å¤§å¼€é”€ï¼Œå¹¶å¼•å…¥å™ªå£°ï¼Œå› æ­¤éœ€è¦è¿›è¡Œå›¾ç¨€ç–åŒ–å¤„ç†ã€‚å¸¸ç”¨çš„æ–¹æ³•åŒ…æ‹¬å–kä¸ªç›¸ä¼¼åº¦æœ€é«˜çš„é‚»èŠ‚ç‚¹ï¼Œæˆ–è€…ç»™èŠ‚ç‚¹é—´çš„ç›¸ä¼¼åº¦è®¾å®šä¸€ä¸ªé˜ˆå€¼ã€‚</p><p>å¦å¤–ï¼Œé™æ€å›¾å’ŒåŠ¨æ€å›¾ä¹Ÿå¯ä»¥ç»“åˆèµ·æ¥ï¼Œæ—¢å¯ä»¥åŠ é€Ÿè®­ç»ƒï¼Œæé«˜ç¨³å®šæ€§ï¼Œä¹Ÿèƒ½æé«˜ä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç° <span class="math display">\[\widetilde{A}=\lambda L^{(0)}+(1-\lambda) \mathrm{f}(A)\]</span> ä¸Šå¼ä¸­<span class="math inline">\(L^{(0)}\)</span>è¡¨ç¤ºé™æ€å›¾ç»“æ„ï¼Œ<span class="math inline">\(\mathrm{f}(A)\)</span>è¡¨ç¤ºå¯å­¦ä¹ çš„åŠ¨æ€å›¾ç»“æ„ã€‚</p><h2 id="å›¾è¡¨ç¤ºå­¦ä¹ ">å›¾è¡¨ç¤ºå­¦ä¹ </h2><p>ç”±äºå›¾çš„ç±»å‹å¤šç§å¤šæ ·ï¼Œå¦‚åŒæ„å›¾ã€å¼‚æ„å›¾ã€å¤šå…³ç³»å›¾ç­‰ç­‰ï¼Œè¿™äº›ä¸åŒçš„å›¾ä¸Šè¿›è¡Œå›¾è¡¨ç¤ºå­¦ä¹ çš„å…·ä½“æ¨¡å‹æˆ–æœ‰å‡ºå…¥ï¼Œä½†æ€»ä½“çš„æ­¥éª¤å’Œæ€è·¯åŸºæœ¬ç±»ä¼¼ã€‚ä¸‹é¢ä»‹ç»çš„å›¾è¡¨ç¤ºå­¦ä¹ æ–¹æ³•æ˜¯åŸºäºåŒæ„å›¾ï¼Œä¸”èŠ‚ç‚¹ä¸èŠ‚ç‚¹ä¹‹é—´ä»…æœ‰ä¸€æ¡æ— å‘è¾¹ã€‚</p><h3 id="basic-gnn">Basic GNN</h3><p>å›¾ç¥ç»ç½‘ç»œå¯¹å›¾ä¸­çš„èŠ‚ç‚¹è¿›è¡Œembeddingï¼Œå¹¶æ ¹æ®éœ€æ±‚ç»™å‡ºæœ€ç»ˆçš„node embeddingæˆ–graph embeddingã€‚å›¾ç¥ç»ç½‘ç»œçš„ç‰¹å¾ä¼ æ’­æ€»ä½“å¯ä»¥åˆ†æˆä¸¤ä¸ªæ­¥éª¤ï¼ŒåŒ…æ‹¬èšåˆ-Aggregationå’Œæ›´æ–°-Updateã€‚ <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\operatorname{AGGREGATE}^{(k)}\left(\left\{\mathbf{h}_{v}^{(k)}, \forall v \in \mathcal{N}(u)\right\}\right)\]</span> <span class="math display">\[\operatorname{UPDATE}\left(\mathbf{h}_{u}, \mathbf{m}_{\mathcal{N}(u)}\right)=\sigma\left(\mathbf{W}_{\text {self }} \mathbf{h}_{u}+\mathbf{W}_{\operatorname{neigh}} \mathbf{m}_{\mathcal{N}(u)}\right)\]</span></p><p>å…¶ä¸­<span class="math inline">\(\mathcal{N}(u)\)</span>è¡¨ç¤ºèŠ‚ç‚¹<span class="math inline">\(u\)</span>çš„é‚»èŠ‚ç‚¹ï¼Œ<span class="math inline">\(\mathbf{h}_{u}\)</span>åˆ™ä»£è¡¨èŠ‚ç‚¹ç‰¹å¾ï¼Œ<span class="math inline">\(\mathbf{W}\)</span>ä¸ºå¯å­¦ä¹ çš„æƒé‡çŸ©é˜µã€‚</p><h3 id="aggregation">Aggregation</h3><p>èšåˆæ“ä½œå°†èŠ‚ç‚¹çš„é‚»èŠ‚ç‚¹ç‰¹å¾è¿›è¡Œæ±‡æ€»ï¼Œå¸¸ç”¨çš„æ–¹æ³•åŒ…æ‹¬ï¼š</p><ul><li><p>Normalizationï¼šæœ€åŸºæœ¬çš„èšåˆæ–¹æ³•å°±æ˜¯å¯¹é‚»èŠ‚ç‚¹embeddingæ±‚å¹³å‡ï¼Œå¹¶é’ˆå¯¹èŠ‚ç‚¹çš„åº¦è¿›è¡Œå½’ä¸€åŒ– <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\frac{\sum_{v \in \mathcal{N}(u)} \mathbf{h}_{v}}{|\mathcal{N}(u)|}\]</span></p></li><li>Poolingï¼šåŸºäºMLPè¿™ç±»çš„ç½®æ¢ä¸å˜(permutation invariant)ç½‘ç»œè¿›è¡Œèšåˆï¼Œé€šç”¨çš„pooling aggregatorå¯ä»¥è¡¨ç¤ºä¸ºï¼š <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\operatorname{MLP}_{\theta}\left(\sum_{v \in N(u)} \operatorname{MLP}_{\phi}\left(\mathbf{h}_{v}\right)\right)\]</span> å¦ä¸€ç§Janossy poolingåˆ™æ˜¯èµ‹äºˆé‚»èŠ‚ç‚¹ä¸€ä¸ªæ¬¡åºï¼Œå¹¶ä½¿ç”¨å¯¹äºæ—¶åºæ•æ„Ÿçš„å‡½æ•°è¿›è¡Œèšåˆ <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\operatorname{MLP}_{\theta}\left(\frac{1}{|\Pi|} \sum_{\pi \in \Pi} \rho_{\phi}\left(\mathbf{h}_{v_{1}}, \mathbf{h}_{v_{2}}, \ldots, \mathbf{h}_{v_{|\mathcal{N}(u)|}}\right)_{\pi_{i}}\right)\]</span></li><li><p>Attentionï¼š å¯¹é‚»èŠ‚ç‚¹åˆ†é…ä¸åŒçš„æƒé‡ï¼Œæƒé‡å¯ä»¥åŸºäºé‚»èŠ‚ç‚¹çš„embeddingï¼Œä¹Ÿå¯ä»¥åŸºäºè¾¹çš„æƒå€¼ <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\sum_{v \in \mathcal{N}(u)} \alpha_{u, v} \mathbf{h}_{v}\]</span> <span class="math display">\[\alpha_{u, v}=\frac{\exp \left(\mathbf{a}^{\top}\left[\mathbf{W h}_{u} \oplus \mathbf{W h}_{v}\right]\right)}{\sum_{v^{\prime} \in \mathcal{N}(u)} \exp \left(\mathbf{a}^{\top}\left[\mathbf{W h}_{u} \oplus \mathbf{W h}_{v^{\prime}}\right]\right)}\]</span></p></li></ul><h3 id="updates">Updates</h3><p>åœ¨ç»è¿‡å¤šå±‚å›¾ç¥ç»ç½‘ç»œåï¼ŒæŸäº›èŠ‚ç‚¹è‡ªèº«çš„ç‰¹æ€§ä¼šå› ä¸ºä¸æ–­èšåˆé‚»èŠ‚ç‚¹çš„ä¿¡æ¯è€Œæ·¡åŒ–æˆ–è¢«æŠ¹å»ï¼Œè¿™å°±å¯¼è‡´æ·±åº¦å›¾ç¥ç»ç½‘ç»œçš„over-smoothingé—®é¢˜ã€‚<br />ä¸ºç¼“è§£over-smoothingé—®é¢˜çš„ä¸€äº›æŠ€å·§ï¼Œæ¯”å¦‚Concatenationå’ŒSkip-Connectionsç­‰åœ¨èŠ‚ç‚¹ä¿¡æ¯èšåˆåçš„æ›´æ–°æ“ä½œå…¥æ‰‹ã€‚ <span class="math display">\[\text { UPDATE }_{\text {concat }}\left(\mathbf{h}_{u}, \mathbf{m}_{\mathcal{N}(u)}\right)=\left[\text { UPDATE }_{\text {base }}\left(\mathbf{h}_{u}, \mathbf{m}_{\mathcal{N}(u)}\right) \oplus \mathbf{h}_{u}\right]\]</span></p><h3 id="graph-convolutional-networks-gcn">Graph Convolutional Networks (GCN)</h3><p>å›¾å·ç§¯å°±å¦‚åŒCVä¸­çš„å·ç§¯ï¼Œè¢«æå‡ºåå—åˆ°äº†å¹¿æ³›å…³æ³¨å’Œç ”ç©¶ã€‚æ¬§æ°ç©ºé—´ä¸­çš„ç¦»æ•£å·ç§¯æˆ‘ä»¬å¾ˆå¥½ç†è§£ï¼Œè€Œå¯¹äºéæ¬§æ•°æ®ä¸­çš„å·ç§¯ï¼Œå®ƒçš„æå‡ºæµç¨‹å¯ä»¥æ¦‚æ‹¬ä¸ºï¼šå›¾ä¿¡å·å¤„ç†GSPå­¦è€…æå‡ºå›¾çš„Fourier Transformationï¼Œè¿›è€Œå¾—åˆ°Graph convolutionï¼Œä»è€Œæ‹“å±•åˆ°ç¥ç»ç½‘ç»œçš„å›¾å·ç§¯ç½‘ç»œã€‚</p><p>å›¾çš„å·ç§¯å®šä¹‰åœ¨spectral domainï¼Œç›¸åº”çš„é‚»æ¥çŸ©é˜µ<span class="math inline">\(A\)</span>ç”¨å›¾çš„Laplacian çŸ©é˜µ<span class="math inline">\(L\)</span>æ›¿ä»£ã€‚<span class="math inline">\(L = D - A\)</span>ï¼Œ<span class="math inline">\(D\)</span>ä¸ºåº¦çŸ©é˜µã€‚æŠŠä¼ ç»Ÿçš„å‚…é‡Œå¶å˜æ¢ä»¥åŠå·ç§¯è¿ç§»åˆ°Graphä¸Š, æ ¸å¿ƒå·¥ä½œå°±æ˜¯æŠŠæ‹‰æ™®æ‹‰æ–¯ç®—å­çš„ç‰¹å¾å‡½æ•° <span class="math inline">\(e^{-i \omega t}\)</span> å˜ä¸ºGraphå¯¹åº”çš„æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ç‰¹å¾å‘é‡ã€‚è¿™å…¶ä¸­å…·ä½“çš„æ¨å¯¼è¿‡ç¨‹åœ¨æ­¤ä¸å†èµ˜è¿°ã€‚åŸºæœ¬çš„GCNä¸­ç¬¬kå±‚å¯ä»¥å†™ä¸ºä¸‹å¼ï¼š <span class="math display">\[\mathbf{H}^{(k)}=\sigma\left(\tilde{\mathbf{A}} \mathbf{H}^{(k-1)} \mathbf{W}^{(k)}\right)\]</span> å…¶ä¸­<span class="math inline">\(\tilde{\mathbf{A}}=(\mathbf{D}+\mathbf{I})^{-\frac{1}{2}}(\mathbf{I}+\mathbf{A})(\mathbf{D}+\mathbf{I})^{-\frac{1}{2}}\)</span>ï¼Œæ˜¯æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µçš„ä¸€ä¸ªå˜å½¢å½¢å¼ã€‚</p><h3 id="graphsage">GraphSAGE</h3><p>GraphSAGEè¿™ä¸€å›¾æ¨¡å‹æ˜¯å½’çº³å¼ (inductive) å­¦ä¹ ã€‚ä¸åŒäºä¹‹å‰çš„transduceræ¨¡å‹ï¼ŒGraphSAGEçš„ç›®æ ‡ä¸æ˜¯å­¦ä¹ åˆ°æ¯ä¸ªèŠ‚ç‚¹çš„embeddingï¼Œè€Œæ˜¯å­¦ä¹ ç”Ÿæˆembeddingçš„èšåˆå‡½æ•°ã€‚ <img src="/images/pd3_5.png" /> æ•´ä¸ªæ¡†æ¶å¦‚ä¸Šå›¾æ‰€ç¤ºï¼ŒåŒ…æ‹¬é‡‡æ ·ã€èšåˆã€é¢„æµ‹ä¸‰ä¸ªæ­¥éª¤ã€‚åœ¨é‡‡æ ·æ—¶ä¼šé€‰æ‹©æ’å®šæ•°é‡çš„é‚»èŠ‚ç‚¹ï¼Œä¸”ä¸ä»…ä»…é€‰æ‹©1-hopçš„èŠ‚ç‚¹ï¼Œè€Œæ˜¯è€ƒè™‘multi-hopã€‚</p><h2 id="åŸºäºå›¾çš„encoder-decoderæ¨¡å‹">åŸºäºå›¾çš„Encoder-Decoderæ¨¡å‹</h2><p>Encoder-Decoderæ˜¯æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­éå¸¸å¸¸è§çš„æ¶æ„ï¼Œå› æ­¤åˆ°äº†å›¾æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œå›¾åˆ°æ ‘ã€graph-graphç­‰æ¨¡å‹ä¹Ÿåº”è¿è€Œç”Ÿã€‚</p><h3 id="graph-to-sequence-model">Graph-to-Sequence Model</h3><p>è¿™ç±»æ¨¡å‹é€šå¸¸ç”¨GNNä½œä¸ºEncoderï¼ŒRNN/Transformerä½œä¸ºDecoderã€‚æ­¤å¤–ï¼Œè¿™ç±»æ¨¡å‹ä¸­å¤šä½¿ç”¨CNNè¿›è¡ŒèŠ‚ç‚¹ç‰¹å¾åˆå§‹åŒ–ï¼Œç”¨äºæ•æ‰GNNä¸æ•æ„Ÿçš„è¿ç»­è¯çš„æ½œåœ¨ä¿¡æ¯ã€‚è¿™ç±»æ¨¡å‹åœ¨å¤šå…³ç³»å›¾æˆ–å¼‚æ„å›¾çš„å¤„ç†ä¸Šæœ‰æ‰€å±€é™ã€‚</p><h3 id="graph-to-tree-model">Graph-to-Tree Model</h3><p>ç±»ä¼¼ç«¯åˆ°ç«¯çš„æ¨¡å‹ï¼Œåœ¨NLPä»»åŠ¡ä¸­ï¼Œæ ‘ä¹Ÿå…·æœ‰å¾ˆå¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ã€‚ç”±äºæ ‘å¹¿ä¹‰ä¸Šæ¥è®²ä¹Ÿæ˜¯ä¸€ç§å›¾ï¼Œå› æ­¤Graph-Treeè¿™ç±»æ¨¡å‹çš„æ ¸å¿ƒåœ¨äºå€ŸåŠ©self-attentionè¿›è¡Œè·å–å±€éƒ¨é‚»èŠ‚ç‚¹çš„æƒé‡ï¼Œå†ç”±decoder ç”ŸæˆåŒ…å«è¯­ä¹‰çš„treeç»“æ„ã€‚è¿™ç±»æ¨¡å‹çš„åº”ç”¨æ¯”å¦‚è¯­ä¹‰è§£æã€æ•°å­¦åº”ç”¨é—®é¢˜ï¼ˆæ¨¡å‹è¾“å‡ºä¸ºç”±æ ‘æ¥è¡¨ç¤ºçš„æ–¹ç¨‹ï¼‰ã€‚</p><h3 id="graph-to-graph-model">Graph-to-Graph Model</h3><figure><img src="/images/pd3_6.png" alt="å›¾çš„ç”Ÿæˆå¼æ¨¡å‹(VAE)" /><figcaption>å›¾çš„ç”Ÿæˆå¼æ¨¡å‹(VAE)</figcaption></figure><h2 id="è¡¥å……">è¡¥å……</h2><h3 id="å›¾ç¥ç»ç½‘ç»œåœ¨nlpä¸­çš„ä¸‹æ¸¸ä»»åŠ¡">å›¾ç¥ç»ç½‘ç»œåœ¨NLPä¸­çš„ä¸‹æ¸¸ä»»åŠ¡</h3><p>å·²æœ‰çš„åŸºäºå›¾ç›¸å…³æŠ€æœ¯çš„NLPä»»åŠ¡åŒ…æ‹¬è‡ªç„¶è¯­è¨€ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†ç±»ã€æ–‡æœ¬åˆ†ç±»ã€çŸ¥è¯†å›¾è°±è¡¥å…¨ã€ä¿¡æ¯æŠ½å–ï¼ˆå‘½åå®ä½“è¯†åˆ«ã€å…³ç³»æŠ½å–ï¼‰ã€è‡ªç„¶è¯­è¨€æ¨ç†ã€è§£æ•°å­¦é—®é¢˜ï¼ˆæ–‡æœ¬ï¼‰ç­‰</p><h3 id="å…³äºå›¾çš„semi-supervised">å…³äºå›¾çš„semi-supervised</h3><p>å¯¹äºç›‘ç£å­¦ä¹ ï¼Œå¦‚ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ ·æœ¬éœ€è¦æ»¡è¶³i.i.d assumptionï¼šæ ·æœ¬ä¹‹é—´æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼ˆä¸ç„¶è¿˜éœ€è¦å»ºæ¨¡æ ·æœ¬ä¹‹é—´çš„è”ç³»ï¼‰ã€‚ç„¶è€Œåœ¨å›¾ç»“æ„ä¸Šåšè¯¸å¦‚èŠ‚ç‚¹åˆ†ç±»é—®é¢˜æ—¶ï¼ŒèŠ‚ç‚¹ä¹‹é—´ç›¸äº’è”ç³»ï¼Œä¸”è¿™äº›è”ç³»åœ¨èŠ‚ç‚¹åˆ†ç±»ä¸­èµ·åˆ°äº†é‡è¦çš„ä½œç”¨ã€‚å› æ­¤åŸºäºå›¾çš„å¾ˆå¤šæ·±åº¦å­¦ä¹ æ˜¯semi-supervisedã€‚è¿™æ„å‘³ç€åœ¨è®­ç»ƒå›¾æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†æµ‹è¯•èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œä½†ä¸åŒ…æ‹¬labelã€‚</p><h3 id="æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¿ç§»">æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¿ç§»</h3><p>è¿‘å‡ å¹´éšç€å›¾ç¥ç»ç½‘ç»œçš„å…´èµ·ï¼Œè®¸å¤šäººéƒ½æ¶Œå‘è¿™å—å¤„å¥³åœ°ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­çš„ä¸€äº›ç»å…¸æ€æƒ³å’Œæ¨¡å‹ä¹Ÿè¢«è¿ç§»åˆ°å›¾ç›¸å…³çš„æ¨¡å‹ä¸­ï¼Œæ¯”å¦‚åŸºäºself-attentionçš„GATã€GraphGANã€Graph Transformerç­‰ã€‚å¦å¤–ï¼Œåœ¨NLPè½åœ°çš„ç»å…¸æœç´¢ã€å¹¿å‘Šã€æ¨èç®—æ³•ä¸­ï¼Œå›¾ç¥ç»ç½‘ç»œä¹Ÿè¢«å¹¿æ³›åº”ç”¨ã€‚<br /><a href="https://arxiv.org/abs/1711.08267">GraphGAN: Graph Representation Learning with Generative Adversarial Nets</a><br /><a href="https://arxiv.org/pdf/1911.07470.pdf">Graph Transformer for Graph-to-Sequence Learning</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;æˆªæ­¢2021å¹´åˆï¼Œå…³äºç°æœ‰çš„å›¾ç¥ç»ç½‘ç»œåº”ç”¨åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ç»¼è¿°&lt;br /&gt;
link: &lt;a href=&quot;https://arxiv.org/abs/2106.06090&quot;&gt;Graph Neural Networks for Natural Language Proce</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Vocabulary Learning via Optimal Transport for Neural Machine Translation [ACL2021]</title>
    <link href="http://example.com/2021/10/10/pd2/"/>
    <id>http://example.com/2021/10/10/pd2/</id>
    <published>2021-10-10T10:22:13.000Z</published>
    <updated>2021-10-26T15:36:20.914Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2012.15671.pdf">Vocabulary Learning via Optimal Transport for Neural Machine Translation</a><br />ACL2021 Best paper Code: <a href="https://github.com/Jingjing-NLP/VOLT">link</a></p><p>è¿™ç¯‡ä¹Ÿå°±æ˜¯è¢«ICLR2021æ‹’äº†åè¢«è¯„ä¸ºACL2021 best paperçš„æ–‡ç« ï¼Œæ¥è‡ªå­—èŠ‚è·³åŠ¨çš„AI Labã€‚</p><h2 id="related-work">Related Work</h2><h3 id="subword-model">Subword model</h3><p>è‹±æ–‡ä¸­ä¼ ç»Ÿåˆ†è¯æ–¹æ³•åŸºäºç©ºæ ¼è¿›è¡Œtokenizationã€‚ä½†è¿™ä¸€æ–¹æ³•é¢ä¸´OOV (Out Of Vocabulary)é—®é¢˜å’ŒåŒä¸€å•è¯çš„ä¸åŒå½¢æ€é€ æˆçš„å†—ä½™ã€‚å› æ­¤å¦‚ä»ŠBERTç­‰æ¨¡å‹å¤šä½¿ç”¨Subwordæ¨¡å‹ï¼Œå®ƒçš„åˆ’åˆ†ç²’åº¦ä»‹äºè¯ä¸å­—ç¬¦ä¹‹é—´ã€‚ä¸»æµçš„(æŒ‡æŸäº›ä¸­æ–‡ç½‘ç«™ä¸Šæœ‰åšå®¢ä»‹ç»çš„ï¼‰Subword modelæœ‰Byte Pair Encoding (BPE), WordPieceå’ŒUnigram Language Modelã€‚</p><h3 id="byte-pair-encodingbpe">Byte-Pair-Encoding(BPE)</h3><p>&quot;Neural machine translation of rare words with subword units.&quot;arXiv preprint arXiv:1508.07909(2015).<br />BPEç®—æ³•è¢«ç”¨äºå¤„ç†NMT (Neural Machine Translation)ä»»åŠ¡ä¸­çš„OOVé—®é¢˜ã€‚<br />BPEæ˜¯ä¸€ç§è‡ªä¸‹è€Œä¸Šçš„å‹ç¼©ç®—æ³•ã€‚å°†å•è¯ä½œä¸ºå•è¯ç‰‡æ®µå¤„ç†ï¼ˆword piecesï¼‰ï¼Œä»¥ä¾¿äºå¤„ç†æœªå‡ºç°å•è¯ã€‚</p><blockquote><p>we adopt BPE generated tokens as the token candidates.</p></blockquote><p>è®ºæ–‡æå‡ºçš„ç®—æ³•è¦å…ˆç”¨BPE...</p><h2 id="æ¦‚è¦">æ¦‚è¦</h2><p>æœºå™¨ç¿»è¯‘ä¸­ï¼Œtoken vocabularyå¯¹æœ€ç»ˆç»“æœä¼šäº§ç”Ÿå¾ˆå¤§çš„å½±å“ã€‚è®ºæ–‡ç ”ç©¶äº†è¯è¡¨çš„è¯„ä»·æŒ‡æ ‡ä»¥åŠå¦‚ä½•ä¸é€šè¿‡è®­ç»ƒç›´æ¥æ‰¾åˆ°æœ€ä¼˜çš„è¯è¡¨ã€‚æ–‡ç« çš„ä¸»è¦å†…å®¹åŒ…æ‹¬ 1. ä»ä¿¡æ¯è®ºè§’åº¦åˆ†æè¯è¡¨çš„ä½œç”¨ 2.å€ŸåŠ©Optimal transportæ¥æ‰¾åˆ°æœ€ä½³tokenè¯å…¸ 3. æ›´å°çš„è¯è¡¨butæ›´é«˜çš„BLEUã€‚</p><h2 id="intro">Intro</h2><p>è¯æ±‡é‡ï¼ˆvocabulary sizeï¼‰ä¼šå½±å“æœºå™¨ç¿»è¯‘ä»»åŠ¡çš„ç»©æ•ˆï¼Œè€Œé€šè¿‡éå†æœç´¢æ¥å¯»æ‰¾æœ€ä¼˜çš„è¯æ±‡é‡éœ€è¦æé«˜çš„è®¡ç®—å¼€é”€ï¼Œå› æ­¤ç°æœ‰çš„ç ”ç©¶å¤§å¤šé‡‡ç”¨ç»Ÿä¸€çš„å¤§å°ï¼Œå¦‚30k-40kã€‚BPEé€šè¿‡é€‰æ‹©é¢‘ç‡æœ€é«˜çš„sub-wordsåšä¸ºè¯å…¸çš„tokenä»¥è¿›è¡Œæ•°æ®å‹ç¼©ï¼Œä»¥æ­¤å‡å°‘ç†µã€‚</p><p>è¯­æ–™ç†µéšç€è¯æ±‡é‡çš„å¢åŠ è€Œå‡å°‘ï¼Œæœ‰åˆ©äºæ¨¡å‹å­¦ä¹ ã€‚å¦ä¸€æ–¹é¢ï¼Œè¿‡å¤šçš„å­—ç¬¦ä¼šå¯¼è‡´å­—ç¬¦ç¨€ç–åŒ–ï¼Œè¿™ä¼šæŸå®³æ¨¡å‹å­¦ä¹ ã€‚æœ¬æ–‡é€šè¿‡åŒæ—¶è€ƒè™‘ç†µå’Œè¯æ±‡é‡å¤§å°æ¥æ¢ç´¢è‡ªåŠ¨è¯æ±‡åŒ–ï¼Œéœ€è¦æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„ç›®æ ‡å‡½æ•°æ¥åŒæ—¶ä¼˜åŒ–å®ƒä»¬ã€‚å…¶æ¬¡ï¼Œå‡è®¾ç»™å‡ºäº†é€‚å½“çš„åº¦é‡ï¼Œç”±äºæŒ‡æ•°æœç´¢ç©ºé—´ï¼ˆ<span class="math inline">\(2^N\)</span>)ï¼Œè§£å†³è¿™ç§ç¦»æ•£ä¼˜åŒ–é—®é¢˜ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚</p><p>é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œè®ºæ–‡æå‡ºVOcabulary Learning approach via optimal Transport, VOLTâ€”â€”æœ€ä¼˜ä¼ è¾“çš„è¯æ±‡å­¦ä¹ æ–¹æ³•</p><p>æ€»çš„æ¥è¯´ï¼Œè®ºæ–‡çš„ç›®æ ‡æ˜¯1.å¾—åˆ°â€œç®€æ´è€Œä¸è‡ƒè‚¿â€çš„è¯æ±‡è¡¨ â€”â€” entropy-size trade off 2. ä¼˜åŒ–æœç´¢è¿‡ç¨‹ã€‚</p><h2 id="marginal-utility-of-vocabularization-muv">Marginal Utility of Vocabularization (MUV)</h2><p>å€Ÿç”¨ç»æµå­¦ä¸­çš„è¾¹é™…æ•ˆåº”çš„æ¦‚å¿µï¼Œä»¥è¯æ±‡çš„è¾¹é™…æ•ˆåº”ï¼ˆMUVï¼‰ä½œä¸ºè¡¡é‡æ ‡å‡†ï¼Œ ç„¶åå°†ç›®æ ‡è½¬å‘åœ¨å¯å¤„ç†çš„æ—¶é—´å¤æ‚åº¦ä¸­æœ€å¤§åŒ– MUVã€‚ <img src="/images/pd2_3.png" title="vocabularyçš„è¾¹é™…æ•ˆç›Šï¼ˆæ²¡æœ‰æ˜¾ç¤ºç»™å‡ºMUVï¼‰" /></p><p>åœ¨ç»æµå­¦ä¸­ï¼Œè¾¹é™…æ•ˆåº”ç”¨äºå¹³è¡¡æ”¶ç›Šå’Œæˆæœ¬ï¼Œå› æ­¤è®ºæ–‡ä½¿ç”¨ MUV æ¥å¹³è¡¡ç†µï¼ˆæ”¶ç›Šï¼‰å’Œè¯æ±‡é‡ï¼ˆæˆæœ¬ï¼‰ã€‚ä¹Ÿå°±æ˜¯ä»æˆæœ¬ï¼ˆå¤§å°ï¼‰çš„å¢åŠ ä¸­è·å¾—å¤šå¤§çš„æ”¶ç›Šï¼ˆç†µï¼‰ã€‚</p><p><img src="/images/pd2_1.png" title="MUV ä¸ä¸‰åˆ†ä¹‹äºŒç¿»è¯‘ä»»åŠ¡çš„ä¸‹æ¸¸æ€§èƒ½ç›¸å…³" /></p><h3 id="definition-of-muv">Definition of MUV</h3><p>MUV è¡¨ç¤ºç†µå¯¹å¤§å°çš„è´Ÿå¯¼æ•° <span class="math display">\[\mathcal{M}_{v(k+m)}=\frac{-\left(\mathcal{H}_{v(k+m)}-\mathcal{H}_{v(k)}\right)}{m}\]</span> å…¶ä¸­ <span class="math inline">\(v(k), v(k+m)\)</span> æ˜¯ä¸¤ä¸ªåˆ†åˆ«å¸¦æœ‰ <span class="math inline">\(k\)</span> å’Œ <span class="math inline">\(k+m\)</span> ä¸ªå­—ç¬¦çš„è¯æ±‡ã€‚<span class="math inline">\(\mathcal{H}_{v}\)</span> è¡¨ç¤ºè¯æ±‡è¡¨ <span class="math inline">\(v\)</span> è¯­æ–™åº“çš„æ¨€ï¼Œå®ƒç”±å­—ç¬¦æ¨€çš„æ€»å’Œå®šä¹‰ã€‚ç”¨å­—ç¬¦çš„å¹³å‡é•¿åº¦å¯¹ç†µè¿›è¡Œå½’ä¸€åŒ–æ¥é¿å…å­—ç¬¦é•¿åº¦çš„å½±å“ã€‚æœ€ç»ˆçš„ç†µå®šä¹‰ä¸ºï¼š <span class="math display">\[\mathcal{H}_{v}=-\frac{1}{l_{v}} \sum_{j \in v} P(j) \log P(j)\]</span> <span class="math inline">\(P(i)\)</span> æ˜¯è®­ç»ƒè¯­æ–™åº“ä¸­token <span class="math inline">\(i\)</span> çš„ç›¸å¯¹é¢‘ç‡, <span class="math inline">\(l_{v}\)</span> æ˜¯è¯æ±‡è¡¨ <span class="math inline">\(v\)</span> ä¸­tokençš„å¹³å‡é•¿åº¦ã€‚</p><h3 id="preliminary-results">Preliminary Results</h3><p>ä¸ºäº†éªŒè¯ MUV ä½œä¸ºè¯æ±‡åŒ–è¡¡é‡æ ‡å‡†çš„æœ‰æ•ˆæ€§ï¼Œä½œè€…å¯¹æ¥è‡ª TED çš„ 45 ä¸ªè¯­è¨€å¯¹è¿›è¡Œäº†å®éªŒï¼Œå¹¶è®¡ç®—äº† MUV å’Œ BLEU åˆ†æ•°ä¹‹é—´çš„Spearmanç›¸å…³ç³»æ•°(<span class="math inline">\(\rho\)</span>)ã€‚Spearman å¾—åˆ†ä¸º 0.4ã€‚</p><blockquote><p>We believe that it is a good signal to show MUV matters</p></blockquote><p>æœ‰äº†MUVä½œä¸ºè¯„ä»·æŒ‡æ ‡ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªé€‰æ‹©æ¥è·å¾—æœ€ç»ˆè¯è¡¨ï¼šæœç´¢å’Œå­¦ä¹ ã€‚ä½œè€…è®¤ä¸ºåŸºäºå­¦ä¹ æ˜¯æ›´é«˜æ•ˆçš„ï¼Œå› æ­¤è¿›ä¸€æ­¥æ¢ç´¢äº†ä¸€ç§åŸºäºå­¦ä¹ çš„è§£å†³æ–¹æ¡ˆ VOLTã€‚ï¼ˆå½“ç„¶æœ€ç»ˆå€ŸåŠ©å®éªŒæ¯”è¾ƒäº† MUV-Search å’Œ VOLTçš„æ€§èƒ½ã€‚ï¼‰</p><h2 id="maximizing-muv-via-optimal-transport">Maximizing MUV via Optimal Transport</h2><h3 id="ä¼˜åŒ–é—®é¢˜">ä¼˜åŒ–é—®é¢˜</h3><p>é¦–å…ˆå¼•å…¥ä¸€ä¸ªè¾…åŠ©å˜é‡<span class="math inline">\(S\)</span>ï¼Œ<span class="math inline">\(\boldsymbol{S}=\{k, 2 \cdot k, \ldots,(t-1) \cdot k, \cdots\}\)</span>ã€‚ <span class="math inline">\(S\)</span>æ˜¯ä¸€ä¸ªé€’å¢åºåˆ—ï¼Œå¯¹äºæ¯ä¸ªæ—¶é—´æˆ³tï¼Œ<span class="math inline">\(S[t]\)</span>ä»£è¡¨<strong>ä¸å¤šäº<span class="math inline">\(S[t]\)</span>ä¸ªè¯æ¡çš„è¯è¡¨é›†åˆ</strong>ã€‚å¼•å…¥è¿™ä¸€å˜é‡ï¼Œæ ¹æ®é€’æ¨å…³ç³»æ¥è®¡ç®—ä»»æ„ä¸€ä¸ªè¯è¡¨çš„MUVï¼ˆå€ŸåŠ©å‰ä¸€ä¸ªæ—¶é—´æˆ³s[t-1]ä¸Šçš„è¯è¡¨é€’è¿›è®¡ç®—ï¼‰</p><p><span class="math inline">\(k\)</span>ä»£è¡¨å‰åä¸¤ä¸ªè¯è¡¨<span class="math inline">\(v(t)\)</span>å’Œ<span class="math inline">\(v(t-1)\)</span>ä¹‹é—´çš„å¤§å°å·®ï¼ˆsize gapï¼‰ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°MUVæœ€é«˜çš„<span class="math inline">\(v[t]\)</span> <span class="math display">\[\begin{array}{l}\underset{t}{\arg \max } \underset{v(t-1) \in \mathbb{V}_{\boldsymbol{S}[t-1]}, v(t) \in \mathbb{V}_{S[t]}}{\arg \max } \mathcal{M}_{v(t)}= \\\underset{t}{\arg \max } \underset{v(t-1) \in \mathbb{V}_{\boldsymbol{S}[t-1]}, v(t) \in \mathbb{V}_{S[t]}}{\arg \max }-\frac{1}{k}\left[\mathcal{H}_{v(t)}-\mathcal{H}_{v(t-1)}\right]\end{array}\]</span> <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t-1]}\)</span>å’Œ <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t]}\)</span>è¡¨ç¤ºä¸¤ä¸ªè¯è¡¨çš„é›†åˆï¼Œå…¶ä¸­æ¯ä¸ªè¯è¡¨å¤§å°çš„ä¸Šç•Œä¸º<span class="math inline">\(s[t-1]\)</span>å’Œ<span class="math inline">\(s[t]\)</span></p><blockquote><p>The inner arg max represents that the target is to find the vocabulary from <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t]}\)</span> with the maximum MUV scores. The outer arg max means that the target is to enumerate all timesteps and find the vocabulary with the maximum MUV scores.</p></blockquote><p>éå†tï¼Œéå†<span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t-1]}\)</span>ã€‚<br />ï¼ˆè¯è¡¨è¶Šå¤§ç†µè¶Šå°ï¼‰ä¸Šè¿°å…¬å¼æ„å‘³ç€ä»v(t-1)è¿™ä¸ªè¯è¡¨ï¼Œå¢åŠ iä¸ªè¯/tokensä¹‹åï¼ŒæœŸæœ›æ–°å¾—åˆ°çš„v(t)è¯è¡¨çš„ç†µé™ä½çš„æœ€å¤šã€‚å³ä¸¤ä¸ªè¯è¡¨å¯¹åº”çš„ç†µçš„å·®å€¼è¶Šå¤§è¶Šå¥½ã€‚</p><blockquote><p>Due to exponential search space, we propose to optimize its upper bound: <span class="math display">\[\underset{t}{\arg \max } \frac{1}{k}\left[\underset{v(t) \in \mathbb{V}_{S[t]}}{\arg \max } \mathcal{H}_{v(t)}-\underset{v(t-1) \in \mathbb{V}_{S[t-11}}{\arg \max } \mathcal{H}_{v(t-1)}\right]\]</span></p></blockquote><p>(è®ºæ–‡ArXivä¸Šçš„å‰ä¸€ç‰ˆæœ¬ä¸­å†™çš„è¿˜æ˜¯lower bound...è€Œæœ€æ–°ç‰ˆæ”¾çš„æ˜¯upper bound...)<br />anywayè‡³æ­¤æ•´ä¸ªæ–¹æ³•å¯ä»¥åˆ†æˆä¸¤ä¸ªæ­¥éª¤ï¼š</p><ul><li>æ¯ä¸ªæ—¶é—´æ­¥tä¸Šï¼Œå¯»æ‰¾æœ€ä¼˜çš„è¯è¡¨ï¼ˆæŒ‰ç…§æœ€å¤§åŒ–ç†µæ¥å¯»æ‰¾ï¼‰</li><li>æšä¸¾æ¯ä¸ªæ—¶é—´æ­¥tï¼Œå¹¶è¾“å‡ºæ»¡è¶³ä¸Šä¸€ä¸ªå…¬å¼çš„è¯è¡¨ï¼ˆå¯¹åº”çš„å°±æ˜¯æ—¶é—´æ­¥tçš„â€æœ€ä¼˜è¯è¡¨â€œï¼‰</li></ul><p>step1çš„ç›®æ ‡å°±æ˜¯æœ€å¤§åŒ–ï¼š <span class="math display">\[\underset{v(t) \in \mathbb{V}_{\boldsymbol{S}[t]}}{\arg \max }-\frac{1}{l_{v(t)}} \sum_{j \in v(t)} P(j) \log P(j)\]</span> <span class="math inline">\(l_{v}\)</span>æ˜¯æ¯ä¸ªtokençš„å¹³å‡å­—ç¬¦é•¿åº¦ï¼Œ<span class="math inline">\(P(j)\)</span>æ˜¯token jçš„æ¦‚ç‡ï¼ˆé¢‘ç‡ï¼‰</p><blockquote><p>However, notice that this problem is in general intractable due to the extensive vocabulary size. Therefore, we instead propose a relaxation in the formulation of discrete optimal transport, which can then be solved efficiently via the Sinkhorn algorithm</p></blockquote><p>å€ŸåŠ©æœ€ä¼˜ä¼ è¾“OTçš„æ€æƒ³ï¼Œæ¾å¼›åŸä¼˜åŒ–é—®é¢˜ï¼Œè¿›è€Œç”¨ä¿¡æ¯è®ºä¸­çš„Sinkhorn algorithmæ±‚è§£ã€‚</p><h3 id="optimal-transport-ä¸å¤ªæ‡‚">Optimal Transport ï¼ˆä¸å¤ªæ‡‚ï¼‰</h3><p><img src="/images/pd2_2.png" title="å¯»æ‰¾ä¸€ä¸ªä»â€œcharacteråˆ†å¸ƒã€å•å­—åˆ†å¸ƒâ€åˆ°â€œè¯è¡¨è¯æ¡åˆ†å¸ƒâ€çš„ä¸€ä¸ªæœ€ä¼˜çš„è¿è¾“çŸ©é˜µçš„è¿‡ç¨‹" /></p><ul><li>æ¯ä¸ªtransport matrixå¯¹åº”ä¸€ä¸ªè¯è¡¨ï¼›</li><li>transport matrixå†³å®šæœ‰å¤šå°‘charsè¢«â€œè¿è¾“â€åˆ°tokenå€™é€‰ï¼ˆè¯æ¡å€™é€‰ï¼‰ï¼›</li><li>é•¿åº¦ä¸º0çš„tokensï¼ˆåŒ…å«0ä¸ªchars)ï¼Œä¸ä¼šè¢«å¢åŠ åˆ°è¯è¡¨ã€‚</li></ul><p>ä¸åŒçš„â€è¿è¾“çŸ©é˜µâ€œä¼šå¸¦æ¥ä¸åŒçš„â€è¿è¾“å¼€é”€â€ã€‚è€Œæœ€ä¼˜åŒ–è¿è¾“ï¼ˆè·¯å¾„ï¼‰é—®é¢˜çš„ç›®æ ‡å°±æ˜¯å¯»æ‰¾ä¸€ä¸ªâ€œè¿è¾“çŸ©é˜µâ€œï¼Œä½¿å¾—â€è¿è¾“å¼€é”€â€œ(å³ï¼Œè´Ÿç†µ)æœ€å°åŒ–ã€‚</p><p>ç›®æ ‡å‡½æ•°ï¼š <span class="math display">\[\begin{array}{c}\min _{v \in \mathbb{V}_{S[t]}} \frac{1}{l_{v}} \sum_{j \in v} P(j) \log P(j) \\\text { s.t. } \quad P(j)=\frac{\operatorname{Token}(j)}{\sum_{j \in v} \operatorname{Token}(j)}, l_{v}=\frac{\sum_{j \in v} \operatorname{len}(j)}{|v|}\end{array}\]</span></p><p>è¿‘ä¼¼(obtain a tractable lower bound of entropy) - å¯å‘å¼è§„åˆ™(æœ€é•¿è¯æ¡åŒ¹é…åŸåˆ™) - å˜æ¢ä¸ºä¸¤éƒ¨åˆ†æŸå¤±</p><p>å¤æ‚çš„æ¨å¯¼åå¾—åˆ°ï¼š <span class="math display">\[\min _{\boldsymbol{P} \in \mathbb{R}^{m \times n}}\langle\boldsymbol{P}, \boldsymbol{D}\rangle-\gamma H(\boldsymbol{P})\]</span></p><p><span class="math display">\[\boldsymbol{D}(j, i)=\left\{\begin{array}{ll}-\log P(i \mid j)=+\infty, &amp; \text { if } i \notin j \\-\log P(i \mid j)=-\log \frac{1}{\operatorname{len}(j)}, &amp; \text { otherwise }\end{array}\right.\]</span></p><p><img src="/images/pd2_4.png" title="ç®—æ³•ï¼ˆä¸å¤ªå¤æ‚ï¼‰" /></p><h2 id="å®éªŒ">å®éªŒ</h2><p>3ä¸ªæ•°æ®é›†ä¸ŠNMTä»»åŠ¡çš„BLEUæ¯”è¾ƒï¼ˆåŒè¯­è¯­æ–™ã€å¤šè¯­è¯­æ–™ç­‰ï¼‰<br />VOLTå¯¹æ¯”BPEã€MUV searchæ›´åŠ é«˜æ•ˆ</p><p>æœ€åï¼Œå…¨è®ºæ–‡çš„æ ¸å¿ƒåº”è¯¥æ˜¯MUVçš„æå‡ºä»¥åŠç”¨OTè¿›è¡Œä¼˜åŒ–è¿™ä¸€trickï¼Œå®éªŒç»“æœä¹Ÿæ¯”è¾ƒsolidï¼Œè™½ç„¶æœ€ç»ˆçš„ç®—æ³•å¹¶ä¸å¤æ‚ï¼Œä½†æ˜¯OTéƒ¨åˆ†Sinkhornç®—æ³•éœ€è¦è¾ƒå¼ºçš„ä¿¡æ¯è®ºèƒŒæ™¯ï¼Œæœ¬CSä¸“ä¸šçœ‹äº†åŠå¹´ä»æ˜¯ä¸€è„¸æ‡µã€‚<br />æŒ‡è·¯ä¸€ç¯‡ä»‹ç»Sinkhornç®—æ³•çš„é“¾æ¥ï¼š <a href="https://arxiv.org/pdf/1803.00567.pdf" class="uri">https://arxiv.org/pdf/1803.00567.pdf</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.15671.pdf&quot;&gt;Vocabulary Learning via Optimal Transport for Neural Machine Translation&lt;/a&gt;&lt;br /&gt;
ACL2021</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="ACL2021" scheme="http://example.com/tags/ACL2021/"/>
    
  </entry>
  
  <entry>
    <title>Encoding Word Order in Complex Embeddings [ICLR2020]</title>
    <link href="http://example.com/2021/10/09/p1-position-encoding/"/>
    <id>http://example.com/2021/10/09/p1-position-encoding/</id>
    <published>2021-10-09T13:29:54.000Z</published>
    <updated>2021-10-26T15:36:04.658Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1912.12333.pdf">Encoding Word Order in Complex Embeddings</a><br />Code: <a href="https://github.com/iclr-complex-order/complex-order">link</a></p><h2 id="æ¦‚è¦">æ¦‚è¦</h2><p>é’ˆå¯¹ä½ç½®ç¼–ç æå‡ºçš„æ”¹è¿›ï¼Œåˆ‡å…¥ç‚¹æ–°é¢–é«˜æ•ˆä¸”ä¸ºä½ç½®ç¼–ç å¸¦æ¥äº†ä¸€å®šçš„å…·ä½“æ„ä¹‰å’Œå¯è§£é‡Šæ€§ã€‚ä¼ ç»Ÿçš„ä½ç½®åµŒå…¥æ•è·å•ä¸ªå•è¯çš„ä½ç½®ï¼Œè€Œä¸æ˜¯å•ä¸ªå•è¯ä½ç½®ä¹‹é—´çš„æœ‰åºå…³ç³»(ä¾‹å¦‚é‚»æ¥å…³ç³»æˆ–ä¼˜å…ˆçº§)ã€‚æœ¬æ–‡æå‡ºçš„æ–¹æ³•å»ºæ¨¡å•è¯çš„å…¨å±€ç»å¯¹ä½ç½®å’Œå®ƒä»¬çš„é¡ºåºå…³ç³»ï¼Œå°†ä»¥å‰å®šä¹‰ä¸ºç‹¬ç«‹å‘é‡çš„è¯åµŒå…¥æ¨å¹¿åˆ°å˜é‡(ä½ç½®)ä¸Šçš„è¿ç»­è¯å‡½æ•°ã€‚æ¯ä¸ªå•è¯çš„è¡¨ç¤ºä¼šéšç€ä½ç½®çš„å¢åŠ è€Œç§»åŠ¨ã€‚å› æ­¤ï¼Œåœ¨è¿ç»­å‡½æ•°ä¸­ï¼Œä¸åŒä½ç½®çš„è¯è¡¨ç¤ºå¯ä»¥ç›¸äº’å…³è”ã€‚å°†è¿™äº›å‡½æ•°çš„é€šè§£æ¨å¹¿åˆ°å¤å€¼åŸŸï¼Œå¾—åˆ°äº†æ›´ä¸°å¯Œçš„è¡¨ç¤ºã€‚ä½œè€…åœ¨æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘å’Œè¯­è¨€æ¨¡å‹æ–¹é¢è¿›è¡Œå®éªŒï¼Œå–å¾—äº†è‰¯å¥½çš„è¡¨ç°ã€‚</p><h2 id="positional-encoding">Positional encoding</h2><p>Positional encoding ä½ç½®ç¼–ç åœ¨transformerä¸­ç”¨äºå­˜å‚¨ä½ç½®ä¿¡æ¯ï¼ˆç”±äºself-attentionæ²¡æ³•è·å–åºåˆ—ä½ç½®çš„ä¿¡æ¯ï¼‰ï¼Œæ­¤å¤–BERTä¸­encodingéƒ¨åˆ†ä¹ŸåŒ…å«äº†ä½ç½®ç¼–ç ã€‚å¯¹äºä½ç½®ç¼–ç ï¼Œæœ¬èƒ½çš„æƒ³æ³•æ˜¯é’ˆå¯¹åºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®å¿…é¡»æ˜¯ç‹¬ä¸€æ— äºŒçš„ï¼Œä¸”ä¸å—åºåˆ—é•¿åº¦çš„å½±å“ã€‚å¸¸è§çš„positional encodingçš„æ–¹æ³•æœ‰:</p><ul><li>ç»å¯¹ï¼ˆæ­£å¼¦ï¼‰ä½ç½®ç¼–ç ï¼ˆSinusoidal Position Encodingï¼‰</li><li>ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆRelative Position Representationsï¼‰</li><li>å¯å­¦ä¹ ä½ç½®ç¼–ç </li></ul><h3 id="æ­£å¼¦ä½ç½®ç¼–ç ">æ­£å¼¦ä½ç½®ç¼–ç </h3><p>Transformerä¸­ä½¿ç”¨çš„å°±æ˜¯è¿™ç§ç¼–ç ï¼Œå®é™…ä¸Šå…·ä½“ç¼–ç è¿‡ç¨‹ä½¿ç”¨äº†æ­£å¼¦å’Œä½™å¼¦ã€‚å…·ä½“å…¬å¼ä¸ºï¼š <span class="math display">\[\begin{aligned}P E_{(p o s, 2 i)} &amp;=\sin \left(p o s / 10000^{2 i / d_{\text {model }}} \right) \\P E_{(p o s, 2 i+1)} &amp;=\cos \left(p o s / 10000^{2 i / d_{\text {model }}} \right)\end{aligned}\]</span> å…¶ä¸­<span class="math inline">\(d_{model}\)</span>ä¸ºè¾“å…¥è¯å‘é‡çš„ç»´åº¦ã€‚å¦‚d(model)=128,é‚£ä¹ˆä½ç½®3å¯¹åº”çš„ä½ç½®å‘é‡ä¸º <span class="math display">\[\left[\sin \left(3 / 10000^{0 / 128}\right), \cos \left(3 / 10000^{1 / 128}\right), \sin \left(3 / 10000^{2 / 28}\right), \cos \left(3 / 10000^{3 / 28}\right), \ldots\right]\]</span> åœ¨å…·ä½“çš„åº”ç”¨æ—¶å¯èƒ½å‰ä¸€éƒ¨åˆ†ç”¨æ­£å¼¦åä¸€éƒ¨åˆ†ç”¨ä½™å¼¦ã€‚</p><p><img src="/images/pe4.png" title="Bertä¸­çš„ä½ç½®ç¼–ç " /></p><h3 id="ç›¸å¯¹ä½ç½®ç¼–ç ">ç›¸å¯¹ä½ç½®ç¼–ç </h3><p>Todo<br /><a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a></p><h3 id="å¯å­¦ä¹ ä½ç½®ç¼–ç ">å¯å­¦ä¹ ä½ç½®ç¼–ç </h3><p>Todo</p><h2 id="intro">Intro</h2><p>æœ¬æ–‡çš„é‡ç‚¹åœ¨äºå»ºæ¨¡æ–‡æœ¬ä¿¡æ¯ä¸­é¢å¤–çš„è¯çš„å†…éƒ¨é¡ºåºå’Œç›¸é‚»å…³ç³»ï¼Œå¯¹æ¯”åŸæœ¬ä½ç½®ç¼–ç æ–¹å¼ä»…ç¼–ç è¯çš„ä½ç½®ã€‚æ¨¡å‹å°†ä¹‹å‰å®šä¹‰ä¸ºç‹¬ç«‹å‘é‡çš„è¯åµŒå…¥æ‰©å±•ä¸ºä½ç½®è‡ªå˜é‡ä¸Šçš„è¿ç»­å‡½æ•°ã€‚åœ¨ä¸€ä¸ªè¿ç»­å‡½æ•°ä¸­ï¼Œä¸åŒä½ç½®çš„è¯è¡¨ç¤ºå¯ä»¥ç›¸äº’å…³è”ã€‚</p><p><img src="/images/pe1.png" /></p><h2 id="methodology">Methodology</h2><p>ç±»ä¼¼äºWord Embeddingï¼Œä½ç½®ç¼–ç ï¼ˆPEï¼‰å®šä¹‰äº†ä¸€ä¸ªæ˜ å°„å…³ç³»ï¼Œå°†è¯çš„åºåˆ—ç´¢å¼•æ˜ å°„ä¸ºä¸€ä¸ªå‘é‡ã€‚<span class="math inline">\(f_{n e}: \mathbb{N} \rightarrow \mathbb{R}^{D}\)</span>ã€‚æœ€ç»ˆæŸä¸ªè¯çš„embeddingé€šå¸¸è¡¨ç¤ºä¸ºä¸ºè¯å‘é‡å’Œä½ç½®å‘é‡çš„å’Œï¼š <span class="math display">\[f(j, p o s)=f_{w e}(j)+f_{p e}(p o s)\]</span></p><p>è®ºæ–‡ä¸­æå‡ºäº†ä¸€ä¸ªä½ç½®ç‹¬ç«‹é—®é¢˜ï¼ˆposition independence problemï¼‰ï¼Œå³ä½ç½®ç¼–ç æ— æ³•æ•è·ç›¸é‚»è¯ä»¥åŠå…¶é¡ºåºä¹‹é—´çš„æ½œåœ¨å…³ç³»ã€‚è€Œå½“åç»­ç”¨äºç‰¹å¾å¤„ç†çš„ç½‘ç»œå¯¹è¿™ç±»ä¿¡æ¯ä¸æ•æ„Ÿæ—¶ï¼Œè¿™ä¸€é—®é¢˜å°±ä¼šé™åˆ¶æ•´ä¸ªæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚ç›¸å¯¹ä½ç½®ç¼–ç é’ˆå¯¹è¿™ä¸€é—®é¢˜è¿›è¡Œäº†ä¸€å®šçš„ç ”ç©¶ï¼Œä½†å…¶æ— æ³•æ¶µç›–æ•´ä¸ªåºåˆ—åŸŸã€‚</p><h3 id="æ€§è´¨">æ€§è´¨</h3><p>è®ºæ–‡æŒ‡å‡ºäº†åœ¨ä½ç½®ç¼–ç ä¸­å»ºç«‹è¯åºæ¨¡å‹æ‰€å¿…éœ€çš„æ€§è´¨ã€‚<br />ç”±äºä½ç½®å‘é‡ä¸­æ¯ä¸ªç»´åº¦çš„å€¼éƒ½æ˜¯æ ¹æ®ç¦»æ•£çš„ä½ç½®indexå¾—åˆ°çš„ï¼Œè¿™ä½¿å¾—ä½ç½®é—´æœ‰åºå…³ç³»å»ºæ¨¡å˜å¾—å›°éš¾ï¼Œå› æ­¤éœ€è¦æ ¹æ®ä½ç½®ç´¢å¼•æ„å»ºä¸€ä¸ªè¿ç»­çš„å‡½æ•°ï¼ˆä»¥åœ¨æ¯ä¸ªç»´åº¦ä¸­è¡¨ç¤ºä¸€ä¸ªç‰¹å®šçš„å•è¯ï¼Ÿï¼‰ <span class="math display">\[f(j, \text { pos })=\boldsymbol{g}_{j}(\text { pos }) \in \mathbb{R}^{D}\]</span> <span class="math inline">\(g_j\)</span>å³<span class="math inline">\(\boldsymbol{g}_{w e}(j) \in(\mathcal{F})^{D}\)</span>ï¼Œè¯<span class="math inline">\(w_j\)</span>åœ¨posä½ç½®å¯ä»¥è¡¨ç¤ºä¸º <span class="math display">\[\left[g_{j, 1}(\operatorname{pos}), g_{j, 2}(\operatorname{pos}), \ldots, g_{j, D}(\text { pos })\right] \in \mathbb{R}^{D}\]</span> å½“è¯<span class="math inline">\(w_j\)</span>ä»posä½ç½®è½¬åˆ°posâ€™ä½ç½®æ—¶ï¼Œåªéœ€è¦æ”¹å˜è‡ªå˜é‡çš„å€¼è€Œä¸éœ€è¦æ”¹å˜æ˜ å°„å‡½æ•°<span class="math inline">\(g_j\)</span>ã€‚</p><h3 id="å‡½æ•°">å‡½æ•°</h3><p>ç”±äºå®æ•°ä¹Ÿè¢«å›Šæ‹¬åœ¨å¤æ•°åŸŸä¸­ï¼Œä¸”å‰äººæœ‰ç›¸å…³å·¥ä½œï¼ˆè¯¦è§è®ºæ–‡åŸæ–‡Section2.2ï¼‰éªŒè¯äº†å¤æ•°åŸŸæ‰€å…·æœ‰çš„æ›´å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½œè€…å°†æ¨¡å‹æ‹“å±•åˆ°äº†å¤æ•°åŸŸã€‚å¯¹äºç†æƒ³çš„æ˜ å°„å‡½æ•°ï¼Œè®ºæ–‡ä¸­æå‡ºäº†ä¸¤æ¡æ€§è´¨ï¼Œå³:</p><ul><li>Position-free offset transformation</li><li>Boundedness</li></ul><p>å˜æ¢å‡½æ•°<span class="math inline">\(Transform\)</span>éœ€æ»¡è¶³å¯¹äºä»»ä½•posï¼Œæœ‰ <span class="math display">\[g(p o s+n)=\operatorname{Transform}_{n}(g(p o s))\]</span> æ»¡è¶³ç­‰å¼çš„å˜æ¢å‡½æ•°è¢«ç§°ä¸ºwitnessï¼Œè€Œæ»¡è¶³è¿™ä¸€æ¡ä»¶çš„æ˜ å°„å‡½æ•°<span class="math inline">\(g_j\)</span>åˆ™è¢«ç§°ä¸º<em>linearly witnessed</em>ã€‚è§„å®šTransform <span class="math inline">\((n\)</span>, pos <span class="math inline">\()=\)</span> Transform <span class="math inline">\(_{n}(\)</span> pos <span class="math inline">\()=w(n)\)</span>ã€‚å¦å¤–ï¼Œæ˜ å°„å‡½æ•°<span class="math inline">\(g_j\)</span>éœ€è¦æœ‰ç•Œã€‚</p><p>è€Œåä½œè€…è¯æ˜äº†æ»¡è¶³ä¸Šè¿°æ€§è´¨çš„æ˜ å°„å‡½æ•°å”¯ä¸€è§£ä¸º <span class="math display">\[g(p o s)=z_{2} z_{1}^{p o s} \text { for } z_{1}, z_{2} \in \mathbb{C} \text { with }\left|z_{1}\right| \leq 1\]</span> å¯¹äºä»»æ„çš„ <span class="math inline">\(z \in \mathbb{C}\)</span>, æˆ‘ä»¬å¯ä»¥å†™æˆ <span class="math inline">\(z=r e^{i \theta}=r(\cos \theta+i \sin \theta)\)</span>ï¼Œå› æ­¤ä¸Šå¼å¯å†™ä¸ºï¼š <span class="math display">\[g(p o s)=z_{2} z_{1}^{p o s}=r_{2} e^{i \theta_{2}}\left(r_{1} e^{i \theta_{1}}\right)^{p o s}=r_{2} r_{1}^{p o s} e^{i\left(\theta_{2}+\theta_{1} p o s\right)} \quad$ subject to $\left|r_{1}\right| \leq 1\]</span></p><p>(...è·³è¿‡è¯æ˜å’Œä¼˜åŒ–è¿‡ç¨‹)</p><p>æœ€ç»ˆçš„ä½ç½®ç¼–ç å‡½æ•°<span class="math inline">\(f(j\)</span>, pos <span class="math inline">\()\)</span>ä¸º <img src="/images/pe2.png" /> <span class="math inline">\(j\)</span>ä»£è¡¨å•è¯ï¼ˆç´¢å¼•ï¼‰ï¼Œ<span class="math inline">\(pos\)</span>è¡¨ç¤ºä½ç½®ç´¢å¼•ã€‚<br />å¯¹äºembeddingä¸­çš„æ¯ä¸€ç»´åº¦ï¼Œéƒ½æœ‰å„è‡ªçš„å‚æ•°ï¼ŒæŒ¯å¹…rã€é¢‘ç‡pã€åˆç›¸<span class="math inline">\(\theta\)</span>ï¼Œè¿™äº›å‚æ•°æ˜¯trainableçš„ã€‚æ­¤å¤–ï¼Œå‘¨æœŸ/é¢‘ç‡å†³å®šäº†å•è¯å¯¹ä½ç½®çš„æ•æ„Ÿç¨‹åº¦ã€‚å½“å‘¨æœŸå¾ˆçŸ­ï¼Œåˆ™è¯´æ˜åµŒå…¥å°†å¯¹positioné«˜åº¦æ•æ„Ÿã€‚æ³¨æ„ï¼ŒæŒ¯å¹…ã€é¢‘ç‡æ˜¯ä¸postionï¼ˆè‡ªå˜é‡ï¼‰æ— å…³çš„ï¼Œä¸å•è¯å’Œç»´åº¦æœ‰å…³ã€‚æ­¤æ—¶ï¼Œword embeddingå¯ä»¥ç”¨è¿™äº›å‚æ•°æ¥è¡¨ç¤ºï¼ˆç»´åº¦ä¸positional embeddingç»´åº¦ç›¸åŒï¼‰ã€‚</p><h2 id="å®éªŒ">å®éªŒ</h2><p>ä½œè€…åœ¨æ–‡æœ¬åˆ†ç±»ã€æœºå™¨ç¿»è¯‘å’Œè¯­è¨€æ¨¡å‹å‡ ä¸ªä»»åŠ¡ä¸Šè¿›è¡Œäº†å®éªŒï¼Œåˆ†åˆ«ç”¨Fasttextã€LSTMã€CNNã€Transformerä½œä¸ºæ¨¡å‹çš„backboneï¼Œè€Œåä½¿ç”¨ä¸åŒçš„ä½ç½®ç¼–ç æ–¹æ³•ä»¥åŠæœ¬æ–‡çš„Complex-orderç¼–ç æ–¹æ³•è¿›è¡Œembeddingï¼Œå¯¹æ¯”å‡ ä¸ªå®éªŒç»“æœå‡å–å¾—äº†å¯è§‚çš„æå‡ã€‚è€Œè®¡ç®—å¼€é”€ï¼ˆæ—¶é—´ï¼‰ä¸Šå¹¶æ²¡æœ‰æ˜¾è‘—çš„å¢åŠ ã€‚ <img src="/images/pe3.png" title="éƒ¨åˆ†å®éªŒç»“æœ" /> å®éªŒåŸºäºtensorflowï¼Œç›®å‰æ²¡æœ‰pytorchç‰ˆæœ¬ï¼Œç¬”è€…å°†ä¼šå°è¯•å°†å…¶è¿ç§»åˆ°pytorchæ¡†æ¶ä¸‹å¹¶å¼€æºã€‚</p><h2 id="ç›¸å…³å·¥ä½œ">ç›¸å…³å·¥ä½œ</h2><p>Vanilla Position Embeddings<br />Trigonometric Position Embeddings<br />Todo</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1912.12333.pdf&quot;&gt;Encoding Word Order in Complex Embeddings&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;https://github.com/iclr</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="Positional_encoding" scheme="http://example.com/tags/Positional-encoding/"/>
    
  </entry>
  
  <entry>
    <title>Improved GCN for Text Classification [GNN]</title>
    <link href="http://example.com/2021/09/29/textGCN/"/>
    <id>http://example.com/2021/09/29/textGCN/</id>
    <published>2021-09-29T14:24:53.000Z</published>
    <updated>2021-09-30T16:45:05.204Z</updated>
    
    <content type="html"><![CDATA[<center><font size = 4> <strong>TextRGNN: Residual Graph Neural Networks for Text Classification</strong></font></center>]]></content>
    
    
      
      
    <summary type="html">&lt;center&gt;
&lt;font size = 4&gt; &lt;strong&gt;TextRGNN: Residual Graph Neural Networks for Text Classification&lt;/strong&gt;&lt;/font&gt;
&lt;/center&gt;
</summary>
      
    
    
    
    <category term="Research" scheme="http://example.com/categories/Research/"/>
    
    
    <category term="TextGCN" scheme="http://example.com/tags/TextGCN/"/>
    
  </entry>
  
  <entry>
    <title>word2vec</title>
    <link href="http://example.com/2021/09/29/word2vec/"/>
    <id>http://example.com/2021/09/29/word2vec/</id>
    <published>2021-09-28T16:10:00.000Z</published>
    <updated>2021-09-28T16:10:08.351Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>sort algorithm 1</title>
    <link href="http://example.com/2021/09/20/sort-1/"/>
    <id>http://example.com/2021/09/20/sort-1/</id>
    <published>2021-09-20T15:01:57.000Z</published>
    <updated>2021-09-28T16:05:07.653Z</updated>
    
    <content type="html"><![CDATA[<h1 id="sorti">Sort(i)</h1><p>ä¸‰ç§å¤æ‚åº¦ä¸º<span class="math inline">\(O(n^2)\)</span>çš„æ’åºç®—æ³•:</p><ul><li>å†’æ³¡æ’åº</li><li>é€‰æ‹©æ’åº</li><li>æ’å…¥æ’åº</li></ul><h2 id="bubble-sort">Bubble sort</h2><p>â€œä¸€è¶Ÿä¸€è¶Ÿæ¥â€ <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span>(<span class="params">ls</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-i-<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> ls[j] &gt; ls[j+<span class="number">1</span>]:</span><br><span class="line">                ls[j+<span class="number">1</span>],ls[j]=ls[j],ls[j+<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span>(ls)</span><br></pre></td></tr></table></figure></p><h2 id="select-sort">Select sort</h2><p>&quot;ä¸€ä¸ªä¸€ä¸ªæ’&quot; <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_sort</span>(<span class="params">ls</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-<span class="number">1</span>):</span><br><span class="line">        min_loc = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(ls)-<span class="number">1</span>-i):</span><br><span class="line">            <span class="keyword">if</span> ls[i+j]&lt;ls[min_loc]:</span><br><span class="line">                min_loc =i+j</span><br><span class="line">        ls[min_loc],ls[i] = ls[i],ls[min_loc]</span><br><span class="line">    <span class="built_in">print</span>(ls)</span><br></pre></td></tr></table></figure></p><h2 id="insert-sort">Insert sort</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_select</span>(<span class="params">ls</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(ls)):</span><br><span class="line">        j = i-<span class="number">1</span></span><br><span class="line">        tmp = ls[i]</span><br><span class="line">        <span class="keyword">while</span> j&gt;=<span class="number">0</span> <span class="keyword">and</span> ls[j]&gt;tmp:</span><br><span class="line">            ls[j+<span class="number">1</span>]=ls[j]</span><br><span class="line">            j-=<span class="number">1</span></span><br><span class="line">        ls[j+<span class="number">1</span>] = tmp</span><br><span class="line">    <span class="built_in">print</span>(ls)</span><br></pre></td></tr></table></figure><h1 id="sortii">Sort(ii)</h1><p>ä¸‰ç§å¤æ‚åº¦ä¸º<span class="math inline">\(O(nlogn)\)</span>çš„æ’åºç®—æ³•:</p><ul><li>å¿«é€Ÿæ’åº</li><li>å½’å¹¶æ’åº</li><li>å †æ’åº</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;sorti&quot;&gt;Sort(i)&lt;/h1&gt;
&lt;p&gt;ä¸‰ç§å¤æ‚åº¦ä¸º&lt;span class=&quot;math inline&quot;&gt;\(O(n^2)\)&lt;/span&gt;çš„æ’åºç®—æ³•:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;å†’æ³¡æ’åº&lt;/li&gt;
&lt;li&gt;é€‰æ‹©æ’åº&lt;/li&gt;
&lt;li&gt;æ’å…¥æ’åº&lt;/li&gt;
&lt;/</summary>
      
    
    
    
    <category term="Leetcode" scheme="http://example.com/categories/Leetcode/"/>
    
    
    <category term="Data Structure" scheme="http://example.com/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>Linear Regression (python &amp; pytorch)</title>
    <link href="http://example.com/2021/08/25/dae%E7%9A%84%E5%89%AF%E6%9C%AC/"/>
    <id>http://example.com/2021/08/25/dae%E7%9A%84%E5%89%AF%E6%9C%AC/</id>
    <published>2021-08-25T15:21:47.000Z</published>
    <updated>2021-09-28T16:09:28.897Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="ML" scheme="http://example.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>search algorithm</title>
    <link href="http://example.com/2021/08/25/test/"/>
    <id>http://example.com/2021/08/25/test/</id>
    <published>2021-08-25T14:42:55.000Z</published>
    <updated>2021-09-28T15:52:31.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="linear-search-and-binary-search">Linear Search and Binary Search</h1><h2 id="linear-search">Linear search</h2><p>Time complexityï¼š<span class="math inline">\(O(n)\)</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_search</span>(<span class="params">ls,val</span>):</span></span><br><span class="line">    <span class="keyword">for</span> index,value <span class="keyword">in</span> <span class="built_in">enumerate</span>(ls):</span><br><span class="line">        <span class="keyword">if</span> value == val:</span><br><span class="line">            <span class="keyword">return</span> index</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure></p><h2 id="binary-search">Binary search</h2><p>Time complexityï¼š<span class="math inline">\(O(log n)\)</span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span>(<span class="params">ls,val</span>):</span></span><br><span class="line">    left = <span class="number">0</span>  <span class="comment">#å·¦æŒ‡é’ˆ</span></span><br><span class="line">    right = <span class="built_in">len</span>(ls)-<span class="number">1</span>  <span class="comment">#å³æŒ‡é’ˆ</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = (left+right)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> ls[mid] == val:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="keyword">elif</span> ls[mid]&lt;val:</span><br><span class="line">            left = mid+<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right = mid-<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;linear-search-and-binary-search&quot;&gt;Linear Search and Binary Search&lt;/h1&gt;
&lt;h2 id=&quot;linear-search&quot;&gt;Linear search&lt;/h2&gt;
&lt;p&gt;Time complexityï¼š&lt;</summary>
      
    
    
    
    <category term="Leetcode" scheme="http://example.com/categories/Leetcode/"/>
    
    
    <category term="Data Structure" scheme="http://example.com/tags/Data-Structure/"/>
    
  </entry>
  
  <entry>
    <title>BERT</title>
    <link href="http://example.com/2021/08/16/test-my-site/"/>
    <id>http://example.com/2021/08/16/test-my-site/</id>
    <published>2021-08-16T14:58:35.000Z</published>
    <updated>2021-09-29T14:29:21.109Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pre-training-of-deep-bidirectional-transformers-for-language-understanding">Pre-training of Deep Bidirectional Transformers for Language Understanding</h1>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;pre-training-of-deep-bidirectional-transformers-for-language-understanding&quot;&gt;Pre-training of Deep Bidirectional Transformers for Lang</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Markdownè¯­æ³•</title>
    <link href="http://example.com/2021/08/16/hello-world/"/>
    <id>http://example.com/2021/08/16/hello-world/</id>
    <published>2021-08-16T14:25:40.470Z</published>
    <updated>2021-11-17T12:11:12.588Z</updated>
    
    <content type="html"><![CDATA[<p>ç”±äºHexoåšå®¢çš„æ’°å†™éœ€è¦ç”¨Markdownï¼Œè™½ç„¶æ¯”Latexè¦ç®€å•ç‚¹ï¼Œä½†æ˜¯å¹³æ—¶ç”¨çš„æ¯”è¾ƒå°‘ï¼Œè¿™äº›æ‚ä¸ƒæ‚å…«çš„è¯­æ³•å¾ˆéš¾ä¸€ä¸‹å­å…¨éƒ¨è®°ä½ï¼Œå› æ­¤åœ¨è¿™é¡µåšå®¢ä¸­è®°å½•ä¸€ä¸‹</p><h2 id="æ–‡å­—">æ–‡å­—</h2><h3 id="æ ‡é¢˜">æ ‡é¢˜</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ä¸€çº§æ ‡é¢˜</span><br><span class="line">## äºŒçº§æ ‡é¢˜</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="å±…ä¸­">å±…ä¸­</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;center&gt;è¿™ä¸€è¡Œéœ€è¦å±…ä¸­&lt;/center&gt;</span><br></pre></td></tr></table></figure><h3 id="å­—ä½“">å­—ä½“</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">**ï¼ˆï¼‰** åŠ ç²—</span><br><span class="line">*ï¼ˆï¼‰* æ–œä½“</span><br><span class="line">ï½ï½ï¼ˆï¼‰ï½ï½ åˆ é™¤çº¿</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;font face= â€œé»‘ä½“â€ color=red size=7&gt;å­—ä½“è®¾ç½®&lt;/font&gt; #size 1-7ï¼Œæµè§ˆå™¨é»˜è®¤3</span><br></pre></td></tr></table></figure><h2 id="å¼•ç”¨">å¼•ç”¨</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;è¿™æ˜¯å¼•ç”¨æ–‡å­—</span><br></pre></td></tr></table></figure><h2 id="åˆ†å‰²çº¿">åˆ†å‰²çº¿</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">***</span><br></pre></td></tr></table></figure><h2 id="å›¾ç‰‡">å›¾ç‰‡</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![å›¾ç‰‡alt](å›¾ç‰‡åœ°å€ &#x27;&#x27;å›¾ç‰‡title&#x27;&#x27;)</span><br></pre></td></tr></table></figure><p>å›¾ç‰‡altå°±æ˜¯æ˜¾ç¤ºåœ¨å›¾ç‰‡ä¸‹é¢çš„æ–‡å­—ï¼Œç›¸å½“äºå¯¹å›¾ç‰‡å†…å®¹çš„è§£é‡Šã€‚ å›¾ç‰‡titleæ˜¯å›¾ç‰‡çš„æ ‡é¢˜ï¼Œå½“é¼ æ ‡ç§»åˆ°å›¾ç‰‡ä¸Šæ—¶æ˜¾ç¤ºçš„å†…å®¹ã€‚titleå¯åŠ å¯ä¸åŠ </p><h2 id="é“¾æ¥">é“¾æ¥</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[è¶…é“¾æ¥å](è¶…é“¾æ¥åœ°å€ &quot;è¶…é“¾æ¥title&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ç«™å†…é“¾æ¥</span><br><span class="line">&#123;% post_link [åšå®¢å] title %&#125;</span><br></pre></td></tr></table></figure><h2 id="é¡µå†…è·³è½¬">é¡µå†…è·³è½¬</h2><p>åˆ†æˆä¸¤éƒ¨ï¼šå®šä¹‰é”šç‚¹ã€ä½¿ç”¨markdownè¯­æ³•è·³è½¬ <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;span id=&quot;jump&quot;&gt; ï¼ˆè·³è½¬åˆ°çš„åœ°æ–¹ï¼‰ &lt;/span&gt;</span><br><span class="line">[ç‚¹å‡»è·³è½¬](#jump)</span><br></pre></td></tr></table></figure></p><h2 id="åˆ—è¡¨">åˆ—è¡¨</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- åˆ—è¡¨å†…å®¹</span><br><span class="line">+ åˆ—è¡¨å†…å®¹</span><br><span class="line">1. åˆ—è¡¨å†…å®¹</span><br></pre></td></tr></table></figure><h2 id="ä»£ç ">ä»£ç </h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">`ä»£ç å†…å®¹`</span><br><span class="line">\``` (é˜²æ­¢æ‰“ä¸å‡ºåŠ ä¸ª\è½¬ä¹‰ä¸€ä¸‹)</span><br><span class="line">  ä»£ç ...</span><br><span class="line">  ä»£ç ...</span><br><span class="line">  ä»£ç ...</span><br><span class="line">\```</span><br></pre></td></tr></table></figure><h2 id="æ•°å­¦å…¬å¼">æ•°å­¦å…¬å¼</h2><p>å…¬å¼ã€å¸Œè…Šå­—æ¯ã€ä¸Šæ ‡ä¸‹æ ‡ç­‰åŸºæœ¬è¯­æ³•ä¸latexç±»ä¼¼ï¼Œå¯å‚è€ƒ<a href="https://www.jianshu.com/p/a0aa94ef8ab2">markdownæ•°å­¦å…¬å¼</a></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">æ­£æ–‡ä¸­$...$</span><br><span class="line">å•è¡Œæ˜¾ç¤º$$...$$</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ç”±äºHexoåšå®¢çš„æ’°å†™éœ€è¦ç”¨Markdownï¼Œè™½ç„¶æ¯”Latexè¦ç®€å•ç‚¹ï¼Œä½†æ˜¯å¹³æ—¶ç”¨çš„æ¯”è¾ƒå°‘ï¼Œè¿™äº›æ‚ä¸ƒæ‚å…«çš„è¯­æ³•å¾ˆéš¾ä¸€ä¸‹å­å…¨éƒ¨è®°ä½ï¼Œå› æ­¤åœ¨è¿™é¡µåšå®¢ä¸­è®°å½•ä¸€ä¸‹&lt;/p&gt;
&lt;h2 id=&quot;æ–‡å­—&quot;&gt;æ–‡å­—&lt;/h2&gt;
&lt;h3 id=&quot;æ ‡é¢˜&quot;&gt;æ ‡é¢˜&lt;/h3&gt;
&lt;figure class=&quot;</summary>
      
    
    
    
    <category term="æ‚ä¸ƒæ‚å…«" scheme="http://example.com/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>æ¼«å¨äººç‰©çŸ¥è¯†å›¾è°±-MARVEL Knowledge Graph</title>
    <link href="http://example.com/2021/05/24/kg/"/>
    <id>http://example.com/2021/05/24/kg/</id>
    <published>2021-05-24T10:16:44.000Z</published>
    <updated>2021-10-24T10:23:21.267Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    <category term="Project" scheme="http://example.com/categories/Project/"/>
    
    
    <category term="KG" scheme="http://example.com/tags/KG/"/>
    
  </entry>
  
</feed>
