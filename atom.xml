<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chen&#39;s Homepage</title>
  
  <subtitle>Hello AI</subtitle>
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-02-15T12:23:59.232Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Chen jiayuan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>中文分词｜Chinese-word-segmentation (3)-基于词</title>
    <link href="http://example.com/2022/02/10/cws3/"/>
    <id>http://example.com/2022/02/10/cws3/</id>
    <published>2022-02-10T05:15:50.000Z</published>
    <updated>2022-02-15T12:23:59.232Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前情提要">前情提要</h1><p>在前面两篇Blog（<a href="/2022/02/07/cws/" title="中文分词1">中文分词1</a>、<a href="/2022/02/09/cws2/" title="中文分词2">中文分词2</a>）介绍了关于中文分词的基本方法以及近年来基于深度学习的部分分词模型，即基于字符的分词技术，接下来讲介绍基于词的中文分词模型。</p><h1 id="基本框架">基本框架</h1><p>Neural Word Segmentation Learning for Chinese</p><p><img src="/images/cws/11.png" title="neural network scoring model" /> 基于的框架如上图所示，不同于基于字符标注的模型，基于词的模型根据字符向量生成词向量。具体而言，模型得到输入的字符向量后，使用一个Gated Combination Neural Network，将L个字符向量组合，生成候选词向量： <span class="math display">\[\mathbf{w}=g\left(\mathbf{W}^{(L)}\left[\begin{array}{c}\mathbf{c}_{1} \\\vdots \\\mathbf{c}_{L}\end{array}\right]\right)\]</span></p><p><span class="math inline">\(\mathbf{W}^{(L)}\)</span>是一个所有词共享的权重。这GCNN中包含reset gate 和 update gate。</p><p><span class="math display">\[\mathbf{w}=\mathbf{z}_{N} \odot \hat{\mathbf{w}}+\sum_{i=1}^{L} \mathbf{z}_{i} \odot \mathbf{c}_{i}\]</span></p><p><span class="math display">\[\hat{\mathbf{w}}=\tanh \left(\mathbf{W}^{(L)}\left[\begin{array}{c}\mathbf{r}_{1} \odot \mathbf{c}_{1} \\\vdots \\\mathbf{r}_{L} \odot \mathbf{c}_{L}\end{array}\right]\right)\]</span></p><p><span class="math inline">\(\mathbf{W}^{(L)} \in \mathbb{R}^{d \times L d}\)</span> 和 <span class="math inline">\(\mathbf{r}_{i} \in \mathbb{R}^{d}(1 \leq i \leq L)\)</span> 表示reset gates，用来决定字符向量的哪部分被结合到词向量中： <span class="math display">\[\left[\begin{array}{c}\mathbf{r}_{1} \\\vdots \\\mathbf{r}_{L}\end{array}\right]=\sigma\left(\mathbf{R}^{(L)}\left[\begin{array}{c}\mathbf{c}_{1} \\\vdots \\\mathbf{c}_{L}\end{array}\right]\right)\]</span> 而update gate则是 <span class="math display">\[\left[\begin{array}{c}\mathbf{z}_{N} \\\mathbf{z}_{1} \\\vdots \\\mathbf{z}_{L}\end{array}\right]=\exp \left(\mathbf{U}^{(L)}\left[\begin{array}{c}\hat{\mathbf{w}} \\\mathbf{c}_{1} \\\vdots \\\mathbf{c}_{L}\end{array}\right]\right) \odot\left[\begin{array}{c}1 / \mathbf{Z} \\1 / \mathbf{Z} \\\vdots \\1 / \mathbf{Z}\end{array}\right]\]</span> 其中<span class="math inline">\(\mathbf{U}^{(L)} \in \mathbb{R}^{(L+1) d \times(L+1) d}\)</span> 是系数矩阵，<span class="math inline">\(\mathbf{Z} \in \mathbb{R}^{d}\)</span>是归一化向量。</p><p>得到候选词向量之后，每个输入句子中的词向量会被转化为一个分数word_score，代表这个词有多大的可能性是一个真实存在的词。这些word_score会与经过LSTM之后的词向量共同组合成sentence score。</p><p>得到分数之后，之前多数序列标注的分词方法多使用Viterbi算法进行动态规划，得到最优的分词方法，但是在这个模型中，由于可能的句子分词方式总数太大，且为了捕捉完整的分割决策（不同于基于字符标注的方法），模型使用了Beam Search算法作为Decoder。</p><h1 id="改进模型">改进模型</h1><p>Fast and Accurate Neural Word Segmentation for Chinese</p><p>首先，这篇文章的作者对字符生成候选词向量的GCNN网络进行了改进。模型引入了一个高频词词典，在词典中则直接对character embedding进行average pooling，而不在词典中则根据构成的字符向量生成词向量，如下图所示 <img src="/images/cws/13.png" /> <span class="math display">\[\operatorname{COMP}\left(c_{1} . . c_{l}\right)=\tanh \left(\mathbf{W}_{l}^{c}\left[\mathbf{r}_{1} \odot \mathbf{c}_{1} ; \ldots ; \mathbf{r}_{l} \odot \mathbf{c}_{l}\right]+\mathbf{b}_{l}^{c}\right)\]</span> <span class="math display">\[\left[\mathbf{r}_{1} ; \ldots ; \mathbf{r}_{l}\right]=\operatorname{sigmoid}\left(\mathbf{W}_{l}^{r}\left[\mathbf{c}_{1} ; \ldots ; \mathbf{c}_{l}\right]+\mathbf{b}_{l}^{r}\right)\]</span> 作者讲gate mechanism进行了简化，使得模型训练更加快速。另外，原模型中的Beam Search也改为了贪心算法，采用了两种训练方法：Early update、LaSO update。Early update指的是一旦最优的分割无法实现，就立即更新。Early update的一个缺点是，搜索可能永远不会到达训练样本的末尾，这意味着数据的其余部分是“浪费”的。而LaSO update在每次更新后都在相同的实例上继续进行正确的假设（将正确的分词序列的对应前缀插入实例中）。</p><p>Transition-Based Neural Word Segmentation</p><p><img src="/images/cws/14.png" title="Transition-Based Neural Model" /> 上图为Transition-Based分词方法，它使用Transition system递增地去分词。Buffer部分用于存储句子已经分词的部分，未分词的在Queue中。action包括SEP-separate和APP-append，分别指分割和把字符pop到Buffer中。模型分词的过程就是寻找一个action序列的过程。</p><p>这个框架中的基本的特征包含三方面的信息。第一个信息是序列q中第一个字符和buffer中的最后一个字符用来分别给SEP和APP动作来打分。第二个信息是通过已经被识别的词来指导SEP。第三个信息是已识别的词的相关信息，比如它们的长度，这个词中的第一个字符或是最后一个字符可以作为额外的特征。从图中可以看到三个RNN分别编码word sequence、character sequence以及action sequence信息。而论文的作者则在这一框架的基础上用LSTM替换了原有的RNN。</p><h1 id="pre-train-model">Pre-train model</h1><p>BERT诞生后横扫各大NLP任务的榜单，中文分词自然也逃不了它的魔爪。BERT作为特征提取器，基于BERT的分词模型事实上大部分属于基于字符的分词技术。而基于BERT的分词模型的关注点则主要包括：在 模型中融合自定义词典、外部知识（领域知识）；如何大模型蒸馏成一个小的模型来提高分词性能；如何通过不同粒度标准的分词预料联合预训练，让分词能够通过某些简单的控制能够适应不用的分词场景。</p><p>部分相关论文：<br />Lexicon Enhanced Chinese Sequence Labelling Using BERT Adapter<br />Pre-training with Meta Learning for Chinese Word Segmentation<br />ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations （这个模型缩写就离谱）</p><p><img src="/images/cws/12.png" title="ZEN：Bert+N-gram" /></p><h1 id="写在最后">写在最后</h1><p><img src="/images/cws/15.png" title="paperwithcode上中文分词任务" /> 目前在几个公开数据集上分词模型的分词准确率都达到了97%以上。作为一个对于中文NLP来说非常重要的任务，中文分词可以说基于属于已经解决的任务，毕竟分词本就很难有绝对统一的标准。而就目前来说，相较于刷新榜单，或许更重要的是寻找更好解决OOV问题的方法（毕竟每段时间都有大量新词涌现）以及研发速度更快、体量更小的模型。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前情提要&quot;&gt;前情提要&lt;/h1&gt;
&lt;p&gt;在前面两篇Blog（&lt;a href=&quot;/2022/02/07/cws/&quot; title=&quot;中文分词1&quot;&gt;中文分词1&lt;/a&gt;、&lt;a href=&quot;/2022/02/09/cws2/&quot; title=&quot;中文分词2&quot;&gt;中文分词2&lt;/a&gt;）</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>中文分词｜Chinese-word-segmentation (2)-基于字符</title>
    <link href="http://example.com/2022/02/09/cws2/"/>
    <id>http://example.com/2022/02/09/cws2/</id>
    <published>2022-02-09T09:15:47.000Z</published>
    <updated>2022-02-13T16:37:31.413Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">背景</h1><p>随着深度学习技术的发展，基于深度神经网络的中文分词模型也不断涌现，这类分词技术大致上可以分为两类，其中一类基于字符的中文分词方法将分词看作字符的标注问题。根据字在词中的位置，分别给每个中文字符打上B、M、E、S的标签，将分词转化为字的标签预测。</p><h1 id="深度神经网络">深度神经网络</h1><p>Deep Learning for Chinese Word Segmentation and POS Tagging</p><p><img src="/images/cws/5.png" title="模型框架" /> 这篇论文是早期使用神经网络来做中文分词和词性标注的联合任务。对比机器学习模型，早期的神经网络致力于改善手工设计或选择特征这一问题，借助深层网络自动获取任务相关的字符特征。作为统计语言模型，这种方法利用大规模非标注数据来改善中文字符的内在表示，然后使用这些改善后的表示来提高有监督的分词模型和词性标注模型的性能。</p><p>对于输入句子中的每一个字，神经网络架构将为其可能的每一个TAG进行评分，为了解决不同句子对应的字序列长短不一的问题，本文中采用的是窗口方法。窗口方法假定一个字的tag主要依赖于与其相邻的字。具体而言，如上图所示，首先对输入的句子中的每个字查词典：通过lookup层得到窗口中每个字的字向量。之后将每个窗口长度的字向量首尾相连得到一个新的特征。接下来经过3层基础的神经网络。神经网络的输出是一个包含每个字可能标签的得分的矩阵。最后使用Viterbi算法进行动态规划完成标注的推断。</p><h1 id="lstm">LSTM</h1><p>Long Short-Term Memory Neural Networks for Chinese Word Segmentation</p><p>基于DNN的分词模型所能关注的是每个字符的窗口内邻近字符的特征，而LSTM则能解决句子内字符的长期依存关系问题。举个简单的例子：<br />冬天，能穿/多少/穿/多少，夏天，能穿/多/少/穿/多/少。<br />此时这个“多少”的分词就需要根据句子开头的“冬天”和“夏天”来确定。 <img src="/images/cws/6.png" title="LSTM进行分词" /></p><p>因此模型在原有深度学习分词的框架下，将传统的3层神经网络改为LSTM，借助输入输出门和遗忘门来传递上文信息。</p><p><span class="math display">\[\begin{aligned}\mathbf{i}^{(t)} &amp;=\sigma\left(\mathbf{W}_{i x} \mathbf{x}^{(t)}+\mathbf{W}_{i h} \mathbf{h}^{(t-1)}+\mathbf{W}_{i c} \mathbf{c}^{(t-1)}\right) \\\mathbf{f}^{(t)} &amp;=\sigma\left(\mathbf{W}_{f x} \mathbf{x}^{(t)}+\mathbf{W}_{f h} \mathbf{h}^{(t-1)}+\mathbf{W}_{f c} \mathbf{c}^{(t-1)}\right) \\\mathbf{c}^{(t)} &amp;=\mathbf{f}^{(t)} \odot \mathbf{c}^{(t-1)}+\mathbf{i}^{(t)} \odot \phi\left(\mathbf{W}_{c x} \mathbf{x}^{(t)}+\mathbf{W}_{c h} \mathbf{h}^{(t-1)}\right) \\\mathbf{o}^{(t)} &amp;=\sigma\left(\mathbf{W}_{o x} \mathbf{x}^{(t)}+\mathbf{W}_{o h} \mathbf{h}^{(t-1)}+\mathbf{W}_{o c} \mathbf{c}^{(t)}\right) \\\mathbf{h}^{(t)} &amp;=\mathbf{o}^{(t)} \odot \phi\left(\mathbf{c}^{(t)}\right)\end{aligned}\]</span></p><p>但是在这个模型中，我们可以发现一个明显的问题，模型只能关注字符上距离的上文内容而无法获取下文信息用于辅助分词。针对这一问题，就有了Bi-LSTM，用双向LSTM来利用上下文信息。</p><p>Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation</p><p>这篇论文使用了双向LSTM，即两个并行的LSTM分别来从左到右和从右到左来提取长距离字符特征。另外，模型输出为softmax之后的概率向量，比起之前的工作省略了viterbi推断的过程，变为end-end的模型。 <img src="/images/cws/7.png" title="Bi-LSTM进行分词" /></p><h1 id="multi-criteria-learning">Multi-Criteria Learning</h1><p>Adversarial Multi-Criteria Learning for Chinese Word Segmentation</p><p>不同的语料库有不同的分词标准，这篇文章的作者试图借助对多个不同标准的分词语料进行模型训练，提取中分词方法中最具普适性的部分。具体而言，模型借助Multi-criteria learning来获取多个准则的共享权重和独有权重，另一方面，通过鉴别器的对抗性训练来更好的实现这一过程，使得多准则的共享特性被更好地提取。</p><p>整体模型架构是建立在Bi—LSTM模型下，而针对具体的特征提取网络，作者针对多准则学习设计了三种网络结构，如下图所示 <img src="/images/cws/9.png" /> 其中黄色为共享LSTM层，灰色为私有LSTM层，<span class="math inline">\(\Theta^{m}, \Theta^{s}\)</span>分别代表他们的参数。每种模型都有两种准则进行训练，训练的目标函数是所有语料库上数据的条件似然。</p><p><span class="math display">\[\mathcal{J}_{s e g}\left(\Theta^{m}, \Theta^{s}\right)=\sum_{m=1}^{M} \sum_{i=1}^{N_{m}} \log p\left(Y_{i}^{(m)} \mid X_{i}^{(m)} ; \Theta^{m}, \Theta^{s}\right)\]</span></p><p>而为了确保共享层中没有参杂特定准则的私有信息，模型引入了一个鉴别器进行对抗训练。 <img src="/images/cws/8.png" title="对抗训练（以Model-III为例）" /> 判别器的任务是预测某一特征向量来源于 多准则语料中的哪一个。假如判别器能够准确预测每一个共享特征向量的来源语料，则说明这些共享特征中混入了太多私有信息。而共享LSTM层的目标则是让判别器无法鉴别输出的特征向量来源于哪个预料。模型通过令这两者进行对抗性训练，使得共享层能够提取出多个准则中最本质的分词特性。</p><h1 id="joint-cws-and-pos-tagging">Joint CWS and POS Tagging</h1><p>A Feature-Enriched Neural Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</p><p>本文的模型将Chinese word segmentation与part-of-speech (POS) tagging两个任务进行联合训练，因为其本质上都属于character based sequence labeling task。而论文作者的改进之处在于一个精心设计的特征提取网络 Feature-Enriched Neural Model。</p><p><img src="/images/cws/10.png" title="Feature-Enriched Neural Model" /></p><p>抛开原有的框架，这个模型增加了2个模块——Convolutional layer; Highway layer。</p><p>首先，卷积层用来对标传统方法中的手工字符特征部分。作者认为简单的神经模型只是将字符的局部信息嵌入并连接起来，不能模拟传统机器学习模型中精心设计的特征。为了像传统的基于特征的模型那样更好地建模复杂的字符特征，作者使用卷积层对每个特征分别建模不同的n-gram特征并串联，然后我们k-max池化层来选择最显著的部分。</p><p><span class="math inline">\(\hat{\mathbf{z}}_{i}^{q}\)</span>代表Q-gram特征 (uni-gram, bi-gram, ... ,)，而<span class="math inline">\(\mathbf{W}_{c o v}^{q} \in \mathbf{R}^{q d \times l_{q}}\)</span> 代表convolutional filter</p><p><span class="math display">\[\hat{\mathbf{z}}_{i}^{q}=\tanh \left(\mathbf{W}_{\operatorname{cov}}^{q}{ }^{\top} \times \mathbf{x}_{i-\left|\frac{q-1}{2}\right|: i+\left[\frac{q-1}{2}\right]}^{+}+\mathbf{b}\right), i \in[1, n]\]</span></p><p>而后分别进行concatenation、k-max pooling操作 <span class="math display">\[\mathbf{z}_{i}=\oplus_{q=1}^{Q} \hat{\mathbf{z}}_{i}\]</span></p><p>此时，输入的原始句子就会被表示为<span class="math inline">\(\hat{\mathbf{X}} \in \mathbf{R}^{n \times d}=\)</span> <span class="math inline">\(\left[\hat{\mathbf{x}}_{1}, \hat{\mathbf{x}}_{2}, \ldots, \hat{\mathbf{x}}_{n}\right]^{\top}\)</span>, 其中<span class="math inline">\(\hat{\mathbf{x}}_{i}\)</span>为:</p><p><span class="math display">\[{\hat{\mathbf{x}}}_{i}=k \max \mathbf{z}_{i}, k=d .\]</span></p><p>Highway layer则用来增加结构的深度，来模拟更复杂的组合特征。此外，highway加快了模型的收敛速度，缓解了梯度消失的问题。（这部分是参考Highway Netowrk（2015）的模型）</p><p>我们讲卷积后的句子表示为 <span class="math inline">\(\hat{\mathbf{X}}=\operatorname{Cov}(\mathbf{X})\)</span>，而其经过Highway layer后会得到 <span class="math display">\[\hat{\mathbf{X}}=\operatorname{Cov}(\mathbf{X}) \odot T(\mathbf{X})+\mathbf{X} \odot C(\mathbf{X})\]</span> 其中<span class="math inline">\(\odot\)</span>代表按元素乘运算，<span class="math inline">\(C(\cdot)=1-T(\cdot)\)</span>，而<span class="math inline">\(T(\cdot)\)</span>则可以写为： <span class="math display">\[T(\mathbf{X})=\sigma\left(\mathbf{W}_{T}{ }^{\boldsymbol{\top}} \times \mathbf{X}+\mathbf{b}_{T}\right)\]</span> 其中<span class="math inline">\(\mathbf{W}_{T} \in \mathbf{R}^{d \times d}\)</span>、<span class="math inline">\(\mathbf{b}_{T} \in \mathbf{R}^{d}\)</span>是可训练参数，<span class="math inline">\(\sigma\)</span>是sigmoid函数。</p><p>而接下来就将结果输入到BLSTM中，并借助CRF作为Decoder得到最终的结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;
&lt;p&gt;随着深度学习技术的发展，基于深度神经网络的中文分词模型也不断涌现，这类分词技术大致上可以分为两类，其中一类基于字符的中文分词方法将分词看作字符的标注问题。根据字在词中的位置，分别给每个中文字符打上B、M、E、S的标签，将分词转化为字的</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>中文分词｜Chinese-word-segmentation (1)</title>
    <link href="http://example.com/2022/02/07/cws/"/>
    <id>http://example.com/2022/02/07/cws/</id>
    <published>2022-02-07T13:01:01.000Z</published>
    <updated>2022-02-09T15:42:51.014Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景">背景</h1><p>中文文本中词与词之间没有明确的分割标记，而是以连续字符串形式呈现。所以，任何中文自然语言处理任务都必须解决中文序列切分的问题——中文分词。当然对于英文等语言来说自带分隔符，不需要分词。但在英语手写字识别时，由于分隔符没有那么明显，因此也需要使用类似中文分词的技术。</p><p>事实上不同的人对于同一个句子的分词结果并不一定相同，因此将机器分词的结果与人工分词的结果进行比对时，一个98%准确率的分词器与一个99%的分词器很难比较谁效果更好。在中文自然语言处理任务中，诸如机器翻译，自动问答，语音识别等任务，都需要分词技术做支撑。但是对于不同的子任务，分词的细粒度要求也不一样，尤其是对于一些复合词。比如在机器翻译时，清华大学就应该以整个“清华大学”而不是“清华-大学”。可以说中文分词在工程上是一项需要与任务要求紧密结合的技术。</p><p>中文分词存在两个难点，一是歧义，二是未登录词（OOV-out of vocabulary）。语言的歧义性一直伴随着语言学的发展进程，也自然限制着NLP技术。另一方面，词典的选择诸如词典中的复合词选择和词典的大小也影响着中文分词任务。举个直观的例子，对于Baidu搜索引擎的分词模型来说，基于人民日报的词典和基于用户搜索数据的词典得到的模型表现就会有很大差异。</p><h1 id="分词方法的演变">分词方法的演变</h1><h2 id="基于匹配的词典分词">基于匹配的词典分词</h2><p>基于匹配的词典分词是非常自然的想法，我们根据词典扫描一个句子，遇到词典中出现过的词就进行分割，这类方法又被称为机械分词。这类方法简单易实现，而且能取得不错的效果，其主要问题包括如何构建一个完备的词典、如何设计高效的匹配算法、匹配中出现的歧义切分。<br />常见的匹配算法包括:</p><ul><li>正向最大匹配法</li><li>逆向最大匹配法</li><li>双向最大匹配法</li><li>最少切分</li><li>......</li></ul><p>最大匹配法从句子中寻找长词条进行查字典，若查不到则去掉最后一个字直到找到为止。其中双向最大匹配分别从左到右和从右到左进行两次扫描。最小切分则是寻找使每一个句子切出的词数量最少。</p><h2 id="基于标注的机器学习算法">基于标注的机器学习算法</h2><p>不同于基于匹配的机械分词，基于统计语言模型的分词技术有效提高了分词的准确率。其中基于标注的机器学习算法将中文分词转化为字序列标注问题。，B表示开始位置、M表示中间位置、E表示结束位置及S表示单字构词。机器学习算法 需要人工设计特征模板，指定窗口的大小。由于算法的复杂度以及对分词结果准确度要求等原因，窗口大小一般不超过5。下面介绍几个具有代表性的模型：</p><ul><li><p>隐马尔可夫模型（HMM）隐马尔可夫不是一个复杂的数学模型，但能解决大多数自然语言处理问题。其基本的思想是根据观测值序列找到隐状态值序列。在中文分词中，一段文字的每个字符可以看作是一个观测值，而这个字符的位置标签（BEMS）可以看作是隐状态。使用HMM的分词，通过对切分语料库进行统计，可以得到模型中5大要要素：起始概率矩阵，转移概率矩阵，发射概率矩阵，观察值集合，状态值集合。有了三个矩阵和两个集合后，HMM问题最终转化成求解隐藏状态序列最大值的问题，求解这个问题最常使用的是Viterbi算法。</p></li><li><p>最大熵马尔可夫模型（MEMM）把HMM模型和maximum-entropy模型的优点集合程一个产生式模型，这个模型允许状态转移概率依赖于序列中彼此之间非独立的特征上，从而将上下文信息引入到模型的学习和识别过程中，提高了识别的精确度。</p></li><li><p>条件随机场（CRF）是用来标注和划分结构数据的概率化结构模型。和HMM类似，当对于给定的输入观测序列<span class="math inline">\(X\)</span>和输出序列<span class="math inline">\(Y\)</span>，CRF通过定义条件概率<span class="math inline">\(P(Y|X)\)</span>，而不是联合概率分布<span class="math inline">\(P(X,Y)\)</span>来描述模型。MEMM模型对每个节点进行独立归一化，存在偏置问题。条件随机场(CRF)结合了多方面优势，对所有特征进行全局归一化，避免了偏置问题，成为传统机器学习中应用最多、最具代表性的模型算法之一。条件随机场能够获得更高的分词准确率，但模型复杂导致分词效率略低。</p></li></ul><h2 id="基于理解的深度学习算法">基于理解的深度学习算法</h2><p>深度学习模型诸如CNN、GRU、LSTM、BiLSTM被引入中文分词，相对于机器学习而言，深度学习算法无需人工进行特征选择。在基础深度学习模型的基础上，有效结合预训练和后处理方式已成为深度学习的一种趋势，一般性流程如下图所示。 <img src="/images/cws/1.png" title="基于深度学习的中文分词" /> 预训练既可以根据领域需要和任务特点进行预训练，也可以直接使用现有的预训练结果进行微调。中文分词预训练的基本单位是词(字)的语义、偏旁、拼音和输人法等。语义表示的预训练模型包括与上下文无关的静态词向量训练模型Word2Vec、Glove以及与上下文相关的动态词向量训练模型ELMo、BERT、XLNet等。<br />近几年的中文分词主要分为两类，一个是基于字符的中文分词（根据字所在词的位置，对每个字打上标签），一类是基于词的中文分词。</p><h1 id="分词工具">分词工具</h1><h2 id="jieba">jieba</h2><p>jieba库是一个简单实用的中文自然语言处理分词库，属于概率语言模型分词。<br />jieba自带一个dict.txt的词典, 里面有2万多条词, 包含了词条出现的次数和词性。将句子根据给定的词典进行查词典操作, 生成所有可能的句子切分，而后根据动态规划查找最大概率路径, 找出基于词频的最大切分组合。对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法。<br />（jieba分词对“自然语言处理”的分词结果为 自然语言｜处理）</p><h2 id="ltp">LTP</h2><p>LTP是哈工大开源的一套中文语言处理系统，涵盖了基本功能：分词、词性标注、命名实体识别、依存句法分析、语义角色标注、语义依存分析等。LTP基于结构化感知器（Structured Perceptron,SP）属于基于字符的分词模型，以最大熵准则建模标注序列<span class="math inline">\(Y\)</span>在输入序列<span class="math inline">\(X\)</span>的情况下的score函数，分词结果则等同于最大score函数所对应的标注序列。 <span class="math display">\[S(Y, X)=\sum_{s} \alpha_{s} \Phi_{s}(Y, X)\]</span> <span class="math inline">\(\Phi_{s}(Y, X)\)</span>为特征函数，分词流程为先提取字符特征，计算特征权重值，然后Viterbi解码。</p><h2 id="thula">THULA</h2><p>THULA（THU Lexical Analyzer for Chinese）为清华大学推出的中文词法分析工具包，具有中文分词和词性标注功能，其原理与LTP非常相似，在字符特征选择方面有所不同。<br />测试发现THULA没有针对python3.8进行维护，因此只支持3.7-版本。</p><h2 id="stanford-corenlp">Stanford CoreNLP</h2><p>CoreNLP的中文分词基于CRF模型： <span class="math display">\[P_{w}(y \mid x)=\frac{\exp \left(\sum_{i} w_{i} f_{i}(x, y)\right)}{Z_{w}(x)}\]</span></p><p><span class="math inline">\(f_{i}(x, y)\)</span>为特征函数，<span class="math inline">\(w\)</span>为模型参数。不同于其他分词器采用B、M、E、S四种label来做分词，CoreNLP的中文分词label只有两种，“1”表示当前字符与前一字符连接成词，“0”则表示当前字符为另一词的开始——换言之前一字符为上一个词的结尾。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;背景&quot;&gt;背景&lt;/h1&gt;
&lt;p&gt;中文文本中词与词之间没有明确的分割标记，而是以连续字符串形式呈现。所以，任何中文自然语言处理任务都必须解决中文序列切分的问题——中文分词。当然对于英文等语言来说自带分隔符，不需要分词。但在英语手写字识别时，由于分隔符没有那么明显，因此</summary>
      
    
    
    
    <category term="NLP" scheme="http://example.com/categories/NLP/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>Improving Chinese Word Segmentation with Wordhood Memory Networks [ACL20]</title>
    <link href="http://example.com/2022/02/05/pd12/"/>
    <id>http://example.com/2022/02/05/pd12/</id>
    <published>2022-02-05T15:41:35.000Z</published>
    <updated>2022-02-09T16:35:42.372Z</updated>
    
    <content type="html"><![CDATA[<p>link: <a href="https://aclanthology.org/2020.acl-main.734/" class="uri">https://aclanthology.org/2020.acl-main.734/</a><br /><a href="https://github.com/SVAIGBA/WMSeg">代码</a></p><h1 id="背景">背景</h1><p>在中文自然语言处理中，分词是一个非常重要的任务。中文分词技术存在两个主要难点:未登录词（OOV）和歧义消除问题。本文属于基于字符的分词模型，主要思想是利用键值记忆网络来辅助分词，使分词的语义更加完整。</p><h1 id="模型框架">模型框架</h1><p>模型将中文分词作为序列标注问题，核心思想是在传统NER模型的Encoder-Decoder之间添加一个memory network，Encoder可以用BERT或BiLSTM等将汉字序列表示为向量，Decoder可以是Softmax或者CRF。模型总体可以表示为： <span class="math display">\[\widehat{\mathcal{Y}}=\underset{\mathcal{Y} \in \mathcal{T}^{l}}{\arg \max } p(\mathcal{Y} \mid \mathcal{X}, \mathcal{M}(\mathcal{X}, \mathcal{N}))\]</span> 其中<span class="math inline">\(\mathcal{T}\)</span>表示句子中所有分词结果的标签集;<span class="math inline">\(l\)</span>是句子长度。<span class="math inline">\(\widehat{\mathcal{Y}}\)</span>为该模型得到的最佳结果，<span class="math inline">\(\mathcal{N}\)</span>为构造的词典，<span class="math inline">\(\mathcal{X}\)</span>为输入句，<span class="math inline">\(\mathcal{M}\)</span>为本文提出的模型。</p><p><img src="/images/cws/2.png" title="The architecture of $WMS_{EG}$." /></p><h2 id="词典构建">词典构建</h2><p>本文构建的词典实际上是一个N-gram词典，它包含了一个句子中所有可能的N-gram。模型利用前人的模型——Accessor Variety，找出输入句子中所有可能的n-gram集合。根据上图给出的例子，所构建的词典如图底部所示。</p><h2 id="wordhood-memory-networks">Wordhood Memory Networks</h2><p>这部分是本文最重要的部分。作者利用键值记忆网络将字符n-gram与它们的词伙（wordhood）度量相结合。其中key-value分别对应n-grams和wordhood。具体可以分成两个步骤：</p><ul><li><p>Key Addressing<br />首先对该句子构建Lexicon，对每一个汉字<span class="math inline">\(x_i\)</span> ，有可能存在很多包含该汉字的n-gram。比如上面的句子中的第四个字&quot;民&quot;构建Lexicon，可以表示为： <span class="math display">\[K_{4}=[\text {&quot;民&quot;,&quot; 居民&quot;, &quot; 民生&quot;, &quot; 居民生活&quot;] }\]</span>然后将这些n-gram进行key embeding后<span class="math inline">\(e_{i, j}^{k}\)</span>再与Encoder传来的<span class="math inline">\(h_i\)</span> 相乘之后做softmax得到一个概率分布。概率大小就表明了相关程度： <span class="math display">\[p_{i, j}=\frac{\exp \left(h_{i} \cdot e_{i, j}^{k}\right)}{\sum_{j=1}^{m_{i}} \exp \left(h_{i} \cdot e_{i, j}^{k}\right)}\]</span></p></li><li><p>Value Reading<br />先将每个<span class="math inline">\(k_i\)</span>映射到一个标注值V上，因为每个字在不同的n-gram中的位置不同，所以需要映射的值也不同，这里使用B I E S标记法：(B:begin ，I:inside，E:end，S:single)，还是上面的例子，对应Key Addressing时的<span class="math inline">\(K_4\)</span>，得到的value集合为： <span class="math display">\[V_{4}=\left[V_{S}, V_{E}, V_{B}, V_{I}\right]\]</span>而后将每个value进行embedding得到<span class="math inline">\(e_{i, j}^{v}\)</span>，与上一步得到的概率进行相乘累加，得到的结果再与Encoder传入的<span class="math inline">\(h_i\)</span>结合得到最终输出的vector <span class="math inline">\(a_i\)</span>。 <span class="math display">\[O_{i}=\sum_{j=1}^{m_{i}} p_{i, j} e_{i, j}^{v}\]</span></p></li></ul><p>接下来就可以和正常的NER模型一样使用一个decoder（softmax、crf等等） 得到一个句子的分词结果了。</p><h1 id="实验">实验</h1><p>1.消融实验。作者使用了5个基准数据集，分别是CTB6、MSR、PKU、AS和CITYU。将Wordhood Memory Networks加入到目前主流的分词模型中进行比较，实验结果表明，虽然原始的模型有很好的性能，但加入Wordhood Memory Networks后仍有很大的改进<br />2.在五个基准数据集的测试集上，WMSEG和以前的SOTA模型的性能比较。经过实验比较，本文提出的模型在中文分词方面达到了SOTA水平，OOV的召回率有了很大的提高。</p><p><img src="/images/cws/3.png" title="Ablation experiments." /></p><h1 id="例子分析">例子分析</h1><p>作者用“他从小学习电脑技术”这一句子的分词结果进行可视化分析，发现通过Wordhood Memory Networks对n-gram语义的捕捉，给了“从小”一个较高的权重、&quot;小学&quot;一个较低的权重，得到了正确的分词结果。 <img src="/images/cws/4.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;link: &lt;a href=&quot;https://aclanthology.org/2020.acl-main.734/&quot; class=&quot;uri&quot;&gt;https://aclanthology.org/2020.acl-main.734/&lt;/a&gt;&lt;br /&gt;
&lt;a href=&quot;ht</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="CWS" scheme="http://example.com/tags/CWS/"/>
    
  </entry>
  
  <entry>
    <title>Graph Wavelet Neural Network[ICLR2019]</title>
    <link href="http://example.com/2022/01/26/pd11-1/"/>
    <id>http://example.com/2022/01/26/pd11-1/</id>
    <published>2022-01-26T04:20:13.000Z</published>
    <updated>2022-01-28T08:22:30.110Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1904.07785.pdf">Graph Wavelet Neural Network</a><br />Code: <a href="https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork">link</a></p><p>基于小波变换的图卷积神经网络，其背后的思想及理论基础在这一篇论文中无法很详尽的阐述。不过本篇论文相当于把一套传统方法迁移到图上。</p><h1 id="intro">Intro</h1><p>作者提出了graph wavelet neural network (GWNN)，对于GCN基于传统的图傅里叶变换，GWNN利用graph wavelet transform，用图小波代替图拉普拉斯特征向量作为一组基，利用小波变换和卷积定理定义了卷积算子。使得计算更高效，且在Cora, Citeseer和Pubmed三个图的半监督分类数据集上取得了更好的表现。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1904.07785.pdf&quot;&gt;Graph Wavelet Neural Network&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;https://github.com/benedekrozembercz</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graphite-Iterative Generative Modeling of Graphs [ICML2019]</title>
    <link href="http://example.com/2022/01/18/pd11/"/>
    <id>http://example.com/2022/01/18/pd11/</id>
    <published>2022-01-18T13:47:01.000Z</published>
    <updated>2022-01-20T01:53:00.164Z</updated>
    
    <content type="html"><![CDATA[<p>ICML2019的论文，作者为Stanford的phd。</p><h1 id="先说几句">先说几句</h1><p>看到摘要</p><blockquote><p>Our model parameterizes variational autoencoders (VAE) with graph neural networks, and uses a novel iterative graph refinement strategy inspired by low-rank approximations for decoding.</p></blockquote><p>基本可以确定模型相当于variational graph autoencoders，但是<a href="/2022/01/18/pd10/" title="VGAE">VGAE</a>这名字已经被人抢了XD。不过对比VGAE，它用了更优的Decoder。另一方面，作者给出了图神经网络中message passing与平均场变分推理之间的理论联系。</p><h1 id="graphite">Graphite</h1><p><img src="/images/pd10/8.png" /> 模型的整体架构与VGAE相似。其中Encoder为 <span class="math display">\[\boldsymbol{\mu}, \boldsymbol{\sigma}=\mathrm{GNN}_{\phi}(\mathbf{A}, \mathbf{X})\]</span> 接下来我们重点来看模型的Decoder。在VGAE中，Decoder是节点隐变量的内积，而Graphite提出了reverse message passing作为Decoder。 <span class="math display">\[\begin{aligned}\widehat{\mathbf{A}} &amp;=\frac{\mathbf{Z Z}^{T}}{\|\mathbf{Z}\|^{2}}+\mathbf{1 1}^{T} \\\mathbf{Z}^{*} &amp;=\operatorname{GNN}_{\theta}(\widehat{\mathbf{A}},[\mathbf{Z} \mid \mathbf{X}])\end{aligned}\]</span> 模型不断迭代上述两个步骤，也就是reverse message passing。 首先第一步借助隐变量矩阵的内积构建一个邻接矩阵（图）<span class="math inline">\(\widehat{\mathbf{A}} \in \mathbb{R}^{n \times n}\)</span>，加上单位矩阵保证非负。第二步中先将Z和X进行级联，而后与构建的图输入到GNN中。通过不断的重复来更新<span class="math inline">\(Z^*\)</span>，最后使用最终的Z进行内积得到生成的邻接矩阵<span class="math inline">\(\hat A\)</span>。另外，由于图学习通常是在大规模图上操作，在迭代过程中的求内积可以借助GNN这一步中的矩阵乘法来降低复杂度。</p><p><img src="/images/pd10/9.png" title="实验结果" /></p><h1 id="theoretical-analysis">Theoretical Analysis</h1><p>这一部分是作者对图神经网络的message passing与近似推断的关系。<br />首先我们定义kennel，<span class="math inline">\(K: \mathcal{Z} \times \mathcal{Z} \rightarrow \mathbb{R}\)</span>；映射函数 <span class="math inline">\(T_{\psi}: \mathcal{P} \rightarrow \mathcal{H}\)</span> 其中<span class="math inline">\(\mathcal{P}\)</span>定义了<span class="math inline">\(\mathcal{Z}\)</span>上所有分布的空间。因此可以定义对变量Z的分布的映射，或者成为kernel embedding： <span class="math display">\[T_{\psi}(p):=\mathbb{E}_{Z \sim p}[\psi(Z)]\]</span> 我们令这种映射<span class="math inline">\(\psi\)</span>是单射的，即对于任意两个分布<span class="math inline">\(p_1,p_2\)</span>，当<span class="math inline">\(p_{1} \neq p_{2}\)</span>，有<span class="math inline">\(T_{\psi}\left(p_{1}\right) \neq T_{\psi}\left(p_{2}\right)\)</span>。接下来我们定义 函数<span class="math inline">\(\mathcal{O}: \mathcal{P} \rightarrow \mathbb{R}^{d}, d \in \mathbb{N}\)</span>，对于每个<span class="math inline">\(T_{\psi}\)</span>和<span class="math inline">\(\mathcal{O}\)</span>，都存在<span class="math inline">\(\tilde{\mathcal{O}}_{\psi}: \mathcal{H} \rightarrow \mathbb{R}^{d}\)</span>使得: <span class="math display">\[\mathcal{O}(p)=\tilde{\mathcal{O}}_{\psi}\left(T_{\psi}(p)\right) \quad \forall p \in \mathcal{P}.\]</span></p><p>在GNN中，我们假设<span class="math inline">\(X\)</span>和<span class="math inline">\(A\)</span>都是观测到且在隐变量的条件概率分布中的相互独立的。也就是说我们期望的图是满足，对于<span class="math inline">\(Z\)</span>上的条件分布，当<span class="math inline">\(A\)</span>、<span class="math inline">\(X\)</span>和由边集<span class="math inline">\(E\)</span>确定的节点i的邻接潜变量时，任意单个<span class="math inline">\(Z_i\)</span>与所有其他<span class="math inline">\(Z_j\)</span>都是独立的。这句话非常抽象，可以从下图来理解。 <img src="/images/pd10/10.png" /></p><p>当图<span class="math inline">\(G\)</span>满足这一条件是，我们就可以用平均场理论 (mean-feild) <span class="math display">\[r\left(\mathbf{Z}_{1}, \cdots, \mathbf{Z}_{n} \mid \mathbf{A}, \mathbf{X}\right) \approx \prod_{i=1}^{n} q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)\]</span> 其中<span class="math inline">\(\phi_{i}\)</span> 代表第i个变分边界的参数。而后我们用真实的条件概率分布与上式的KL散度来优化这些参数: <span class="math display">\[\min _{\phi_{1}, \cdots, \phi_{n}} \mathrm{KL}\left(\prod_{i=1}^{n} q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right) \| r\left(\mathbf{Z}_{1}, \cdots, \mathbf{Z}_{n} \mid \mathbf{A}, \mathbf{X}\right)\right)\]</span></p><p>而最优的变分边界满足以下形式（证明见论文） <span class="math display">\[q_{\phi_{i}}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)=\mathcal{O}_{\mathcal{G}}^{M F}\left(\mathbf{Z}_{i},\left\{q_{\phi_{j}}\right\}_{j \in \mathcal{N}(i)}\right)\]</span> 其中<span class="math inline">\(\mathcal{N}(i)\)</span>为<span class="math inline">\(\mathbf{Z}_{i}\)</span>的邻节点。<span class="math inline">\(\mathcal{O}\)</span>是一个由不动点方程确定的函数，它依赖于图自身的性质。这一形式意味着最优的<span class="math inline">\(q_{\phi_{i}}\)</span>的参数只与i的邻节点的q_{}有关。这就意味着平均场近似推断的迭代算法是在图上执行信息传递，直到收敛： <span class="math display">\[q_{\phi_{i}}^{(l)}\left(\mathbf{Z}_{i} \mid \mathbf{A}, \mathbf{X}\right)=\mathcal{O}_{\mathcal{G}}^{M F}\left(\mathbf{Z}_{i},\left\{q_{\phi_{j}}^{(l-1)}\right\}_{j \in \mathcal{N}(i)}\right) .\]</span></p><p>令<span class="math inline">\(\boldsymbol{\mu}_{i}=\mathbb{E}_{\mathbf{Z}_{i} \sim q_{\phi_{i}}}\left[\psi\left(\mathbf{Z}_{i}\right)\right]\)</span>，根据上文提到的结论，我们就可以绕开 具体的<span class="math inline">\(\mathcal{O}\)</span>，将其变为 <span class="math display">\[\boldsymbol{\mu}_{i}^{(l)}=\tilde{O}_{\psi, \mathcal{G}}^{M F}\left(\left\{\boldsymbol{\mu}_{j}^{(l-1)}\right\}_{j \in \mathcal{N}(i)}\right)\]</span> 将上式在0处做一阶泰勒展开，有 <span class="math display">\[\mu_{i}^{(l)} \approx \tilde{O}_{\psi, \mathcal{G}}(\mathbf{0})+\mathbf{N}_{i}^{(l-1)} \cdot \nabla \tilde{O}_{\psi, \mathcal{G}}(\mathbf{0})\]</span> 这式从形式上就与message passing机制非常类似。 <span class="math display">\[H_{i}^{(l)}=\eta_{l}\left(B_{l, i}+\sum_{f \in \mathcal{F}_{l}} f\left(\mathbf{A}_{i}\right) \mathbf{H}^{(l-1)} W_{l}\right)\]</span> 由于在看推导过程时把一些分量的维度等忽略了，因此最后一步的具体细节直接在这里贴原文。</p><p><img src="/images/pd10/11.png" title="具体证明过程" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ICML2019的论文，作者为Stanford的phd。&lt;/p&gt;
&lt;h1 id=&quot;先说几句&quot;&gt;先说几句&lt;/h1&gt;
&lt;p&gt;看到摘要&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Our model parameterizes variational autoencoders (VA</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Graph Auto-Encoders</title>
    <link href="http://example.com/2022/01/18/pd10/"/>
    <id>http://example.com/2022/01/18/pd10/</id>
    <published>2022-01-18T13:39:46.000Z</published>
    <updated>2022-01-19T12:47:34.022Z</updated>
    
    <content type="html"><![CDATA[<p>本文来自Thomas Kipf的博士论文，其论文其他内容包括GCN、relational GCN、compositional imitation learning and execution等。</p><p>对于图<span class="math inline">\(G=(V,E)\)</span>，有<span class="math inline">\(N=|V|\)</span>，邻接矩阵<span class="math inline">\(A\)</span>为<span class="math inline">\(N \times N\)</span>，用<span class="math inline">\(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\)</span>来度量两个节点ij的相似度，其中<span class="math inline">\({z}_{i}, {z}_{j}\)</span>为节点的embedding</p><h1 id="gae">GAE</h1><p><img src="/images/pd10/1.png" title="encoder-decoder architecture" /></p><p>Graph Auto-Encoder同样采用encoder-decoder架构，其中scoring function <span class="math inline">\(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\)</span>作为decoder，其根据节点嵌入来重构邻接矩阵；Encoder的输入为图的邻接矩阵<span class="math inline">\(A\)</span>以及节点的特征向量<span class="math inline">\(\left\{\mathbf{x}_{i}\right\}_{i \in \mathcal{V}}\)</span>，输出为node representations <span class="math inline">\(\mathbf{Z}_{i}\)</span>。</p><ul><li><p>Encoder<br />GAE的Encoder借助GNN来处理节点的初始化向量和图的结构信息，从而得到节点的表示。比如使用GCN作为Encoder时，有： <span class="math display">\[\mathbf{Z}=\operatorname{GCN}(\mathbf{X}, \mathbf{A})=\widehat{\mathbf{A}} \operatorname{ReLU}\left(\widehat{\mathbf{A}} \mathbf{X} \mathbf{W}_{0}\right) \mathbf{W}_{1}\]</span> 除了用GNN作为encoder之外，还有其他的embedding方法，比如最简单的shallow embedding直接根据节点的编号，以及DeepWalk、node2vec等方法。</p></li><li><p>Decoder<br />Decoder用来根据<span class="math inline">\(Z\)</span>重建邻接矩阵 <span class="math display">\[\mathbf{A}^{\prime}=l\left(\mathbf{Z Z}^{\top}\right)\]</span> 其中<span class="math inline">\(l(.)\)</span>是logistic sigmoid function，也就是说<span class="math inline">\(A_{i, j}^{\prime}=l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)\)</span></p></li></ul><p>不过在图表示学习一书中给出了多种decoder <img src="/images/pd10/4.png" title="decoder" /></p><ul><li>Training<br />使用交叉熵进行训练 <span class="math display">\[\mathcal{L}=-\frac{1}{N^{2}} \sum_{i=1}^{N} \sum_{j=1}^{N} A_{i, j} \log l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)+\left(1-A_{i, j}\right) \log \left(1-l\left(s\left(\mathbf{z}_{i}, \mathbf{z}_{j}\right)\right)\right)\]</span></li></ul><h1 id="vae">VAE</h1><p><img src="/images/pd10/2.png" title="VAE" /></p><p>在介绍变分图自编码器 (VGAE)之前，我们先简单介绍一下变分自编码器Variational Auto-encoders。[Auto-Encoding Variational Bayes]</p><p><img src="/images/pd10/5.png" title="思路" /> 上图中的实线就代表了生成模型<span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z}) p_{\boldsymbol{\theta}}(\mathbf{x} \mid \mathbf{z})\)</span>，也就是根据隐变量z生成目标数据。而这一生成模型中，我们需要用<span class="math inline">\(q_{\phi}(\mathbf{z} \mid \mathbf{x})\)</span>来拟合无法得到的后验分布<span class="math inline">\(p_{\boldsymbol{\theta}}(\mathbf{z} \mid \mathbf{x})\)</span>。而这里就涉及到变分推断VI的内容。</p><p>接下来我们具体展开来说。</p><p>给定一个真实样本 <span class="math inline">\(x_{k}\)</span>, 我们假设存在一个后验分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span> 并假设这个分布是正态分布。后续我们要训练一个生成器<span class="math inline">\(x=g(z)\)</span>, 使其能从分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span> 采样出来的一个 <span class="math inline">\(\hat x_{k}\)</span> 还原为<span class="math inline">\(x_{k}\)</span>。<br />对于正态分布的两个参数：均值 <span class="math inline">\(\mu\)</span> 和方差 <span class="math inline">\(\sigma^{2}\)</span>，我们用神经网络进行拟合<span class="math inline">\(\mu_{k}=f_{1}\left(x_{k}\right), \log \sigma_{k}^{2}=f_{2}\left(x_{k}\right)\)</span>。再借助重参数技巧从<span class="math inline">\(z_k\)</span>的分布中采样得到<span class="math inline">\(z_k\)</span>。（对于重参数技巧，就是使采样这一步骤变为可导的一种方法）。</p><p>由于VAE最开始假设了隐变量服从正态分布，这就需要神经网络拟合的分布<span class="math inline">\(p\left(z \mid x_{k}\right)\)</span>向标准正态分布看起，因为 <span class="math display">\[p(z)=\sum_{x} p(z \mid x) p(x)=\sum_{x} \mathcal{N}(0, I) p(x)=\mathcal{N}(0, I) \sum_{x} p(x)=\mathcal{N}(0, I)\]</span> 使分布与标准正态看齐这一过程借助在loss增加一个额外的loss（生成分布与标准正态分布的KL散度）来实现 <span class="math display">\[\mathcal{L}_{\mu, \sigma^{2}}=\frac{1}{2} \sum_{i=1}^{d}\left(\mu_{(i)}^{2}+\sigma_{(i)}^{2}-\log \sigma_{(i)}^{2}-1\right)\]</span></p><p>上述的过程介绍从Auto-Encoder的角度来介绍VAE，事实上如果阅读原论文会发现这种介绍VAE的思路是很令人费解的。回到最基础的贝叶斯学习，我们需要<span class="math inline">\(q_{\phi}\left(\mathrm{z} \mid \mathrm{x}^{(i)}\right)\)</span> 去逼近真实的后验概率 <span class="math inline">\(p_{\theta}\left(\mathrm{z} \mid \mathrm{x}^{(i)}\right)\)</span>，很自然的我们选择用KL散度作为loss，而后经过变分推断的推到转换为优化变分下界 <span class="math display">\[\tilde{\mathcal{L}}\left(\theta, \phi ; \mathrm{x}^{(i)}\right)=\frac{1}{L} \sum_{l=1}^{L}\left[\log p_{\theta}\left(\mathrm{x}^{(i)}, \mathrm{z}^{(i, l)}\right)-\log q_{\phi}\left(\mathrm{z}^{(i, l)} \mid \mathrm{x}^{(i)}\right)\right]\]</span> 其中, <span class="math inline">\(\mathrm{z}^{(i, l)}=g_{\phi}\left(\epsilon^{(i, l)}, \mathrm{x}^{(i)}\right), \quad \epsilon^{(i, l)} \sim p(\epsilon)\)</span> 。</p><p>而VAE正是给定上述结果中<span class="math inline">\(\epsilon, p_{\theta}(\mathrm{x} \mid \mathrm{z}), q_{\phi}(\mathrm{z} \mid \mathrm{x}), p_{\theta}(\mathrm{z})\)</span> 分布具体形式（正态分布）后的特例。</p><h1 id="vgae">VGAE</h1><p><img src="/images/pd10/3.png" title="VGAE" /></p><p>对于变分图自编码器，简单来看就是输入变为节点特征和邻接矩阵，输出为生成的邻接矩阵。</p><p><span class="math inline">\(p_{\theta}(\mathbf{A} \mid \mathbf{X})\)</span>为节点特征<span class="math inline">\(X\)</span>与邻接矩阵A的条件概率分布 <span class="math display">\[p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{X})=\int p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X}) p(\mathbf{Z} \mid \mathbf{X}) d \mathbf{Z}\]</span> 其中隐变量先验分布独立于特征向量X，只和节点自身有关<span class="math inline">\(p(\mathbf{Z} \mid \mathbf{X})=\prod_{i=1}^{N} p\left(\mathbf{z}_{i}\right)\)</span>。更具体的说，我们令<span class="math inline">\(p\left(\mathbf{z}_{i}\right)=\mathcal{N}\left(\mathbf{z}_{i} ; \mathbf{0}, \mathbf{I}\right)\)</span>。我们的目标是得到最优的参数<span class="math inline">\(\theta\)</span>。</p><p>根据变分推断的框架，我们引入inference model： <span class="math display">\[q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A})=\prod_{i=1}^{N} q_{\phi}\left(\mathbf{z}_{i} \mid \mathbf{X}, \mathbf{A}\right)\]</span> 其中 <span class="math inline">\(q_{\phi}\left(\mathbf{z}_{i} \mid \mathbf{X}, \mathbf{A}\right)=\mathcal{N}\left(\mathbf{z}_{i} ; \boldsymbol{\mu}_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right)\)</span></p><p>具体模型中，我们用两个GCN作为学习inference model的参数： <span class="math display">\[\mu_{i}=\left[\mathrm{GCN}^{(1)}(\mathbf{X}, \mathbf{A})\right]_{i}\]</span> <span class="math display">\[\log \sigma_{i}=\left[\mathrm{GCN}^{(2)}(\mathbf{X}, \mathbf{A})\right]_{i}\]</span></p><p>接下来的generative model，我们假设它与初始输入的节点特征无关，只与隐变量有关， <span class="math display">\[p_{\boldsymbol{\theta}}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X})=\prod_{i=1}^{N} \prod_{j=1}^{N} p_{\boldsymbol{\theta}}\left(A_{i, j} \mid \mathbf{z}_{i}, \mathbf{z}_{j}\right)\]</span></p><p>模型所要优化的KL散度与变分推断的过程相似，可以变为优化 <span class="math display">\[\operatorname{ELBO}=\mathbb{E}_{q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A})}\left[\log p_{\theta}(\mathbf{A} \mid \mathbf{Z}, \mathbf{X})\right]-\operatorname{KL}\left[q_{\phi}(\mathbf{Z} \mid \mathbf{X}, \mathbf{A}) \| p(\mathbf{Z})\right].\]</span></p><p>我们可以将上述过程写的通俗一点，其中编码过程为： <span class="math display">\[q\left(z_{i} \mid X, A\right)=N\left(z_{i} \mid \mu_{i}, \operatorname{diag}\left(\sigma_{i}^{2}\right)\right).\]</span> 解码（以内积为例）的过程为： <span class="math display">\[p\left(A_{i j}=1 \mid z_{i}, z_{j}\right)=\sigma\left(z_{i}^{T} z_{j}\right)_{\circ}\]</span> 损失函数为： <span class="math display">\[L=E_{q(Z \mid X, A)}[\log p(A \mid Z)]-K L[q(Z \mid X, A) \| p(Z)]\]</span></p><p>作者在边预测任务上测试了VGAE的表现。不过，在Decoder时不考虑节点的特征<span class="math inline">\(X\)</span>仅仅是为了将模型简化，作者发现这不影响link prediction的准确率。 <img src="/images/pd10/6.png" /></p><p>值得推敲的是，在论文的前面有关于图卷积GCN在这几个数据集上的表现。而作者却没有用统一的评价指标（精度和准确率）来对比这两个模型的表现。</p><p><img src="/images/pd10/7.png" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文来自Thomas Kipf的博士论文，其论文其他内容包括GCN、relational GCN、compositional imitation learning and execution等。&lt;/p&gt;
&lt;p&gt;对于图&lt;span class=&quot;math inline&quot;&gt;\(G</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>QUINE&#39;S EMPIRICAL ASSUMPTIONS [1968]</title>
    <link href="http://example.com/2021/12/21/pd9/"/>
    <id>http://example.com/2021/12/21/pd9/</id>
    <published>2021-12-21T12:09:17.000Z</published>
    <updated>2021-12-23T02:02:25.713Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><h2 id="絮絮叨叨">絮絮叨叨</h2><p>为什么会突然看一篇60年代的文章？要从几天前我从学校图书馆借了一本叫《数学之美》的书说起。首先安利一下这本书，这书薄薄的似乎只有100页，但是书的作者的文学素养和专业知识让AI问题及其背后的数学充满了浪漫主义色彩。然后在书里刚开始关于自然语言处理的相关介绍中，我看到了一个名字-Noam Chomsky （乔姆斯基）。当时我并没有听说过他，于是就去搜索了一下，搜索结果几个关键词抓住了我的眼球：xxxx创始人、最伟大的学者之一。so, who is he?</p><h2 id="从规则到统计">从“规则”到“统计”</h2><p>自然语言处理作为人工智能下的子领域，自然也陪伴着人工智能走过几个“春季”与“冬季”。具体而言，NLP从早起的基于规则到如今的基于统计，背后不仅是深度学习技术的发展，也是其从“理想主义”为主流到“经验主义”占主导的转变过程。</p><blockquote><p>基于统计的自然语言处理是哲学中的经验主义，基于规则的自然语言处理是哲学中的理性主义。在哲学领域中经验主义与理性主义的斗争一直是此消彼长，这种矛盾与斗争也反映在具体科学上，如自然语言处理。 早期的自然语言处理具有鲜明的经验主义色彩。如1913年马尔科夫提出马尔科夫随机过程与马尔科夫模型的基础就是“手工查频”，具体说就是统计了《欧根·奥涅金》长诗中元音与辅音出现频度；1948年香农把离散马尔科夫的概率模型应用于语言的自动机，同时采用手工方法统计英语字母的频率。<br />然而这种经验主义到了乔姆斯基时出现了转变。<br />1956年乔姆斯基借鉴香农的工作，把有限状态机作用刻画语法的工具，建立了自然语言的有限状态模型，具体来说就是用“代数”和“集合”将语言转化为符号序列，建立了一大堆有关语法的数学模型。这些工作非常伟大，为自然语言和形式语言找到了一种统一的数学描述理论，一个叫做“形式语言理论”的新领域诞生了。但乔老爷子干完这一票之后，挥一挥衣袖，说了一句“有限状态模型不适合用来描述自然语言”。 随后老爷子又补了一刀“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”。 -------《统计自然语言处理》</p></blockquote><p>从如今language model在NLP中的表现来看这句“应当认识到‘句子的概率’这个概念，在任何已知术语的解释中，都是一个无用的概念”是非常的离谱，而网上的几篇博客（内容几乎一模一样😅）说这句如此绝对的话来源于<em>QUINE'S EMPIRICAL ASSUMPTIONS</em>这篇文章。因此本着对伟人的尊重和求真务实的态度，我就决定去读一下这篇文章。另外这篇文章是在springer出版的书籍。不过在说这篇文章的内容之前还需要补充介绍几个名词</p><h2 id="willard-van-orman-quine-avram-noam-chomsky">Willard Van Orman Quine &amp; Avram Noam Chomsky</h2><p>这里我们首先简单的介绍一下两个人。<br />WVO Quine就是文章标题中的这个Quine，文章中从他的作品<em>word and object</em>开始谈起。直接检索Willard Van Orman Quine会发现百度百科并没有收录这个词条，不过wikipedia倒是有。简单来说，Quine是美国一位著名的哲学家，主张经验主义，另外他倡导Semantic holism-语义整体论。语义整体论可以简单理解为语言的某个部分，无论是术语还是完整的句子，只能通过它与更大语言部分的关系来理解。</p><p>Avram Noam Chomsky也就是乔姆斯基（任在世），是美国哲学家，麻省理工学院语言学的荣誉退休教授。乔姆斯基的《句法结构》被认为是20世纪理论语言学研究上最伟大的贡献。<br />《句法结构》（Syntactic Structures）是乔姆斯基介绍转换生成语法的《语言学理论的逻辑结构》一书的精华版。这一理论认为说话的方式（词序）遵循一定的句法，这种句法是以形式的语法为特征的，具体而言就是一种不受语境影响并带有转换生成规则的语法。儿童被假定为天生具有适用于所有人类语言的基本语法结构的知识。这种与生俱来的知识通常被称作普遍语法理论。</p><h2 id="humean-theory">Humean Theory</h2><p>休谟（David Hume）是苏格兰不可知论哲学家。他认为人的认知是有局限的。在休谟看来，我们所能认知的“自我”，其实只是感知，人的感知受人的感官局限。休谟认为，因果关系是人的理念，我们倾向于把某种序列中理念间的必然联系归于这种因果关系的本质。也就是说，因果只是我们头脑中的理念而已，两个客体造成恒定感知，比如每天我们看到太阳升起——天就亮了，我们就会把二者视为因果关系。但并不是自然界真的存在因果关系。</p><h1 id="正文">正文</h1><p>这篇文章是对奎恩经验主义假设的解读，不过这篇文章阅读下来非常晦涩，因为作者在叙述Quine的理论时会通过各种逗号断句或小括号来表达自己的观点，所以文章的内容没有结构化的组织（就这还语言学家呢，就这就这），另外这一篇十多页的文章居然一个小标题都没有。</p><p>首先quine的理论来源于Humean theory of language acquisition，他认为人们对于语言的知识可以被表示为a network of linguistic，这也意味着人类的theory，比如chemistry这种二级学科或者基础的学科都可以被表示为a fabric of sentences variously associated to one another。进而人类所有的知识都可以用这些结构来描述。quine的理论中提到了“language”和“theory”。乔姆斯基指出理论与语言是相互渗透的，另外理论还涵盖了common-sense和belief。</p><blockquote><p>Beneath the uniformity that unites us in communication there is a chaotic <strong>personal diversity of connections</strong>, and, for each of us, <strong>the connections continue to evolve</strong>. No two of us learn our language alike, nor, in a sense, does any finish learning it while he lives.</p></blockquote><p>奎因表示如果语言是通过<strong>条件反应的机制相互关联</strong>并<strong>与外部刺激相关联</strong>的句子网络，那么一个人对言语行为的倾向可以根据这种网络来表征。按照这种语言的抽象形式，我们如何从语言中获取知识？奎恩提出了一个prelinguistic quality space，其中定义了距离度量（意味着可以度量相似度）。简单来说，在这个空间的某个维度上来看red ball， yellow ball之间的距离比red kerchief要近。这一想法似乎是背离经验主义的，因为这种质量空间可以想象和定义得到的，而非学习得到的。</p><p>然而，奎因在他关于语言是如何学习的假说中回到了经典的经验主义概念。与他认为语言是一个句子网络的观点相一致，他列举了学习句子的三种可能机制。首先，句子可以通过“直接条件反射”到“适当的非语言刺激”来学习，也就是说，通过在适当的条件下重复配对句子和刺激；第二，通过句子与句子的关联；第三，新句子可以通过“类比合成”产生，不过这种类比指的并不是类似英语语法规则的东西，而是在固定的上下文中用一个词代替一个类似的词（“手”、“脚”）。他认为一种语言是相关句子的有限网络，有些也与刺激相关，因为这只是两个假定的机制所产生的结构，具有实质性内容的语言学习。</p><p>但是乔姆斯基认为语言是句子的无限集合构成的。由假定的机制推导出的网络必定是有限的（对应上文的学习句子的机制），它只会包含人们曾经接触过的句子。</p><blockquote><p>Presumably, a complex of dispositions is a structure that can be represented as a set of probabilities for utterances in certain definable 'circumstances' or 'situations'. But it must be recognized that the notion 'probability of a sentence' is an entirely useless one, under any known interpreta- tion of this term.</p></blockquote><p>这里乔姆斯基给出了这句话——句子的概率是没有意义的。他举例说“birds fly”或者“Tuesday follows Monday”这两个英语下句子的概率对日语中产生这两个句子的概率没有意义。他认为probability relative to a situation没有任何意义。如果complex of dispositions是由根据经验观察确定的，那么只有少数传统的问候语、陈词滥调等才有可能与语言的倾向相关联，因为在技术意义上，在任何合理的语料库或数据集中，很少有其他句子可能具有非空的相对频率。且随着语料库的增加，任何给定句子的频率都会无限制地减少。 有人可能会设想用其他方法根据经验为句子分配概率，但乔姆斯基认为，没有一种方法可以避免这些困难。因此，如果一种语言被理解为在正常情况下作出反应的复杂倾向（奎恩的经验主义假说），那么它不仅是有限的、而且非常“小”。</p><p>Quine在提出“言语倾向”时指出了翻译的不确定性问题，简单来说可以理解为每个人的说话习惯几乎没有相似之处，因此根本无法建立与这种倾向相一致的翻译手册。对于理论和语言的有限性假设带来的问题，乔姆斯基提出语言是人类头脑的先天属性所带来的，存在一种“普遍语法”。</p><p>到这里，我们简单的概括一下前文提到的大概内容，即Quine的理论和乔姆斯基的看法:</p><blockquote><p>We are left with the fact that Quine develops his explicit notion of 'language' and 'theory' within a narrowly conceived Humean framework (except for the possible intrusion of a rich system of innate ideas), and that he characterizes language learning (&quot;learning of sentences&quot;) in a way consistent with this narrow interpretation, although the conclusion that <strong>a language (or theory) is a finite fabric of sentences, constructed pairwise by training, or a set of sentences with empirically detectable probabilities of being produced</strong> (hence a nearly empty set) is incompatible with various truisms to which Quine would certainly agree.</p></blockquote><p>Quine依靠他关于知识获取和语言学习的经验主义假设来支持他的一些主要哲学结论。一个重要的例子可以说明这一点。知识的基础是从某些证据上做“分析假设”。对Quine来说，一个关键点是，在基本语言和“常识知识”的情况下，分析假设的正确性并不是“客观问题”，它可以是“对或错”。这些分析假设是超越了 “任何一个本地人的言语行为倾向所隐含的任何东西”。因此，当我们在翻译、学习一门语言时，我们自然而然地会使用这些分析性假设（知识）与母语进行类比。也就是说在Quine的经验主义观点建模下超越言语倾向的知识（分析假设）是一个主观的概念，而这就会带来“翻译的不确定性”。</p><p>另外，Quine对基于数据的分析假设的构建和基于数据的“观察句子的刺激意义”的假设进行了鲜明的区分。他指出，后者只涉及“正常感应”类型的不确定性。显然，包含真值功能连接词的句子的翻译（类似地，学习和理解）中涉及的归纳推理也是如此。在这些情况下，归纳法将我们引向“真正的假设”，这与“分析假设”截然不同（在讨论翻译的不确定性时提到的“分析假设”）。因此，Quine认为“<strong>正常归纳</strong>”与“<strong>假设形成或理论建构</strong>”之间存在区别，前者不涉及严重的认识论问题，后者确实涉及此类问题。毫无疑问，这种区别是可以区分的；然而，Quine没有具体说明“正常归纳”所基于的先验属性。这里，乔姆斯基认为大脑天生具有允许从“正常归纳”到“真实假设”的属性，但不允许“理论建构”和一些可能受到狭隘限制的“分析假设”。也就是说，他认为在经验主义下根据数据进行归纳而后得到一个假设的真值（对或错）这个过程是合理，但是直接归纳知识这一过程是不合理的。<br />因此，一般来说关于语言不可能有一套固定的“分析假设”。我们需要为每种语言（更准确地说，为每种语言的每一个说话者）建立一套新的分析，因为语言的形式没有任何普遍性。</p><p>这里还是强调了乔姆斯基对于统计自然语言处理的观点，他认为每种语言，每个说话者的说话倾向会导致无法建立一套普遍的“分析假设”。因此乔姆斯基认为，当我们学习一门语言时，我们并不是在“学习句子”或通过训练获得“行为技能”。相反，我们以某种方式发展了某些原则（当然是无意识的），这些原则决定了许多句子的形式和意义。</p><h1 id="转换生成语法">转换生成语法</h1><p>在乔姆斯基的《句法结构》一书中，他提出了转换生成语法理论，他认为语言是人类特有的一种先天机制，不仅应该研究语言行为，而且应该研究语言能力，转换-生成语法就是关于语言能力的理论。具体而言，乔姆斯基认为语法主要包括基础和转换两个部分，基础部分生成深层结构，深层结构通过转换得到表层结构，语义部分属于深层结构，它为深层结构作出语义解释。语音部分属于表层结构并为表层结构作出语音解释。强调从认知学的角度对人类语言共性的解释，区分先天的语言能力和后天的语言知识，认为语言有生成能力，是有限规则的无限使用，转换则是生成的重要手段。</p><p>他的思想对当时主流的结构主义语言学产生了重要的影响。他的理论包含了几个关键的思想，首先是语义学是独立于语法学之外的，合乎语法的并不一定有意义。另外，他认为语言能力就像行走一样，是人与生俱来的理解语言及遣词造句的能力。</p><p>转换生成语法自创立以来, 就以对语言现象的解释充分性为目标, 试图建立一套能像数理公式般进行形式运算推理的规则来解释自然语言。期间虽经反复的修改否定再修改, 每一次都会有新的理论突破, 但其研究的对象、方法和原则却始终如一, 从而极大的推动了当代语言学的发展, 并为语言研究开辟了一条新的道路, 展现了一个全新的发展方向。<br />比如说，基于规则的句法剖析主要是使用Chomsky的上下文无关语法。在上下文无关语法的基础上, 学者们提出了自顶向下分析法、自底向上分析法、左角分析法、CYK算法、Earley 算法、线图分析法等行之有效的剖析技术。</p><p>关于基于规则的自然语言处理在工业界中的应用，可以参考这个链接 https://www.zhihu.com/question/30748126</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;h2 id=&quot;絮絮叨叨&quot;&gt;絮絮叨叨&lt;/h2&gt;
&lt;p&gt;为什么会突然看一篇60年代的文章？要从几天前我从学校图书馆借了一本叫《数学之美》的书说起。首先安利一下这本书，这书薄薄的似乎只有100页，但是书的作者的文学素养和专业知识让AI问题及其背</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="NLP" scheme="http://example.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Gel Candy Evaluation 凝胶糖果测评</title>
    <link href="http://example.com/2021/12/20/candies/"/>
    <id>http://example.com/2021/12/20/candies/</id>
    <published>2021-12-20T06:25:28.000Z</published>
    <updated>2022-01-05T03:20:44.092Z</updated>
    
    <content type="html"><![CDATA[<h1 id="abstract">Abstract</h1><p>Todo</p><h1 id="introduction">Introduction</h1><p>改革开放以来，人民生活水平日益提升，人们的物质文化生活也越来越丰富多彩。糖果作为小孩子最爱的零食，种类也越来越丰富。在跟王同学一起在研讨间学习的这段时间，在去往图书馆的路上，我偶尔会从天猫超市[1]或者苏果超市[2]买一包软糖给王同学，希望糖果的甜能为她备战考研的枯燥时光中带来些许快乐。由于笔者在这段时间中有点咳嗽，医生叮嘱不可以吃甜食，因此在测评过程中每包糖果我也只吃了几颗（有时一转眼整包就被某人吃完了），但是保证了每一类糖果中包含的每种味道都吃过。下面将对两个超市中售卖的几种凝胶糖果进行测评和打分。对于糖果的评价指标，由于笔者才疏学浅，只能使用个人主观评价以及王同学对部分糖果的一些评价进行评估。</p><h1 id="method">Method</h1><h2 id="gel-candy">Gel Candy</h2><h1 id="experiment">Experiment</h1><h2 id="evaluation">Evaluation</h2><p>笔者对于这些糖果的味道和口感按照1～5🌟进行打分，另外附带一些文字评价。另外王同学的评价一般包括：就这，还行，味太大，一般。</p><h2 id="alpenliebe">Alpenliebe</h2><ul><li>乐嚼Q动物果果</li><li>乐嚼Q虫虫派对</li><li>乐嚼Q乳酸果果</li><li>乐嚼Q熊猫小队</li></ul><h2 id="skittles">Skittles</h2><h2 id="uha">UHA</h2><h1 id="reference">Reference</h1><p>【1】东南大学九龙湖校区梅园天猫超市<br />【2】东南大学九龙湖校区桃园苏果超市</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;
&lt;p&gt;Todo&lt;/p&gt;
&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;改革开放以来，人民生活水平日益提升，人们的物质文化生活也越来越丰富多彩。糖果作为小孩子最爱的零食，种类也越</summary>
      
    
    
    
    <category term="杂七杂八" scheme="http://example.com/categories/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB/"/>
    
    
  </entry>
  
  <entry>
    <title>Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks [ACL2020]</title>
    <link href="http://example.com/2021/12/08/pd8/"/>
    <id>http://example.com/2021/12/08/pd8/</id>
    <published>2021-12-08T05:49:22.000Z</published>
    <updated>2021-12-14T13:04:34.134Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍的内容包含多篇基于图神经网络进行文本分类的论文，从提出GCN后的简单应用到去年ACL上的TextING以及今年的BertGCN，GNN在文本分类上取得了非常好的效果。</p><p><a href="https://arxiv.org/abs/1809.05679">Graph Convolutional Networks for Text Classification</a><br /><a href="https://arxiv.org/abs/1910.02356v2">Text Level Graph Neural Network for Text Classification</a><br /><a href="https://arxiv.org/abs/2004.13826v1">Every Document Owns Its Structure - Inductive Text Classification via Graph Neural Networks</a><br /><a href="https://arxiv.org/abs/2105.05727">BertGCN: Transductive Text Classification by Combining GCN and BERT</a></p><h1 id="text-classification">Text Classification</h1><p>文本分类的应用非常广泛，包括Sentiment Analysis、News Categorization、Topic Analysis甚至是Question Answering等。而automatic text classification方法大致可以被归为两类：基于规则的^rule-based以及基于机器学习的^数据驱动的。</p><p>基于规则的文本分类需要先验知识，包括pre-defined rules、domain knowledge等。而基于机器学习的方法则是借助标注数据集。通常我们可以把机器学习文本分类的步骤归为两步，首先是文本特征提取，而后将特征输入分类器进行分类。而发展到现在，基于神经网络的文本分类模型大致也随着深度学习的发展从前馈神经网络到CNN、RNN以及attention、transformer到如今pre-trained model如BERT等。</p><p>前馈神经网络进行文本分类通常将文本作为bag of words；RNN则把文本作为词的序列；CNN用于训练提取文本中的关键短语、词组等进行匹配分类；attention机制能识别文本中相互关联的词，可以嵌入到其他深度学习模型中；至于transformer以及BERT这类大规模预训练模型，则是“大力出奇迹”。</p><p>而本文的重点，则是基于GNN-图神经网络的文本分类方法。借助图神经网络进行文本中句法、语义解析树之类的图结构信息挖掘，进而进行文本分类。另外经过下文的介绍我们还能发现，基于GNN的模型能与其他深度神经网络进行级联并进行联合训练，进而有效提升分类准确率。<br />基于图神经网络的文本分类模型的差异大致体现在三个方面：图的构建、节点嵌入的初始化、图神经网络。</p><h1 id="textgcn">TextGCN</h1><p>Graph Convolutional Networks for Text Classification<br /><img src="/images/gcn/2.png" title="Framework of TextGCN" /> TextGCN为AAAI2018的论文，现在很多人看到这篇文章的时候可能会感叹“这也能发？”，但事实上这篇论文是最先构建了transductiive的基于GNN进行文本分类的框架，并取得了非常好的表现。</p><p>模型整体的框架如上图所示，包括图的构建和图神经网络两个模块。其中图神经网络由简单的两层卷积层构成。另一方面，图节点的特征初始化用one-hot vector，输入的信息仅为边信息和节点的结构信息，而其在分类结果上取得的准确率也反映了GNN应用文本分类的合理性。 <span class="math display">\[Z=\operatorname{softmax}\left(\tilde{A} \operatorname{ReLU}\left(\tilde{A} X W_{0}\right) W_{1}\right)\]</span></p><p>对于构图方面，模型基于整个语料库构建一个异构图，图中的节点包括文档节点和词节点。而这两类节点之间的边，word-word、doc-word定义如下 <span class="math display">\[A_{i j}=\left\{\begin{array}{ll}\operatorname{PMI}(i, j) &amp; i, j \text { are words, } \operatorname{PMI}(i, j)&gt;0 \\\operatorname{TF}-\operatorname{IDF}_{i j} &amp; i \text { is document, } j \text { is word } \\1 &amp; i=j \\0 &amp; \text { otherwise }\end{array}\right.\]</span> 其中词i与j之间的PMI (point-wise mutual information) 计算为 <span class="math display">\[\begin{aligned}\operatorname{PMI}(i, j) &amp;=\log \frac{p(i, j)}{p(i) p(j)} \\p(i, j) &amp;=\frac{\# W(i, j)}{\# W} \\p(i) &amp;=\frac{\# W(i)}{\# W}\end{aligned}\]</span> <span class="math inline">\(\#W(i,j)\)</span>代表滑窗中两个词共现次数。另外，图中的词节点会将语料库统计后的低频词过滤掉。至于为什么选择PMI以及TF-IDF这两个指标作为边权重，作者提到是从实验的结果出发作出的选择。 <img src="/images/pd8/2.png" title="Results: TextGCN" /></p><h1 id="text-level-gnn">Text-level GNN</h1><p>Text Level Graph Neural Network for Text Classification [ENMLP2019]<br />TextGCN模型跟大部分直推式GNN模型一样，应用时存在明显的缺陷，即没有办法进行在线测试。当我们要输入的新文本进行分类时，需要将文本加入语料库后重新构建graph训练，这就会带来极大的开销。而这篇Text-level GNN则是构建基于文本级别的图，使得基于GNN的文本分类模型提供在线测试的功能，虽然模型依旧是Transductive。 <img src="/images/pd8/3.png" title="Text-level GNN" /> Text-level图的构建如上图所示，其中词与词之间连接的边权重以及词的embedding为整个语料库全局共享，保存在全局矩阵中（上图的两个矩阵），另外文档中的每个词不止和相邻的词存在边，而由一个超参数控制多跳邻居。</p><p>在图神经网络模块，虽然论文中介绍的是non-spectral message passing mechanism，但事实上与TextGCN本质上是一样的，不过边的权重会在训练过程中进行更新。 <span class="math display">\[\begin{aligned}\mathbf{M}_{\mathbf{n}} &amp;=\max _{a \in \mathcal{N}_{n}^{p}} e_{a n} \mathbf{r}_{\mathbf{a}} \\\mathbf{r}_{\mathbf{n}}^{\prime} &amp;=\left(1-\eta_{n}\right) \mathbf{M}_{\mathbf{n}}+\eta_{n} \mathbf{r}_{\mathbf{n}}\end{aligned}\]</span> 上式中<span class="math inline">\(r\)</span>代表节点特征，<span class="math inline">\(\eta_{n}\)</span>为可训练的参数。<br />最后，使用文本中的所有词的embedding进行类别的推断；而在TextGCN中则是直接使用文档节点的embedding进行分类。 <span class="math display">\[y_{i}=\operatorname{softmax}\left(\operatorname{Relu}\left(\mathbf{W} \sum_{n \in N_{i}} \mathbf{r}_{\mathbf{n}}^{\prime}+\mathbf{b}\right)\right)\]</span></p><p>在这里简单对Corpus-level GNN（TextGCN）和Text-level GNN进行简单的比较。首先两者在下游任务的准确率上有较小的差异，其中后者略胜一筹，不过后者使用了Glove词向量进行初始化，所以事实上将TextGCN使用一些小技巧后两者的准确率是非常接近的。不过Text-level GNN优越性体现在其能够提供在线测试上，当输入新文档进行分类时，它的计算开销会远小于TextGCN。而对于它使用的MPM神经网络而不是GCN，是因为MPM更适合它的构图模式，而不是MPM比常规的GCN更强的信息提取能力。Text-level使得全局的边权重必须成为可训练的参数，而MPM中的另一个可训练的参数<span class="math inline">\(\eta_{n}\)</span>实质上与GCN中结合<span class="math inline">\(I\)</span>的拉普拉斯矩阵<span class="math inline">\(L\)</span>是一致的。</p><h1 id="texting">TextING</h1><p>Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks<br />上文的两个模型以及后续的BertGCN都是transductive，而本文的TextING则是inducive模型。论文发表在ACL2020上，也就是博客标题。 <img src="/images/pd8/4.png" title="TextING" /> 模型针对每篇文档构建一个图，以词共现作为边节点，借助滑窗（size 3）构建图。节点嵌入用Glove进行初始化。 模型的图神经网络模块使用了Gated Graph Neural Networks(GGNN)和图注意力(GAT)。 <span class="math display">\[\begin{aligned}\mathbf{a}^{t} &amp;=\mathbf{A h}^{t-1} \mathbf{W}_{a} \\\mathbf{z}^{t} &amp;=\sigma\left(\mathbf{W}_{z} \mathbf{a}^{t}+\mathbf{U}_{z} \mathbf{h}^{t-1}+\mathbf{b}_{z}\right) \\\mathbf{r}^{t} &amp;=\sigma\left(\mathbf{W}_{r} \mathbf{a}^{t}+\mathbf{U}_{r} \mathbf{h}^{t-1}+\mathbf{b}_{r}\right) \\\tilde{\mathbf{h}}^{t} &amp;=\tanh \left(\mathbf{W}_{h} \mathbf{a}^{t}+\mathbf{U}_{h}\left(\mathbf{r}^{t} \odot \mathbf{h}^{t-1}\right)+\mathbf{b}_{h}\right) \\\mathbf{h}^{t} &amp;=\tilde{\mathbf{h}}^{t} \odot \mathbf{z}^{t}+\mathbf{h}^{t-1} \odot\left(1-\mathbf{z}^{t}\right)\end{aligned}\]</span></p><p>上式中<span class="math inline">\(h\)</span>表示节点embedding，<span class="math inline">\(a\)</span>代表接受的信息，<span class="math inline">\(z\)</span>和<span class="math inline">\(r\)</span>分别代表更新和遗忘。而后将节点借助readout模块输出为graph-level的embedding： <span class="math display">\[\begin{array}{l}\mathbf{h}_{v}=\sigma\left(f_{1}\left(\mathbf{h}_{v}^{t}\right)\right) \odot \tanh \left(f_{2}\left(\mathbf{h}_{v}^{t}\right)\right) \\\mathbf{h}_{\mathcal{G}}=\frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \mathbf{h}_{v}+\text { Maxpooling }\left(\mathbf{h}_{1} \ldots \mathbf{h}_{\mathcal{V}}\right)\end{array}\]</span></p><p>上文提到的两个模型中GNN并没有attention模块，这是由于TextGCN的PMI、TF-IDF信息会损失，另一方面Text-level GNN全局的边权重也不应该引入文本图中的attention机制进行更新。</p><p>将上文的三个模型进行简单的对比，可以隐约感到存在一个图的构建与图神经网络之间的trade-off。前两个模型的图构建过程都嵌入了大量的信息（先验信息、全局信息），而他们的图神经网络都非常简单。事实上我曾在TextGCN上做过一些实验，尝试把GAT融入到信息传播过程中，发现准确率会有明显下降。而TextING的构图过程中的信息仅是词节点的共现所包含的上下文信息和结构信息，因此它可以接受更复杂的信息传播和聚合过程。这也使得TextING可以面对新词和新输入的文本直接进行分类。</p><h1 id="bertgcn">BertGCN</h1><p>BertGCN: Transductive Text Classification by Combining GCN and BERT<br />BertGCN是由香侬科技提出，发表在ACL2021上的文章，也是目前文本分类的SOTA模型。不过这篇文章的贡献主要是工程上的。另外，不妨排列组合一下将Bert与TextING结合，应该能取得更好的结果XD（虽然在R8上的结果已经非常接近100了）。<br />回顾TextGCN，模型中图节点初始化用的是one-hot向量，而BertGCN则是用Bert进行embedding的初始化，另外将Bert与GNN两个模块进行联合训练，取得了很好的表现。</p><p>两者的结合存在两个问题，一是难收敛：BERT与GCN处理数据的方式不同、模型大小不同；二是GCN是在整个图上运算，而BERT过大的模型无法一次全部加载图中所有结点，这就给BertGCN的训练带来阻碍。 针对第一个问题，模型使用了Interpolating损失。 <span class="math display">\[Z=\lambda Z_{\mathrm{GCN}}+(1-\lambda) Z_{\mathrm{BERT}}, Z_{\mathrm{BERT}}=\operatorname{softmax}(W X)\]</span> 当<span class="math inline">\(\lambda=1\)</span>时，BERT模块没有更新；当<span class="math inline">\(\lambda=0\)</span>时，GCN模块没有更新；当<span class="math inline">\(\lambda \in (0,1)\)</span>时，两个模块都能得到更新，并且通过调节<span class="math inline">\(\lambda\)</span>实现BertGCN整体模块的快速收敛。<br />对于无法整图训练这一问题，BertGNN提出了一个Memory Bank用于保存所有节点特征，每次从中加载batch进行训练并更新，其他保持不变。将整个语料库中的文档特征分批更新，为了防止异步更新带来的不一致性，模型在训练Bert模型时采用了小学习率。</p><p><img src="/images/pd8/1.png" title="Results: BertGCN" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文介绍的内容包含多篇基于图神经网络进行文本分类的论文，从提出GCN后的简单应用到去年ACL上的TextING以及今年的BertGCN，GNN在文本分类上取得了非常好的效果。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.05679&quot;</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="Text Classification" scheme="http://example.com/tags/Text-Classification/"/>
    
  </entry>
  
  <entry>
    <title>就叫“深入浅出图卷积(GCN)”吧</title>
    <link href="http://example.com/2021/11/28/gcn/"/>
    <id>http://example.com/2021/11/28/gcn/</id>
    <published>2021-11-28T10:35:25.000Z</published>
    <updated>2021-12-02T01:53:08.111Z</updated>
    
    <content type="html"><![CDATA[<p>图卷积网络作为图神经网络之一，具有非常广泛的应用。因此网上也有非常多的关于GCN的介绍，但是各种博客看的多了搞得我脑壳嗡嗡的，而刚好最近的一个工作涉及到GCN的内核，因此借助这篇博客整理对GCN进行整理。</p><p>在介绍GCN先介绍几篇文献：<br />Semi-Supervised Classification with Graph Convolutional Networks-首次提出GCN的论文（ICLR2017)<br />Data Analytics on Graphs-图机器学习的教材<br /><a href="https://www.zhihu.com/question/54504471" class="uri">https://www.zhihu.com/question/54504471</a>-介绍GCN的博客</p><h2 id="gnn-message-passing">GNN &amp; Message Passing</h2><p>GNN作为一种graph embedding的手段，可以借助节点特征的message passing提取图结构信息 <span class="math display">\[\begin{aligned} \mathbf{h}_{u}^{(k+1)} &amp;=\operatorname{UPDATE}^{(k)}\left(\mathbf{h}_{u}^{(k)}, \text { AGGREGATE }^{(k)}\left(\left\{\mathbf{h}_{v}^{(k)}, \forall v \in \mathcal{N}(u)\right\}\right)\right) \\ &amp;=\operatorname{UPDATE}^{(k)}\left(\mathbf{h}_{u}^{(k)}, \mathbf{m}_{\mathcal{N}(u)}^{(k)}\right) \end{aligned}\]</span></p><p><img src="/images/gcn/1.png" title="GNN Framework" /> GNN进行k轮迭代，每轮包括一个聚合（aggregate）和更新（update）操作。聚合来获取邻节点的信息，而后更新节点的自身特征。这种多轮message passaging的机制使得图的结构信息以及邻节点特征被提取到节点的特征中。广义上来说，GCN也是GNN中的一种。GCN的message passaging非常的简单。从下式（最基础形式的GCN）可以看出，GCN借助边信息对节点信息进行聚合。 <span class="math display">\[f\left(H^{(l)}, A\right)=\sigma\left(A H^{(l)} W^{(l)}\right)\]</span></p><h2 id="gnn-cnn">GNN &amp; CNN</h2><p>对于图像来说，nxn的卷积核可以作为图像中的特征提取器（为了防止图和图像这两个词的太过接近导致可能出现的问题，下文提到图时将用graph）。但是这种卷积操作无法直接用在Graph上，为什么？<br />由于图像与graph的数据特性和卷积操作的特性。首先，图像具有局部平移不变性(local translational invariance)，使得卷积核能够对图像矩阵进行扫描卷积；而graph作为非欧空间的数据 (Non Euclidean Structure)，每个节点邻接点的各异导致传统CNN操作无法应用。第二，卷积核是参数共享的，且可以实现层次化特征提取-卷积层可以在前一层的基础上提取更高阶的特征；而graph的层数加深则是使节点获取更广的感受野。</p><h2 id="spectrum">Spectrum</h2><p>参考<a href="https://zhuanlan.zhihu.com/p/120311352" class="uri">https://zhuanlan.zhihu.com/p/120311352</a><br />在空间域上的图卷积碰壁并不意味着在图上没法进行操作，我们可以从频域中进行分析。</p><h3 id="图的拉普拉斯矩阵">图的拉普拉斯矩阵</h3><p>首先定义几个概念：<br />在图上最基本的拉普拉斯矩阵Laplacian matrix为： <span class="math display">\[\mathbf{L}=\mathbf{D}-\mathbf{A}\]</span> 其中<span class="math inline">\(\mathbf{D}\)</span>为度矩阵，<span class="math inline">\(\mathbf{A}\)</span>为邻接矩阵。拉普拉斯矩阵有一些基本的性质：对称 (<span class="math inline">\(\mathbf{L}^{T}=\mathbf{L}\)</span>)；半正定 (<span class="math inline">\(\mathbf{x}^{\top} \mathbf{L} \mathbf{x} \geq 0, \forall \mathbf{x} \in \mathbb{R}^{|\mathcal{V}|}\)</span>)，这也意味着拉普拉斯矩阵的特征值都是非负的：<span class="math inline">\(0=\lambda_{|\mathcal{V}|} \leq \lambda_{|\mathcal{V}|-1} \leq \ldots \leq \lambda_{1}\)</span>。 <span class="math display">\[\begin{aligned}\mathbf{x}^{\top} \mathbf{L} \mathbf{x} &amp;=\frac{1}{2} \sum_{u \in \mathcal{V}} \sum_{v \in \mathcal{V}} \mathbf{A}[u, v](\mathbf{x}[u]-\mathbf{x}[v])^{2} \\&amp;=\sum_{(u, v) \in \mathcal{E}}(\mathbf{x}[u]-\mathbf{x}[v])^{2}\end{aligned}\]</span> 另外，对称规范化拉普拉斯矩阵symmetric normalized Laplacian定义如下，这是GCN相关工作中比较常用的。 <span class="math display">\[\mathbf{L}_{\mathrm{sym}}=\mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}}\]</span></p><h3 id="拉普拉斯算子">拉普拉斯算子</h3><p>接下来我们来一步步理解为什么要这样定义图的拉普拉斯矩阵。对于空间中的任意函数<span class="math inline">\(f\)</span>来说， <span class="math display">\[\Delta f=\nabla^{2} f=\nabla \cdot \nabla f =\sum_{i=1}^{n} \frac{\partial^{2} f}{\partial x_{i}^{2}}\]</span> 拉普拉斯算子 (Laplacian)是欧式空间中的函数梯度的散度 (Divergence)对应的微分算子。在n维空间中计算的是函数各个维度二阶偏导的和。在二维空间中，可以<strong>近似</strong>为差分的计算 <span class="math display">\[\begin{aligned}\Delta f(x, y) &amp;=\frac{\partial^{2} f}{\partial x^{2}}+\frac{\partial^{2} f}{\partial y^{2}} \\&amp;=[f(x+1, y)+f(x-1, y))-2 f(x, y)]+[f(x, y+1)+f(x, y-1))-2 f(x, y)] \\&amp;=f(x+1, y)+f(x-1, y))+f(x, y+1)+f(x, y-1))-4 f(x, y)\end{aligned}\]</span> 上式事实上就是在图像上作用拉普拉斯卷积核 <span class="math display">\[\begin{array}{|r|r|r|}\hline 0 &amp; 1 &amp; 0 \\\hline 1 &amp; -4 &amp; 1 \\\hline 0 &amp; 1 &amp; 0 \\\hline\end{array}\]</span> 因此拉普拉斯算子可以理解为——在所有自由度上进行微小变化后所获得的增益。<br />而将其推广到有N节点的graph上时，<span class="math inline">\(f\)</span>维度最高为N，<span class="math inline">\(f=\left(f_{1}, \ldots, f_{N}\right)\)</span>。其中<span class="math inline">\(f_{i}\)</span> 表示函数<span class="math inline">\(f\)</span>在网络图中节点i处的函数值, 类比<span class="math inline">\(f(x, y)\)</span>为函数<span class="math inline">\(f\)</span>在 <span class="math inline">\((\mathrm{x}, \mathrm{y})\)</span>的函数值。<br />因此当拉普拉斯算子作用在加权graph（<strong>边权重为</strong><span class="math inline">\(w_{i j}\)</span>）上时，借助差分近似后有： <span class="math display">\[\begin{aligned}\Delta \boldsymbol{f}_{i} &amp;=\sum_{j \in N_{i}} \frac{\partial f_{i}}{\partial j^{2}} \\&amp; \approx \sum_{j} w_{i j}\left(f_{i}-f_{j}\right) \\&amp;=\sum_{j} w_{i j}\left(f_{i}-f_{j}\right) \\&amp;=\left(\sum_{j} w_{i j}\right) f_{i}-\sum_{j} w_{i j} f_{j} \\&amp;=d_{i} f_{i}-w_{i:} f\end{aligned}\]</span> 对于任意<span class="math inline">\(i \in N\)</span>都成立，所以就得到了： <span class="math display">\[\begin{aligned}\Delta f=\left(\begin{array}{c}\Delta f_{1} \\\vdots \\\Delta f_{N}\end{array}\right) &amp;=\left(\begin{array}{cc}d_{1} f_{1}-w_{1:} f \\\vdots \\d_{N} f_{N}-w_{N:} f\end{array}\right) \\&amp;=\left(\begin{array}{ccc}d_{1} &amp; \cdots &amp; 0 \\\vdots &amp; \ddots &amp; \vdots \\0 &amp; \cdots &amp; d_{N}\end{array}\right) f-\left(\begin{array}{c}w_{1:} \\\vdots \\w_{N:}\end{array}\right) f \\&amp;=\operatorname{diag}\left(d_{i}\right) f-\mathbf{W} f \\&amp;=(\mathbf{D}-\mathbf{W}) f \\&amp;=\mathbf{L} f\end{aligned}\]</span> 这就意味着，对由图节点特征构成的向量<span class="math inline">\(f\)</span>做拉普拉斯等价于图拉普拉斯矩阵与向量<span class="math inline">\(f\)</span>进行点积。</p><h3 id="graph-fourier-transformer">Graph Fourier Transformer</h3><p>拉普拉斯矩阵的特征分解 <span class="math display">\[\mathbf{L} \mathbf{u}_{\mathbf{k}}=\lambda_{k} \mathbf{u}_{\mathbf{k}}\]</span> 继而进行正交相似对角化后就得到 <span class="math display">\[\mathbf{L}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{-1}=\mathbf{U}\left(\begin{array}{ccc}\lambda_{1} &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \\&amp; &amp; \lambda_{n}\end{array}\right) \mathbf{U^{-1}}=\mathbf{U}\boldsymbol{\Lambda} \mathbf{U}^{T}\]</span> 其中<span class="math inline">\(\boldsymbol{\Lambda}\)</span> 为特征值构成对角矩阵, <span class="math inline">\(\mathbf{U}\)</span> 为特征向量构成的正交矩阵。</p><p>在这里补充一条性质 <span class="math display">\[\Delta e^{-i \omega t} =\frac{\partial^{2} e^{-i \omega t}}{\partial t^{2}}= -\omega^{2} e^{-i \omega t}\]</span> 从广义上来看，这符合特征方程<span class="math inline">\(AV=\lambda V\)</span>的定义，也就是说<span class="math inline">\(e^{-i \omega t}\)</span>是拉普拉斯算子的特征函数</p><p>把传统的傅里叶变换以及卷积迁移到Graph上来, 核心工作其实就是把<strong>拉普拉斯算子的特征函数<span class="math inline">\(e^{-i \omega t}\)</span></strong> 变为Graph对应的<strong>拉普拉斯矩阵的特征向量</strong>。 傅立叶变化是信号函数<span class="math inline">\(f(t)\)</span>与基函数<span class="math inline">\(e^{-i \omega t}\)</span>的内积 <span class="math display">\[\mathcal{F}_{T}(\omega)=\int_{-\infty}^{+\infty} f(t) e^{-i \omega t} d t\]</span> 因此对于Graph我们就可以定义 <span class="math display">\[F\left(\lambda_{l}\right)=\hat{f}\left(\lambda_{l}\right)=\sum_{i=1}^{N} f(i) u_{l}^{*}(i)\]</span> 其中<span class="math inline">\(f\)</span>是graph上的N维向量，<span class="math inline">\(f(i)\)</span>对应于graph上的第i个顶点，<span class="math inline">\(u_{l}(i)\)</span>表示第l个特征向量的第i个分量。特征值<span class="math inline">\(\lambda_l\)</span>（频率）下的<span class="math inline">\(f\)</span>的graph傅立叶变换就是与<span class="math inline">\(\lambda_l\)</span>对应的特征向量<span class="math inline">\(u_{l}\)</span>进行内积运算。将上式推广到矩阵形式，就有 <span class="math display">\[\left(\begin{array}{c}\hat{f}\left(\lambda_{1}\right) \\\hat{f}\left(\lambda_{2}\right) \\\vdots \\\hat{f}\left(\lambda_{N}\right)\end{array}\right)=\left(\begin{array}{cccc}u_{1}(1) &amp; u_{1}(2) &amp; \ldots &amp; u_{1}(N) \\u_{2}(1) &amp; u_{2}(2) &amp; \ldots &amp; u_{2}(N) \\\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\u_{N}(1) &amp; u_{N}(2) &amp; \ldots &amp; u_{N}(N)\end{array}\right)\left(\begin{array}{c}f(1) \\f(2) \\\vdots \\f(N)\end{array}\right)\]</span> <span class="math display">\[\hat{f} = U^T f\]</span></p><p>图上的傅立叶逆变换类似于传统傅立叶逆变换的对频率求积分： <span class="math display">\[f(i)=\sum_{l=1}^{N} \hat{f}\left(\lambda_{l}\right) u_{l}(i)\]</span> <span class="math display">\[f=U \hat{f}\]</span></p><h2 id="gcn">GCN</h2><p>上文从百草园讲到三味书屋，终于要讲到了本文的主角-GCN。在上面graph傅立叶变换的基础上，我们可以将卷积推广到graph上。 <span class="math display">\[f * h=\mathcal{F}^{-1}[\hat{f}(\omega) \hat{h}(\omega)]=\frac{1}{2 \Pi} \int \hat{f}(\omega) \hat{h}(\omega) e^{i \omega t} d \omega\]</span> 类比到graph上，函数<span class="math inline">\(f\)</span>与卷积核<span class="math inline">\(h\)</span>在graph上的卷积 <span class="math display">\[(f * h)_{G}=U\left(\begin{array}{lll}\hat{h}\left(\lambda_{1}\right) &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \hat{h}\left(\lambda_{n}\right)\end{array}\right) U^{T} f\]</span> 式中<span class="math inline">\(\hat{h}\left(\lambda_{l}\right)=\sum_{i=1}^{N} h(i) u_{l}^{*}(i)\)</span>是根据需要设计的卷积核<span class="math inline">\(h\)</span>在graph上的傅立叶变换。 图表示学习中用的定义为 <span class="math display">\[(f * h)_{G}=U\left(\left(U^{T} h\right) \odot\left(U^{T} f\right)\right)\]</span> <span class="math inline">\(\odot\)</span> 表示Hadamard product，对两个维度相同的向量、矩阵进行对应位置的逐元素乘积。</p><p>将神经网络与graph卷积结合，只需令卷积核变为可学习的参数，也就得到 <span class="math display">\[y_{\text {output }}=\sigma\left(U \left(\begin{array}{lll}\theta_{1} &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \theta_{n}\end{array}\right) U^{T} x\right)\]</span> 我们将卷积核记为<span class="math inline">\(g(\Lambda)\)</span> (<span class="math inline">\(\Lambda\)</span>就是大写的<span class="math inline">\(\lambda\)</span>)。<br />这种图卷积被称为<a href="https://arxiv.org/abs/1312.6203">第一代图卷积</a>，但这类图卷积计算开销非常大，且卷积核有n个参数，这种图卷积很难处理工业级的数据。 <a href="https://arxiv.org/pdf/1606.09375.pdf">第二代图卷积</a>使用了polynomial filter <span class="math display">\[g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} \Lambda^{k}=\left(\begin{array}{ccc}\sum_{j=0}^{K} \alpha_{j} \lambda_{1}^{j} &amp; &amp; \\&amp; \ddots &amp; \\&amp; &amp; \\&amp; &amp; &amp; \sum_{j=0}^{K} \alpha_{j} \lambda_{n}^{j}\end{array}\right)\]</span> 其中<span class="math inline">\(\theta \in \mathbb{R}^{K}\)</span>是多项式系数向量。进而可以推出 <span class="math display">\[U \sum_{j=0}^{K} \alpha_{j} \Lambda^{j} U^{T}=\sum_{j=0}^{K} \alpha_{j} U \Lambda^{j} U^{T}=\sum_{j=0}^{K} \alpha_{j} L^{j}\]</span> <span class="math display">\[y_{\text {output }}=\sigma\left(\sum_{j=0}^{K-1} \alpha_{j} L^{j} x\right)\]</span> 此时，我们计算图卷积运算就不需要再乘上特征向量矩阵<span class="math inline">\(U\)</span>，而是直接使用拉普拉斯矩阵<span class="math inline">\(L\)</span>的k 次方（K远小于n），这样就避免了进行特征分解。而我们可以事先计算好<span class="math inline">\(L^K\)</span> ，这样就只需要计算矩阵相乘。 此外，高阶拉普拉斯可以用切比雪夫展开来近似，因此卷积核还可以用切比雪夫多项式来表示 <span class="math display">\[g_{\theta}(\Lambda)=\sum_{k=0}^{K-1} \theta_{k} T_{k}(\tilde{\Lambda})\]</span> 而将切比雪夫多项式的阶数限制为2的时候就得到在下游任务中常用的图卷积公式： <span class="math display">\[H^{(l+1)}=\sigma\left(\widetilde{D}^{-\frac{1}{2}} \widetilde{A} \widetilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)\]</span> 上式中，<span class="math inline">\(\widetilde{A}=A+I, \mathrm{~A}\)</span>为邻接矩阵, <span class="math inline">\(I\)</span>为单位矩阵, 所以<span class="math inline">\(\widetilde{A}\)</span> 为添加自连接的邻接矩阵;<span class="math inline">\(W^{(l)}\)</span> 为神经网络第<span class="math inline">\(l\)</span>层的权重矩阵; <span class="math inline">\(\sigma(\cdot)\)</span>是激活函数</p><h2 id="gcn-self-attention">GCN &amp; Self-attention</h2><p>上面从频域分析graph convolution，当我们从空间域上分析得到的卷积公式时，可以看出它仍是一种message passing机制。 <span class="math display">\[A  H^{(l)}  W^{(l)}\]</span> 上式可以分为两个步骤，首先是<span class="math inline">\(H\)</span>与参数矩阵<span class="math inline">\(W\)</span>做一个线性映射，而后与邻节点及其边信息进行聚合汇总。这一框架在某种意义上说与self- attention是非常类似的。</p><p>self- attention包含query、key、value；其中输入的query与每个key计算相似度，而后得到一个注意力系数<span class="math inline">\(\alpha\)</span>，再由注意力系数对value进行加权求和输出最终的结果。抛开邻节点后，这两者的计算机制可以说非常类似，都可以被囊括在message passing这一框架下，而self-attention也可以理解为在完成图（所有节点都相连）上的GCN。而Transformer以及GNN在NLP中的广泛应用也从某种意义上说明了两者之间存在某种相似性。</p><h2 id="gcn应用">GCN应用</h2><h3 id="textgcn">TextGCN</h3><p><img src="/images/gcn/2.png" title="TextGCN" /> 论文<a href="https://arxiv.org/abs/1809.05679v3">Graph Convolutional Networks for Text Classification</a>所构建的TextGCN将GCN用于文本分类中，在电影评价、新闻等数据集上都取得了不错的表现。<br />如上图所示，模型将语料库中的每一篇文档和语料库中词作为节点，联合构建了一个异构图，再借助GCN进行特征传播，得到每个文档节点的embedding后进行softmax分类。<br />文章中节点都使用one-hot vector进行初始化，而文档-词边、词-词边分别用TF-IDF、PMI赋以不同的权重，而最终得到的分类准确率比传统的CNN、LSTM等网络效果要高，足以证明GCN在NLP任务中的潜力。此外，后续用BERT进行节点初始化的BERTGCN也是目前的文本分类的SOTA模型。</p><h3 id="st-gcn">ST-GCN</h3><p><img src="/images/gcn/4.png" title="ST-GCN" /> <a href="https://arxiv.org/pdf/1801.07455.pdf">ST-GCN</a>可以说是GCN在骨骼行为识别里面的开山之作。</p><h3 id="sgc">SGC</h3><p><img src="/images/gcn/3.png" title="Simplifying Graph Convolutional Networks (SGC)" /> 作者将图卷积层中的激活函数去掉，得到了SGC在许多NLP任务上更优的结果，且模型速度有了极大的提升。</p><h2 id="再回首">再回首</h2><p>回顾一下上文的内容，首先GCN是GNN的一种，从公式上看聚合函数采用的是图的拉普拉斯矩阵。当我们从频域上分析图的拉普拉斯矩阵及其特征分解之后可以发现，拉普拉斯矩阵的特征向量可以作为傅里叶变换的基、特征值表示频率，从而就可以定义图上的傅立叶变换，进而扩展到卷积操作。而将图卷积与神经网络结合后，借助多项式优化后就得到了现在常用的卷积公式。而从空间域中看，图卷积本质上也就是一种信息传播机制，借助边的权重信息对邻节点的特征做限制后传播、聚合、更新节点原本的特征。</p><p>在频域分析过程中我们可以得到，在由Graph确定的<span class="math inline">\(n\)</span>维空间中，越小的特征值 <span class="math inline">\(\lambda_{l}\)</span> 表明：拉普拉斯矩阵 <span class="math inline">\(L\)</span> 其所对应的基 <span class="math inline">\(u_{l}\)</span> 上的分量、&quot;信息&quot;越少、高频部分。所以图卷积有时候也被认知为是图上的高斯平滑，一种滤波的过程，这也导出了图卷积中的一大问题：over-smooshing。当图卷积层数加深时，图上节点自身的特征会因为不断的传播后导致自身的特征消失，所有节点的特征会越来越接近，进而使得下游任务准确率下降。</p><h3 id="gcn-vs-cnn">GCN vs CNN</h3><p>GCN可以退化为CNN...TODO</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;图卷积网络作为图神经网络之一，具有非常广泛的应用。因此网上也有非常多的关于GCN的介绍，但是各种博客看的多了搞得我脑壳嗡嗡的，而刚好最近的一个工作涉及到GCN的内核，因此借助这篇博客整理对GCN进行整理。&lt;/p&gt;
&lt;p&gt;在介绍GCN先介绍几篇文献：&lt;br /&gt;
Semi-S</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>读书人的事，能叫偷吗？ [EMNLP2021/AAAI2021]</title>
    <link href="http://example.com/2021/11/23/pd7/"/>
    <id>http://example.com/2021/11/23/pd7/</id>
    <published>2021-11-23T14:09:00.000Z</published>
    <updated>2022-01-06T01:42:20.071Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>最近似乎吃到了一个瓜</p><blockquote><p>如何看待EMNLP'21的文章涉嫌抄袭EMNLP'20上的文章?<br />本人在阅读EMNLP2021的文章时，偶然发现一篇名为Seeking Common but Distinguishing Difference, A Joint Aspect-based Sentiment Analysis Model的文章，这篇文章的内容与EMNLP2020上的一篇文章Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders高度相似。</p></blockquote><p>无独有偶，AAAI2021的论文 Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance 也被人指出涉嫌抄袭ACL2020的文章 A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</p><p>先不谈到底有没有抄袭事件，我们来看一看这几篇文章的所做的工作</p><h1 id="emnlp20">EMNLP'20</h1><p>Two are Better than One: Joint Entity and Relation Extraction with Table-Sequence Encoders</p><h2 id="intro">Intro</h2><p>本文针对实体关系联合抽取问题。现有的方法将NER和RE使用Table-Filling的方法完成联合抽取，但他们在学习过程中仅使用单个encoder来捕获两个任务的信息，而在本文提出的模型中，作者设计了表格式编码器和序列式编码器来进行表示学习。</p><h2 id="model">Model</h2><p><img src="/images/pd7/1.png" title="table filling for NER and RE" /> 对于传统的联合抽取问题，首先构建一个二维表，而后将NER问题视为序列标注为题（沿着表对角线），将RE问题视为对表中的其他条目进行标注，这个二维表能解决两个任务，并能实现两个任务间的潜在交互。<br />但是作者认为这种现有的方式会出现两个问题，一是用于同样的特征表示进行NER和RE(关系分类)两个任务，可能会对模型的学习造成误解 (Feature Confusiong)；二是基于Table-Filling方法去完成联合抽取的工作，会将表结构转化成一个序列结构，这样导致丢失了重要的结构信息。</p><p>对此，本文中作者设计了两个encoder：sequence representations and table representations, for NER and RE respectively。而且设计了一个两者之间交互机制来获取两个任务间的潜在联系。</p><h3 id="text-embedding">Text Embedding</h3><p>对于一个输入的包含n个words的句子，其词向量（LSTM）、字符向量（LSTM）和BERT词向量的共同构成了每个word的表示。 <span class="math display">\[\boldsymbol{S}_{0}=\operatorname{Linear}\left(\left[\boldsymbol{x}^{c} ; \boldsymbol{x}^{w} ; \boldsymbol{x}^{\ell}\right]\right)\]</span></p><p><img src="/images/pd7/2.png" title="table-sequence encoders" /></p><h3 id="table-encoder">Table Encoder</h3><p>Table中第i行第j列分别代表句子中的第i和第j个词，而Table encoder则要学习每个单元格的向量表示。文中使用基于GRU结构的MD-RNN(多维RNN)作为Text Encoder，在更新表格中当前cell的信息时，通过MDRNN融合其上下左右四个方向上的信息，从而利用了表格的结构特点： <img src="/images/pd7/4.png" title="GRU结构的MD-RNN" /> <span class="math display">\[T_{l, i, j}=\operatorname{GRU}\left(X_{l, i, j}, T_{l-1, i, j}, T_{l, i-1, j}, T_{l, i, j-1}\right)\]</span> 同时引入当前cell所对应的两个词在Sequence Encoder下的表示，使得Table Encoder和Sequence Encoder之间发生信息的交流；</p><p>另外，Text Embeddings部分有用到BERT, 因此将BERT中各个层上多头attention每个头上的 atention权重堆叠起来, 得到张量<span class="math inline">\(T^{l} \in \mathbb{R}^{N \times N \times\left(L^{l} \times A^{l}\right)}\)</span>。基于<span class="math inline">\(T^{l}\)</span> 和Text Embedding中每个词的表示，构成Table的初始化输入表示。</p><h3 id="sequence-encoder">Sequence Encoder</h3><p>Sequence Encoder的结构与Transformer类似，不同之处在于将Transformer中的scaled dot-product attention 替 换为了文中提出的table-guided attention。具体地, 将Transformer中计算 <span class="math inline">\(g\left(Q_{i}, K_{j}\right)\)</span> 的部分, 直接替换为对应两个word在表格中的 向量表示 <span class="math inline">\(T_{i, j}\)</span> 。由于 <span class="math inline">\(T_{i, j}\)</span> 融合了四个方向上的信息, 能够更加充分的捕捉上下文信息以及词与词之间的关系, 同时也使Table Encoder和Sequence Encoder之间产生了双向的信息交流。</p><h1 id="emnlp21">EMNLP'21</h1><p>20的文章所针对的问题是实体和关系的联合抽取，而21的文章所针对的问题是情感三元组抽取，但本质上二者所解决的问题都属于信息抽取中的关系抽取。 <img src="/images/pd7/3.png" title="论文部分截图" /> 这篇文章相当于将上面的模型换了一个任务跑，文章的配图除了配色外几乎一模一样。“幸运”的事审稿人应该没有读过20年的文章，而本文的作者也没有在论文中提及或引用，最终使得文章能成功过审。</p><h1 id="acl20">ACL'20</h1><p><strong>A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</strong></p><h2 id="intro-1">Intro</h2><p>文章针对多模态机器翻译问题 (Multi-modal Neural Machine Translation) 。为了更好的捕获细粒度的不同模态语义单元间的对应关系，论文提出了一种基于图的多模态融合编码器 graph-based multi-modal fusion encoder。具体而言，模型首先把图和句子用一个统一多模态图来表示。然后基于该图，我们堆叠基于多个图的多模态融合层，这些层迭代地表示节点之间的语义交互以进行图编码， 依次进行模态内和模态间融合以学习多模态节点表示。 最后，解码器可以通过注意机制来利用这些表示形式。</p><h2 id="multi-modal-graph">Multi-modal graph</h2><p><img src="/images/pd7/5.png" title="多模态图构建" /></p><p>上图中绿色和蓝色的节点分别代表视觉节点和文本节点，节点间的边包括跨模态的边和单模态内部的边。具体而言，首先对于节点而言，每个节点为a textual word or a visual object。每个词作为一个节点；而后利用斯坦福检测器 (Stanford parser) 识别输入句子中的所有名词短语，然后应用可视化基础工具包检测每个名词短语的边界框，得到对应的视觉节点。intra-modal edge用于连接单模态内部的节点，inter-modal edge用于将视觉节点与其对应的文本节点相连</p><h2 id="encoder-decoder">Encoder-Decoder</h2><p><img src="/images/pd7/6.png" title="模型整体框架" /> 首先借助embedding layer对节点嵌入进行初始化。<br />对于文本节点，使用其词嵌入和位置编码的和；对于视觉节点，首先从Faster-RCNN中的全连接层中提取视觉特征，而后映射到相应维度上。</p><p>在embedding层的后堆叠了多个基于图的多模态融合层进行编码。在每个融合层，模型依次进行单模态内融合和跨模态间融合以更新所有节点状态。最终节点状态对模态内和模态间信息同时进行编码。由于视觉节点和文本节点是包含不同模态信息的两种语义单元，因此我们分别应用相似的操作，但使用不同的参数来对它们的状态更新过程进行建模。</p><ul><li>Intra-modal fusion</li></ul><p>对于单模态内的，文本信息和视觉信息都用mulit-head attention来提取： <span class="math display">\[\mathbf{C}_{x}^{(l)}=\operatorname{MultiHead}\left(\mathbf{H}_{x}^{(l-1)}, \mathbf{H}_{x}^{(l-1)}, \mathbf{H}_{x}^{(l-1)}\right)\]</span> <span class="math display">\[\mathbf{C}_{o}^{(l)}=\operatorname{MultiHead}\left(\mathbf{H}_{o}^{(l-1)}, \mathbf{H}_{o}^{(l-1)}, \mathbf{H}_{o}^{(l-1)}\right)\]</span></p><ul><li>Inter-modal fusion</li></ul><p>跨模态的信息，作者设计了跨模态门控机制。对于文本跨模态信息<span class="math inline">\(M_x\)</span>，<span class="math inline">\(A\left(v_{x_{i}}\right)\)</span>是与词节点<span class="math inline">\(v_{x_{i}}\)</span>相邻的视觉节点，<span class="math inline">\(W^{(l)}\)</span>为参数矩阵。<span class="math inline">\(\alpha_{ij}\)</span>指第i个词节点对第j个视觉节点的权重，结合了节点的语义信息和视觉信息。相应的也可以得到视觉跨模态信息<span class="math inline">\(M_o\)</span>。 <span class="math display">\[\begin{aligned}M_{x_{i}}^{(l)} &amp;=\sum_{j \in A\left(v_{x_{i}}\right)} \alpha_{i, j} \odot C_{o_{j}}^{(l)} \\\alpha_{i, j} &amp;=\operatorname{Sigmoid}\left(\mathbf{W}_{1}^{(l)} C_{x_{i}}^{(l)}+\mathbf{W}_{2}^{(l)} C_{o_{j}}^{(l)}\right)\end{aligned}\]</span> <span class="math display">\[\begin{aligned}M_{o_{j}}^{(l)} &amp;=\sum_{i \in A\left(v_{o_{j}}\right)} \beta_{j, i} \odot C_{x_{i}}^{(l)} \\\beta_{j, i} &amp;=\operatorname{Sigmoid}\left(\mathbf{W}_{3}^{(l)} C_{o_{j}}^{(l)}+\mathbf{W}_{4}^{(l)} C_{x_{i}}^{(l)}\right)\end{aligned}\]</span> 而后分别经过对应的全连接前馈神经网络： <span class="math display">\[\begin{array}{l}\mathbf{H}_{x}^{(l)}=\operatorname{FFN}\left(\mathbf{M}_{x}^{(l)}\right) \\\mathbf{H}_{o}^{(l)}=\operatorname{FFN}\left(\mathbf{M}_{o}^{(l)}\right)\end{array}\]</span></p><p>Decoder的结构与常规的Transformer的decoder非常类似。由于视觉信息已通过多模式融合层合并到所有文本节点中，因此解码器仅使用文本节点状态来动态利用多模式上下文信息。输入的embedding会经过两个attention，分别是decoder内部的和encoder-decoder之间的attention。而后经过全连接层、Softmax得到输出结果。</p><h1 id="aaai21">AAAI'21</h1><p><strong>Multi-modal Graph Fusion for Named Entity Recognition with Targeted Visual Guidance</strong></p><p>论文的关于多模态命名实体识别(Multi-modal named entity recognition, MNER)，模型能发现文本中的命名实体，并利用图像将其分类为预定义的类型。图片用于辅助分类时，本文模型在图片中找到实体（来源于文本信息）所在区域，截出对应的部分进行视觉信息抽取。</p><p><img src="/images/pd7/7.png" title="模型整体架构" /> 先来看一下这篇论文的主图，从图上可以看出模型的整个框架与2020年的论文非常类似。其中图节点嵌入的初始化分别用BERT和ResNet来代替；而后的encoder-decoder与20年的模型基本一致。</p><p><img src="/images/pd7/8.png" title="Graph-based Multi-modal Fusion" /> 这一部分从论文中也可以看出，多模态图中跨模态间和单模态内的处理与20年论文的处理方法一致。</p><h1 id="p.s.">p.s.</h1><p>最近尝试复现旷视的<a href="/2021/10/26/pd5/" title="ML-GCN">ML-GCN</a>也遇到了大无语事件，其一是用了好几个版本的Pytorch都无法较好的收敛，看网友的一些讨论说居然要用19年特定的0.3.1版本；其二就是ML-GCN中最主要的模块GCN对模型的准确率没有任何贡献，去掉该模块反而有更好的效果。😅<br />怪不得会有人用“魔改”这种略带侮辱性的词来形容AI的科研。希望能有越来越多的人做有意义的科研，也希望“独角兽”们和“master”们把挣钱和科研分开。“挣钱嘛，不寒碜“ （让子弹飞赶紧申遗），但既然想挣钱就别往科研里钻，整些无意义灌水甚至是剽窃他人的成果。<br />当然，抛开环境现状不谈来批判某些个体的行为似乎有些耍流氓。还记得大二时候导师跟我和zb说要做一个一身正气的科研工作者。（虽然现在也说不准以后是进学术界还是工业界）国家快速发展总会留下一些弊病，希望能从高校老师和学生开始，逐渐构建一片净土，让热爱学术的人能够坚定不移的走下去。至少以后学术造假等事件曝光时，我们看到的主角不应该是中国人的名字。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;最近似乎吃到了一个瓜&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如何看待EMNLP&#39;21的文章涉嫌抄袭EMNLP&#39;20上的文章?&lt;br /&gt;
本人在阅读EMNLP2021的文章时，偶然发现一篇名为Seeking Common but D</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="NER" scheme="http://example.com/tags/NER/"/>
    
  </entry>
  
  <entry>
    <title>SVGD补充</title>
    <link href="http://example.com/2021/11/18/svgd-2/"/>
    <id>http://example.com/2021/11/18/svgd-2/</id>
    <published>2021-11-18T02:03:58.000Z</published>
    <updated>2021-11-18T05:41:21.352Z</updated>
    
    <content type="html"><![CDATA[<p>关于<a href="/2021/11/09/svgd/" title="SVGD">SVGD</a>的内容，这一篇博客中大致做了简单的介绍，包括从上到下的数学推导和简单的逻辑链。但整个贝叶斯推断背后还有许多思想，且SVGD背后也隐藏的一些思维过程。这篇Blog将作为SVGD相关内容的补充以及涉及到的知识体系的简要归纳。</p><h1 id="svgd实验">SVGD实验</h1><h2 id="拟合高斯分布">拟合高斯分布</h2><h2 id="贝叶斯神经网络">贝叶斯神经网络</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;关于&lt;a href=&quot;/2021/11/09/svgd/&quot; title=&quot;SVGD&quot;&gt;SVGD&lt;/a&gt;的内容，这一篇博客中大致做了简单的介绍，包括从上到下的数学推导和简单的逻辑链。但整个贝叶斯推断背后还有许多思想，且SVGD背后也隐藏的一些思维过程。这篇Blog将作为SVG</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="SVGD" scheme="http://example.com/tags/SVGD/"/>
    
  </entry>
  
  <entry>
    <title>Kernels and Hilbert Space</title>
    <link href="http://example.com/2021/11/16/kernel/"/>
    <id>http://example.com/2021/11/16/kernel/</id>
    <published>2021-11-16T11:43:16.000Z</published>
    <updated>2021-11-17T12:21:34.962Z</updated>
    
    <content type="html"><![CDATA[<p>Reference：<br />‘Introduction to Hilbert Spaces with Application.’<br />‘Introduction to RKHS, and some simple kernel algorithms.’</p><p>Since Kernel trick is one of the core methods in SVM and SVGD also involves expertise related to RKHS. I looked up several books on Kernel method, trying to get a systematic understanding of Kernel and Hilbert space. This blog can also be regarded as a summary and summary of the book ‘Introduction to Hilbert Spaces with Application ’.</p><h1 id="introduction">Introduction</h1><p><img src="/images/kernel/1.png" title="XOR example" /> <img src="/images/kernel/2.png" title="Document classification example" /></p><h2 id="kernel">Kernel</h2><p>Definition: Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}\]</span></p><h1 id="normed-vector-spaces">Normed Vector Spaces</h1><p>First, the space defined in mathematics can be divided from simple to complex as:</p><ul><li><p>Vector Space<br />a nonempty set <span class="math inline">\(E\)</span> with two operations: <em>addition</em> and <em>multiplication by scalars</em>.<br />e.g. <span class="math inline">\(\mathbb{R}^{N}\)</span> <span class="math inline">\(\mathbb{C}^{N}\)</span></p></li><li><p>Normed Space<br />norm is an abstract generalization of the length of a vector:<br />function <span class="math inline">\(x \mapsto\|x\|\)</span> from a vector space <span class="math inline">\(E\)</span> into <span class="math inline">\(\mathbb{R}\)</span></p></li><li><p>Banach Space: complete normed space<br />A normed space is complete if and only if every absolutely convergent series converges. (The contents of Cauchy sequence and Cauchy series are put in the appendix)<br />Actually, Banach space introduces the concept of Limits</p></li><li><p>Inner Product Spaces<br />The space that defines the <a href="#jump">inner product</a>.</p></li><li><p>Hilbert Spaces: A complete inner product space</p></li></ul><h1 id="hilbert-spaces">Hilbert Spaces</h1><h1 id="appendix">Appendix</h1><h2 id="cauchy-sequence-and-cauchy-series">Cauchy sequence and Cauchy series</h2><p>Definition of <strong><em>Cauchy sequence</em></strong>. A sequence <span class="math inline">\(\left\{f_{n}\right\}_{n=1}^{\infty}\)</span> of elements in a normed space <span class="math inline">\(\mathcal{H}\)</span> is said to be a Cauchy sequence if for every <span class="math inline">\(\epsilon&gt;0\)</span>, there exists <span class="math inline">\(N=N(\varepsilon) \in \mathbb{N}\)</span>, such that for all <span class="math inline">\(n, m \geq N,\left\|f_{n}-f_{m}\right\|_{\mathcal{H}}&lt;\epsilon\)</span></p><h2 id="inner-product">Inner product</h2><p><span id="jump"> </span> Definition of <strong><em>Inner product</em></strong>. Let <span class="math inline">\(\mathcal{H}\)</span> be a vector space over <span class="math inline">\(\mathbb{R}\)</span>. A function <span class="math inline">\(\langle\cdot, \cdot\rangle_{\mathcal{H}}: \mathcal{H} \times \mathcal{H} \rightarrow \mathbb{R}\)</span> is said to be an inner product on <span class="math inline">\(\mathcal{H}\)</span> if:</p><ul><li><p><span class="math inline">\(\left\langle\alpha_{1} f_{1}+\alpha_{2} f_{2}, g\right\rangle_{\mathcal{H}}=\alpha_{1}\left\langle f_{1}, g\right\rangle_{\mathcal{H}}+\alpha_{2}\left\langle f_{2}, g\right\rangle_{\mathcal{H}}\)</span></p></li><li><p><span class="math inline">\(\langle f, g\rangle_{\mathcal{H}}=\langle g, f\rangle_{\mathcal{H}}{ }^{1}\)</span></p></li><li><p><span class="math inline">\(\langle f, f\rangle_{\mathcal{H}} \geq 0\)</span> and <span class="math inline">\(\langle f, f\rangle_{\mathcal{H}}=0\)</span> if and only if <span class="math inline">\(f=0\)</span>.</p></li></ul><p>the inner product between matrices <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(B \in\)</span> <span class="math inline">\(\mathbb{R}^{m \times n}\)</span> is <span class="math display">\[\langle A, B\rangle=\operatorname{trace}\left(A^{\top} B\right)\]</span></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Reference：&lt;br /&gt;
‘Introduction to Hilbert Spaces with Application.’&lt;br /&gt;
‘Introduction to RKHS, and some simple kernel algorithms.’&lt;/p&gt;
</summary>
      
    
    
    
    <category term="Preliminary AI" scheme="http://example.com/categories/Preliminary-AI/"/>
    
    
    <category term="Kernels" scheme="http://example.com/tags/Kernels/"/>
    
  </entry>
  
  <entry>
    <title>Stein variational gradient descent (NIPS2018)</title>
    <link href="http://example.com/2021/11/09/svgd/"/>
    <id>http://example.com/2021/11/09/svgd/</id>
    <published>2021-11-09T12:28:58.000Z</published>
    <updated>2021-11-18T05:48:02.952Z</updated>
    
    <content type="html"><![CDATA[<h1 id="intro">Intro</h1><p>这一工作是清华大学liu qiang老师提出的，相关论文从2016年开始也一直在更新，分别发表在NIPS、ICLR等顶会上。<br /><a href="https://arxiv.org/abs/1704.07520">Stein Variational Gradient Descent as Gradient Flow</a><br /><a href="https://arxiv.org/abs/1810.11693">Stein Variational Gradient Descent as Moment Matching</a><br /><a href="https://arxiv.org/pdf/1608.04471.pdf">Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm</a></p><p>从定义上来说，SVGD是一种确定性的采样算法，用一组粒子来近似给定的分布。基于这两点，它和MCMC以及VI都有共通之处。但SVGD即保证了在大量数据下的计算速度，也比变分推断具有更高的准确性。</p><p>从整体上来看，这一工作通过引入Stein discrepancy来度量两个分布之间的距离，再借助RKHS使其容易计算，最后借助gradient descent进行优化。因此下文也就从这三部分一一介绍。</p><h1 id="background">Background</h1><h2 id="steins-method">Stein's method</h2><p>首先我们需要引入几个定义：</p><ul><li><p><em>Stein score function</em> <span class="math display">\[\boldsymbol{s}_{p}=\nabla_{x} \log p(x)=\frac{\nabla_{x} p(x)}{p(x)}\]</span> 这一函数被称为<span class="math inline">\(q(x)\)</span>的Stein score function</p></li><li><p><em>Stein class</em><br />当函数<span class="math inline">\(f: \mathcal{X} \rightarrow \mathbb{R}\)</span>满足下式时则称其在stein class中 <span class="math display">\[\int_{x \in \mathcal{X}} \nabla_{x}(f(x) p(x)) d x=0\]</span> 其中<span class="math inline">\(\mathcal{X}\)</span> 是<span class="math inline">\(\mathbb{R}^{d}\)</span>下的子集，而<span class="math inline">\(p(x)\)</span>则是在<span class="math inline">\(\mathcal{X}\)</span> 下连续可微的分布。</p></li><li><p><em>Stein's operator</em>：作用在<span class="math inline">\(p\)</span>上的线性操作 <span class="math display">\[\mathcal{A}_{p} f(x)=\boldsymbol{s}_{p}(x) f(x)+\nabla_{x} f(x)\]</span> 其中<span class="math inline">\(s_{p}\)</span>和<span class="math inline">\(\mathcal{A}_{p} f\)</span> 都是<span class="math inline">\(d \times 1\)</span> 函数(mapping from <span class="math inline">\(\mathcal{X}\)</span> to <span class="math inline">\(\mathbb{R}^{d}.\)</span>)</p></li></ul><p>有了以上三个定义后，我们可以尝试得到stein discrepancy。首先作为一个度量手段，必然需要满足一些条件。<br />当且仅当 <span class="math display">\[\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]=0   \qquad   (1) \]</span> <span class="math inline">\(p(x)\)</span> 和 <span class="math inline">\(q(x)\)</span>是相等的。而当两个分布<span class="math inline">\(p=q\)</span>时又被称为stein identity。<br />借助 (1) 式，我们可以定义Stein discrepancy来度量两个分布<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>之间的差异： <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\boldsymbol{s}_{q}(x) f(x)+\nabla_{x} f(x)\right]\right)^{2}\]</span> 借助之前定义的stein operator，也可以把上式写为 <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\mathcal{A}_{q} f(x)\right]\right)^{2}\]</span></p><p><span class="math inline">\(\mathcal{F}\)</span>是一系列连续可微的且满足<span class="math inline">\(\mathbb{S}(p, q)\)</span>不为0(<span class="math inline">\(p \neq q\)</span>时)函数集合。当<span class="math inline">\(p \neq q\)</span>时，<span class="math inline">\(\mathbb{S}(p,q)&gt;0\)</span>，而<span class="math inline">\(max\)</span>则是因为我们希望距离尽可能明显。</p><p><span class="math inline">\(\mathbb{S}(p, q)\)</span>并没有被广泛应用在机器学习中，因为其计算和优化的复杂性: <span class="math inline">\(q(x)=f(x) / Z\)</span> 而<span class="math inline">\(Z=\int f(x) d x\)</span>的计算往往设计高维积分。<br />但是论文提出了将函数<span class="math inline">\(\mathcal{F}\)</span>用核函数代替时，会得到易于计算的Stein discrepancy <span class="math inline">\(\mathbb{S}(p, q)\)</span>。具体而言，我们令<span class="math inline">\(\mathcal{F}\)</span>来源于希尔伯特再生核空间的一个球 (reproducing kernel Hilbert space (RKHS))。</p><h2 id="kernelized-stein-discrepancy">Kernelized Stein Discrepancy</h2><p>对于映射后的函数，对应的正定核<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>，我们有 <span class="math display">\[\mathbb{S}(p, q)=\mathbb{E}_{x, x^{\prime} \sim p}\left[u_{q}\left(x, x^{\prime}\right)\right]\]</span> 其中<span class="math inline">\(x, x^{\prime}\)</span>是<span class="math inline">\(p\)</span>中独立同分布的两个变量，函数<span class="math inline">\(u_{q}\left(x, x^{\prime}\right)\)</span>由<span class="math inline">\(q\)</span>确定，如果展开的话实际上是： <span class="math display">\[u_{q}\left(x, x^{\prime}\right)= \boldsymbol{s}_{q}(x)^{\top} k\left(x, x^{\prime}\right) \boldsymbol{s}_{q}\left(x^{\prime}\right)+\boldsymbol{s}_{q}(x)^{\top} \nabla_{x^{\prime}} k\left(x, x^{\prime}\right)+\nabla_{x} k\left(x, x^{\prime}\right)^{\top} \boldsymbol{s}_{q}\left(x^{\prime}\right)+\operatorname{trace}\left(\nabla_{x, x^{\prime}} k\left(x, x^{\prime}\right)\right)\]</span></p><p>当我们从未知分布<span class="math inline">\(p(x)\)</span>采样出一个样本<span class="math inline">\({x_i}\)</span>时，我们可以进行近似计算 <span class="math display">\[\hat{\mathbb{S}}(p, q)=\frac{1}{n(n-1)} \sum_{i \neq j} u_{q}\left(x_{i}, x_{j}\right)\]</span></p><p>接下来我们详细介绍上述的过程。</p><h3 id="kernels-and-reproducing-kernel-hilbert-spaces">Kernels and Reproducing Kernel Hilbert Spaces</h3><a href="/2021/11/16/kernel/" title="Kernel and Hilbert Spaces介绍 (未完待续)">Kernel and Hilbert Spaces介绍 (未完待续)</a><p>令<span class="math inline">\(k\left(x, x^{\prime}\right)\)</span>为一个正定核，根据Mercer’s theorem我们对其进行谱分解： <span class="math display">\[k\left(x, x^{\prime}\right)=\sum_{j} \lambda_{j} e_{j}(x) e_{j}\left(x^{\prime}\right)\]</span> 其中<span class="math inline">\(\left\{e_{j}\right\},\left\{\lambda_{j}\right\}\)</span>分别是正交特征函数和正特征值，满足<span class="math inline">\(\int e_{i}(x) e_{j}(x) d x=\mathbb{I}[i=j]\)</span>, for <span class="math inline">\(\forall i, j\)</span></p><p>对于一个正定核，它可以分解为RKHS中特征函数的线性组合（空间中任何一个函数可以用这组基的线性组合来表示）。由一个特定的核函数能产生一个唯一的Hilbert空间，有性质 <span class="math display">\[f(x)=\langle f, k(\cdot, x)\rangle_{\mathcal{H}}, \quad k\left(x, x^{\prime}\right)=\left\langle k(\cdot, x), k\left(\cdot, x^{\prime}\right)\right\rangle_{\mathcal{H}}\]</span> 当我们定义 <span class="math inline">\(\mathcal{H}^{d}=\mathcal{H} \times \mathcal{H} \times \cdots \mathcal{H}\)</span> 为 <span class="math inline">\(d\)</span> 维向量函数 <span class="math inline">\(\mathbf{f}=\left\{f_{i}: f_{i} \in \mathcal{H} \quad i=1, \cdots, d\right\}\)</span> 组成的 Hilbert空间, <span class="math inline">\(\mathcal{H}^{d}\)</span> 上的内积定义为 <span class="math inline">\(&lt;\mathbf{f}, \mathbf{g}&gt;_{\mathcal{H}^{d}}=\sum_{i=1}^{d}&lt;f_{i}, g_{i}&gt;_{\mathcal{H}}\)</span> 。如果觉得上述的介绍太过抽象，可以看附录部分关于RKHS的一个<a href="#jump">toy example</a></p><h3 id="lemmas">lemmas</h3><p><strong>Stein's Identity</strong> ： <span class="math display">\[\mathbb{E}_{p}\left[\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)\right]=0\]</span> 证明的话根据<span class="math inline">\(\boldsymbol{s}_{p}(x) \boldsymbol{f}(x)^{\top}+\nabla \boldsymbol{f}(x)=\nabla_{x}(\boldsymbol{f}(x) p(x)) / p(x)\)</span>和分布积分法则就可以推导出来。</p><p>有了上面的引理，我们可以得到 <span class="math display">\[\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)-\mathcal{A}_{p} \boldsymbol{f}(x)\right]=\mathbb{E}_{p}\left[\left(\boldsymbol{s}_{q}(x)-\boldsymbol{s}_{p}(x)\right) \boldsymbol{f}(x)^{\top}\right]\]</span></p><p>也就是说<span class="math inline">\(\mathbb{E}_{p}\left[\mathcal{A}_{q} \boldsymbol{f}(x)\right]\)</span>是一个由<span class="math inline">\(f(x)\)</span>加权的期望，对于<span class="math inline">\(\left(s_{q}(x)-s_{p}(x)\right)\)</span>的期望。</p><h3 id="ksd">KSD</h3><p>借助上面的推导以及RKHS的性质，我们可以定义出核空间下的stein discrepancy： <span class="math display">\[S(p, q)=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right)\right]\]</span> （这里我们将Kernelized前后的stein discrepancy分别写为<span class="math inline">\(\mathbb{S}(p,q)\)</span>和<span class="math inline">\(S(p, q)\)</span>。另外需要注意后续部分推导是针对stein discrepancy中的期望项）<br />我们要用一个可采样的分布<span class="math inline">\(q\)</span>拟合分布<span class="math inline">\(p\)</span>，因此我们希望<span class="math inline">\(S(p, q)\)</span>[]中的式子是与<span class="math inline">\(p\)</span>无关的。把式子展开后可以发现 <span class="math display">\[\begin{aligned}S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}-s_{p}\right)\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T}\left(k(\mathbf{x}, \mathbf{y}) s_{q}+\nabla_{y} k(\mathbf{x}, \mathbf{y})-k(\mathbf{x}, \mathbf{y}) s_{p}-\nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})\right)\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim}\left[\left(s_{q}-s_{p}\right)^{T} v(\mathbf{x}, \mathbf{y})\right]\end{aligned}\]</span> 其中<span class="math inline">\(v(\mathbf{x}, \mathbf{y})=k(\mathbf{x}, \mathbf{y}) s_{q}(\mathbf{y})+\nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})=\mathcal{A}_{q} k_{\mathbf{x}}(\mathbf{y}), k_{\mathbf{x}}(\cdot)=k(\mathbf{x}, \cdot)\)</span></p><p>而对于固定的 <span class="math inline">\(\mathbf{y}\)</span>, 容易证明 <span class="math inline">\(v(\cdot, \mathbf{y})\)</span> 是Stein class of <span class="math inline">\(p(\mathbf{x})\)</span>, 即满足 <span class="math inline">\(\int_{\mathbf{x} \in \mathcal{X}} \nabla_{\mathbf{x}}(v(\mathbf{x}, \mathbf{y}) p(\mathbf{x})) d \mathbf{x}=0\)</span> 。因此就可以进一步将上式写开为 <span class="math display">\[\begin{aligned}S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})-\left(\nabla_{\mathbf{x}} \ln p(\mathbf{x})\right)^{T} v(\mathbf{x}, \mathbf{y})\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})\right]-\int d \mathbf{x} d \mathbf{y} p(\mathbf{x}) p(\mathbf{y})\left(\nabla_{\mathbf{x}} \ln p(\mathbf{x})\right)^{T} v(\mathbf{x}, \mathbf{y}) \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[s_{q}^{T} v(\mathbf{x}, \mathbf{y})\right]+\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\operatorname{tr} \nabla_{\mathbf{x}} v(\mathbf{x}, \mathbf{y})\right]\end{aligned}\]</span></p><p>其中<span class="math inline">\(\nabla_{\mathbf{x}} v(\mathbf{x}, \mathbf{y})=\nabla_{\mathbf{x}} k(\mathbf{x}, \mathbf{y}) s_{q}(\mathbf{y})^{T}+\nabla_{\mathbf{x}} \nabla_{\mathbf{y}} k(\mathbf{x}, \mathbf{y})\)</span>为一个矩阵。</p><p>这样就得到了前文提到了 <span class="math display">\[S(p, q)=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[u_{q}(\mathbf{x}, \mathbf{y})\right]\]</span> 其中<span class="math inline">\(u_{q}(\mathbf{x}, \mathbf{y})\)</span>仅与分布<span class="math inline">\(q\)</span>有关。</p><p>上述的推导说明了什么？说明引入核方法的可行性。而另一方面，我们要用核方法来加速内积的计算，如何体现？<br />借助RKHS的对称性和再生性，我们可以将<span class="math inline">\(S(p,q)\)</span>进行变化： <span class="math display">\[\begin{aligned}S(p, q) &amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)^{T} k(\mathbf{x}, \mathbf{y})\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)\right] \\&amp;=\mathbb{E}_{\mathbf{x}, \mathbf{y} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)^{T}&lt;k(\mathbf{x}, \cdot), k(\cdot, \mathbf{y})&gt;_{\mathcal{H}}\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{y})\right)\right] \\&amp;=\sum_{i=1}^{d}&lt;\mathbb{E}_{\mathbf{x} \sim p}\left[\left(s_{q}^{i}(\mathbf{x})-s_{p}^{i}(\mathbf{x})\right) k(\mathbf{x}, \cdot)\right], \mathbb{E}_{\mathbf{y} \sim p}\left[\left(s_{q}^{i}(\mathbf{y})-s_{p}^{i}(\mathbf{y})\right) k(\cdot \mathbf{y})\right]&gt;_{\mathcal{H}} \\&amp;=\sum_{i=1}^{d}&lt;\beta_{i}, \beta_{i}&gt;_{\mathcal{H}} \\&amp;=\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}^{2}\end{aligned}\]</span> 其中<span class="math inline">\(\boldsymbol{\beta}(\mathbf{y})\)</span>是一个向量函数 <span class="math display">\[\boldsymbol{\beta}(\mathbf{y})=\mathbb{E}_{\mathbf{x} \sim p}\left[\mathcal{A}_{q} k_{\mathbf{y}}(\mathbf{x})\right]=\mathbb{E}_{\mathbf{x} \sim p}\left[\left(s_{q}(\mathbf{x})-s_{p}(\mathbf{x})\right) k_{\mathbf{y}}(\mathbf{x})\right]\]</span></p><p>在这里再展示一下最开始的stein discrepancy <span class="math display">\[\mathbb{S}(p, q)=\max _{f \in \mathcal{F}}\left(\mathbb{E}_{p}\left[\mathcal{A}_{q} f(x)\right]\right)^{2}\]</span> 对于任意向量 <span class="math inline">\(\mathbf{f} \in \mathcal{H}^{d}\)</span> 与 <span class="math inline">\(\boldsymbol{\beta}\)</span> 的内积为 <span class="math display">\[\begin{aligned}&lt;\mathbf{f}, \boldsymbol{\beta}&gt;_{\mathcal{H}^{d}} &amp;=\sum_{i=1}^{d}&lt;f_{i}, \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x}) k(\mathbf{x}, \cdot)+\nabla_{x_{i}} k(\mathbf{x}, \cdot)\right]&gt;_{\mathcal{H}} \\&amp;=\sum_{i=1}^{d} \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x})&lt;f_{i}, k(\mathbf{x}, \cdot)&gt;_{\mathcal{H}}+&lt;f_{i}, \nabla_{x_{i}} k(\mathbf{x}, \cdot)&gt;_{\mathcal{H}}\right] \\&amp;=\sum_{i=1}^{d} \mathbb{E}_{\mathbf{x} \sim p}\left[s_{q}^{i}(\mathbf{x}) f_{i}(\mathbf{x})+\nabla_{x_{i}} f_{i}(\mathbf{x})\right] \\&amp;=\mathbb{E}_{\mathbf{x} \sim p}\left[\operatorname{tr}\left(\mathcal{A}_{q} \mathbf{f}(\mathbf{x})\right)\right] \leq\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}} (因为任意两个向量内积小于它与自身的内积)\end{aligned}\]</span> 因此我们有 <span class="math display">\[\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}=S(p, q)=\max _{f \in \mathcal{H}^{d}}\left\{\mathbb{E}_{\mathbf{x} \sim p}\left[\operatorname{tr}\left(\mathcal{A}_{q} f(\mathbf{x})\right)\right]\right \}.\]</span> 上式中<span class="math inline">\(\|f\|_{\mathcal{H}^{d}} \leq 1\)</span>，对应于最初提到的映射到希尔伯特空间中的一个球中的最优向量。这个<span class="math inline">\(S(p,q)\)</span>最大值所对应的向量为 <span class="math inline">\({f}^{*}=\boldsymbol{\beta} /\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}\)</span></p><p>简单来说，引入kernel之后，我们可以直接得到stein discrepancy定义式中的函数。也就是说，KSD非常容易就能求得。</p><h1 id="stein-variational-gradient-descent-svgd">Stein Variational Gradient Descent (SVGD)</h1><p>SVGD的另一个核心公式在于 <span class="math display">\[\left.\nabla_{\epsilon} \mathrm{KL}\left(q_{[T]} \| p\right)\right|_{\epsilon=0}=-\mathbb{E}_{x \sim q}\left[\operatorname{tr}\left(\mathcal{A}_{p} \phi(x)\right)\right]\]</span> 也就是说KL散度变分求导等于KSD（具体推导过程见附录），这意味着KL散度变化最快的方向就是KSD所对应的向量函数 <span class="math inline">\(\phi^{*}=\boldsymbol{\beta} /\|\boldsymbol{\beta}\|_{\mathcal{H}^{d}}\)</span> <img src="/images/vimcmc/2.png" title="算法伪代码" /> 具体而言粒子<span class="math inline">\(\left\{x_{i}^{l}\right\}_{i=1}^{n}\)</span> 表示第<span class="math inline">\(l\)</span>次达代的第<span class="math inline">\(i\)</span>个粒子, 一共<span class="math inline">\(n\)</span> 个。粒子最开始是从分布<span class="math inline">\(q_{0}\)</span>中采样的, 最初的分布<span class="math inline">\(q\)</span>可以是任意 的。也就是说，该算法不依赖于初始的分布。</p><p>算法中的更新项包含了两个部分 <span class="math display">\[k\left(x_{j}^{\ell}, x\right) \nabla_{x_{j}^{\ell}} \log p\left(x_{j}^{\ell}\right)+\nabla_{x_{j}^{\ell}} k\left(x_{j}^{\ell}, x\right)\]</span> 其中第一项意味着粒子会朝<span class="math inline">\(p\)</span>分布概率高的地方移动，而第二项代表着粒子将会朝着远离当前迭代轮数$l ll的粒子，从而减轻局部最优的风险。</p><p><img src="/images/vimcmc/2.gif" title="SVGD拟合一维分布" /></p><h1 id="回顾与总结">回顾与总结</h1><p>从上文繁杂的推导中，SVGD算法确保粒子的移动是朝着KL散度的减小最快方向，而这个方向可以有核化的stein discrepancy导出 我们回看一下KL divergence的定义式： <span class="math display">\[\mathrm{KL}(P \| Q)=\int P(x) \log \frac{P(x)}{Q(x)} d x\]</span></p><p>对于目标分布<span class="math inline">\(p(x)\)</span>，变分推断 (VI)目标是从一类分布族<span class="math inline">\(\mathcal{Q}\)</span>中找到最优的<span class="math inline">\(q(x)\)</span> <span class="math display">\[q^{*}=\underset{q \in \mathcal{Q}}{\arg \min }\left\{K L(q \| p)=\mathbb{E}_{q}[\log q(x)]-\mathbb{E}_{q}[\log \bar{p}(x)]+\log Z\right\}\]</span> 而SVGD在再生核希尔伯特空间下给出了使得KL散度下降最快的确定性方向，类似经典的梯度下降算法，可以理解为迭代构建增量变化的方法。</p><h1 id="appendix">Appendix</h1><p><span id="jump"> </span></p><h2 id="the-reproducing-kernel-hilbert-space">The reproducing kernel Hilbert space</h2><p>先回顾一下kernel的定义： Let <span class="math inline">\(\mathcal{X}\)</span> be a non-empty set. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\)</span> is called a kernel if there exists an <span class="math inline">\(\mathbb{R}\)</span>-Hilbert space and a map <span class="math inline">\(\phi: \mathcal{X} \rightarrow \mathcal{H}\)</span> such that <span class="math inline">\(\forall x, x^{\prime} \in \mathcal{X}\)</span> <span class="math display">\[k\left(x, x^{\prime}\right):=\left\langle\phi(x), \phi\left(x^{\prime}\right)\right\rangle_{\mathcal{H}}\]</span></p><p>在此基础上，我们用一个异或问题的例子来介绍RKHS。考虑特征映射</p><p><span class="math display">\[\phi: \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}\]</span> <span class="math display">\[x=\left[\begin{array}{l}x_{1} \\x_{2}\end{array}\right] \quad \mapsto \quad \phi(x)=\left[\begin{array}{c}x_{1} \\x_{2} \\x_{1} x_{2}\end{array}\right]\]</span> <img src="/images/vimcmc/4.png" title="特征空间和特征映射。希尔伯特空间的元素一般是函数，而函数可以被视为无穷维的向量。因此事实上希尔伯特空间的基底是一组无限维的函数，可以参考傅立叶变化或泰勒展开" /></p><p>kernel <span class="math display">\[k(x, y)=\left[\begin{array}{c}x_{1} \\x_{2} \\x_{1} x_{2}\end{array}\right]^{\top}\left[\begin{array}{c}y_{1} \\y_{2} \\y_{1} y_{2}\end{array}\right]\]</span></p><p>接下来我们可以定义一个特征函数： <span class="math display">\[f(x)=a x_{1}+b x_{2}+c x_{1} x_{2}\]</span> 这个函数属于从<span class="math inline">\(\mathcal{X}=\mathbb{R}^{2}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>的函数空间。此时，我们也可以把函数<span class="math inline">\(f\)</span>等价表示为： <span class="math display">\[f(\cdot)=\left[\begin{array}{l}a \\b \\c\end{array}\right]\]</span> 至此，我们可以把<span class="math inline">\(f(x)\)</span>写为： <span class="math display">\[\begin{aligned}f(x) &amp;=f(\cdot)^{\top} \phi(x) \\&amp;:=\langle f(\cdot), \phi(x)\rangle_{\mathcal{H}}\end{aligned}\]</span> 也就是说，特征函数<span class="math inline">\(f\)</span>在<span class="math inline">\(x\)</span>的值可以被写为特征空间中的内积。<span class="math inline">\(\mathcal{H}\)</span>是一个将<span class="math inline">\(\mathbb{R}^{2}\)</span>映射到<span class="math inline">\(\mathbb{R}\)</span>的函数空间。上面这些乱七八糟的怎么体现再生性呢？我们仔细看下面的等式 <span class="math display">\[k(\cdot, y)=\left[\begin{array}{c}y_{1} \\y_{2} \\y_{1} y_{2}\end{array}\right]=\phi(y)\]</span> 上式我们参考<span class="math inline">\(f(\cdot)\)</span>类似的定义。具体来说，如果我们令<span class="math inline">\(a=y_{1}, b=y_{2}\)</span>, and <span class="math inline">\(c=y_{1} y_{2}\)</span>，就有 <span class="math display">\[\langle k(\cdot, y), \phi(x)\rangle_{\mathcal{H}}=a x_{1}+b x_{2}+c x_{1} x_{2}\]</span></p><p>总的来说RKHS两个特性：<br />每个点的特征映射在特征空间中 <span class="math display">\[\forall x \in \mathcal{X}, \quad k(\cdot, x) \in \mathcal{H}\]</span> 再生性：<br /><span class="math display">\[\forall x \in \mathcal{X}, \forall f \in \mathcal{H},\langle f, k(\cdot, x)\rangle_{\mathcal{H}}=f(x)\]</span> <span class="math display">\[k(x, y)=\langle k(\cdot, x), k(\cdot, y)\rangle_{\mathcal{H}}\]</span></p><h2 id="kl散度一阶导与ksd">KL散度一阶导与KSD</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;intro&quot;&gt;Intro&lt;/h1&gt;
&lt;p&gt;这一工作是清华大学liu qiang老师提出的，相关论文从2016年开始也一直在更新，分别发表在NIPS、ICLR等顶会上。&lt;br /&gt;
&lt;a href=&quot;https://arxiv.org/abs/1704.07520&quot;</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="SVGD" scheme="http://example.com/tags/SVGD/"/>
    
  </entry>
  
  <entry>
    <title>Multi-Label Image Recognition with Graph Convolutional Networks [CVPR2019]</title>
    <link href="http://example.com/2021/10/26/pd5/"/>
    <id>http://example.com/2021/10/26/pd5/</id>
    <published>2021-10-26T15:25:47.000Z</published>
    <updated>2021-10-27T16:57:37.967Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1904.03582">Multi-Label Image Recognition with Graph Convolutional Networks</a><br />Code: <a href="https://github.com/chenzhaomin123/ML_GCN">link</a></p><p>针对多标签图像识别 (multi-label image recognition) 问题，旷视研究院提出一种基于图卷积网络的模型取得了良好的表现，该模型包含一个CNN的图像特征提取模块和一个图卷积网络进行标签间关系提取模块。</p><h2 id="intro">Intro</h2><p>对于多标签图像的识别问题，传统的方法往往是对每个标签进行孤立的二分类，即预测每个物体是否出现。基于概率图模型或RNN模型的方法则考虑显式的建模标签之间的依赖关系。也有方法将图像区域划分后考虑区域间的局部相关性，从而隐式的建模标签相关性。本文提出的基于GCN的端到端模型将标签的表示映射到相互独立的对象分类器上。</p><h2 id="related-work">Related Work</h2><p>最简单的多标签识别方法就是为每个标签独立训练一个二分类器，这种模型没有考虑标签之间的关系。当数据集中可能的标签数量增长时，可能的标签组合就会指数级增长（当一个数据集包含20个标签，则标签组合就有<span class="math inline">\(2^{20}\)</span>种。基于RNN、LSTM之类的模型将标签嵌入为向量，从而发掘标签间的相关性。<br />本文提出的模型将多标签构建为有向图，借助GCN在标签间的信息传播来学习图像标签间依赖、共现关系，并实现端到端训练。</p><h2 id="framework">Framework</h2><p><img src="/images/pd5/2.png" title="模型框架" /></p><h3 id="图像特征提取">图像特征提取</h3><p>论文用CNN进行图像特征提取，具体为ResNet-101的网络结构，输入图像<span class="math inline">\(I\)</span>，经过cnn和global max-pooling后得到2048维图像特征。 <span class="math display">\[\boldsymbol{x}=f_{\mathrm{GMP}}\left(f_{\mathrm{cnn}}\left(\boldsymbol{I} ; \theta_{\mathrm{cnn}}\right)\right) \in \mathbb{R}^{D}\]</span></p><h3 id="图卷积">图卷积</h3><p>卷积模块与最基本的卷积相同，如下式 <span class="math display">\[\boldsymbol{H}^{l+1}=h\left(\widehat{\boldsymbol{A}} \boldsymbol{H}^{l} \boldsymbol{W}^{l}\right)\]</span> 我们主要关注如何构图，在这一方面，本文的idea似乎有些超脱CV领域。模型针对图片数据集构建图，图中的节点为数据中的标签，并使用word embedding（pre-trained glove）对节点特征进行初始化。<br />而对于图的边，也对应图卷积中的矩阵<span class="math inline">\(\boldsymbol{A}\)</span>（文中称其为相关系数矩阵），模型使用条件概率<span class="math inline">\(P\left(L_{j} \mid L_{i}\right)\)</span>进行建模，已期获得标签相关性信息。 <img src="/images/pd5/3.png" /> 具体而言，论文统计了数据集中的标签对的共现次数，然后构建共现矩阵，并设定一个阈值来进行二值化处理，借此过滤噪声边。 <img src="/images/pd5/1.png" title="基于多标签构建有向图" /></p><p>借助模型框架图可以看到，模型中图卷积模块起的是类似辅助分类器的作用，图中每个标签节点就是该标签的一个二分类器，将基于整个数据集训练的分类器<span class="math inline">\(\boldsymbol{W} \in \mathbb{R}^{C \times D}\)</span>与图像的特征<span class="math inline">\(x \in \mathbb{R}^{D}\)</span>进行点积，得到<span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^{C}\)</span>（C表示标签的总数）。图卷积利用的信息也只有图的边，也就是标签的共现，而后借助图卷积与图像特征提取进行共同训练，得到标签之间关系的隐式表示，最终推动更准确的多标签识别。</p><h2 id="实验">实验</h2><p><img src="/images/pd5/4.png" title="实验结果—非常不错 XD" /> 不过在尝试复现该模型时，本人试验了几个数据集似乎始终无法到达论文中的结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.03582&quot;&gt;Multi-Label Image Recognition with Graph Convolutional Networks&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;https</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="CV-GNN" scheme="http://example.com/tags/CV-GNN/"/>
    
  </entry>
  
  <entry>
    <title>Multi-hop Question Generation with Graph Convolutional Network [Arxiv]</title>
    <link href="http://example.com/2021/10/24/pd6/"/>
    <id>http://example.com/2021/10/24/pd6/</id>
    <published>2021-10-24T15:56:42.000Z</published>
    <updated>2021-10-27T17:48:33.014Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2010.09240.pdf">Multi-hop Question Generation with Graph Convolutional Network</a><br />Code: <a href="https://github.com/HLTCHKUST/MulQG">link</a></p><h2 id="background">Background</h2><p>问题生成(QG)是一个从给定的上下文自动生成问题或答案的任务，而多跳问题生成 (Multi-hop Question Generation) 需要从多个不同的段落中推理生成与答案相关的问题。QG可以应用于教育系统，也可以结合QA模型作为双重任务来增强QA系统的推理能力。对于多跳问题生成，核心问题在于如何连接多个段落间的零散的信息以及答案。</p><p><img src="/images/pd6/1.png" title="多跳问题生成" /></p><h2 id="模型">模型</h2><p><img src="/images/pd6/2.png" title="模型框架" /></p><h3 id="multi-hop-encoder">Multi-hop Encoder</h3><p>对于输入的文本段落和答案，先分割成word-level的token，并分别用pre-trained Glove进行embedding，并在文本的token embedding中加入答案 embedding tag。对于得到的token embedding 输入到LSTM-RNN中学习初步的上下文相关的representation，再输入到Encoder中，模型的Encoder包括三个部分:</p><ul><li><p>Answer-aware context encoder 这一部分参考了阅读理解中的co-attention reasoning机制: <span class="math display">\[\begin{aligned}S &amp;=C_{0}^{T} A_{0} \in R^{n \times m} \\S^{\prime} &amp;=\operatorname{softmax}(S) \in R^{n \times m} \\S^{\prime \prime} &amp;=\operatorname{softmax}\left(S^{T}\right) \in R^{m \times n} \\A_{0}^{\prime} &amp;=C_{0} \cdot S^{\prime} \in R^{d \times m} \\\tilde{C}_{1} &amp;=\left[A_{0} ; A_{0}^{\prime}\right] \cdot S^{\prime \prime} \in R^{2 d \times n}\\C_{1} &amp;=\operatorname{BiLSTM}\left(\left[\tilde{C}_{1} ; C_{0}\right]\right) \in R^{d \times n}\end{aligned}\]</span> 相关性矩阵S表示答案与上下文的相关性，整个过程比较复杂，这一模块的有效性在阅读理解任务中被验证，大致操作即将答案与文本计算attention后生成新的“答案”而后同样进行一遍相关性计算，最后输入Bi-LSTM中。</p></li><li><p>GCN-based entity-aware answer encoder 将上述encoder得到的embedding输入到GCN中进行多跳信息的嵌入。 <img src="/images/pd6/3.png" title="GCN-based entity-aware answer encoder" /> 图中的节点为文本中的命名实体（由BERT自动化提取），如果实体对在同一句子中，则为它们创建边。将上面Answer-aware context encoder 的结果结合到多跳图卷积中，并最终和图的结果结合，输入到Bi-attention模型，进一步得到token的representations <span class="math display">\[A_{1}=\text { BiAttention }\left(A_{0}, E_{M}\right)\]</span></p></li><li><p>Gated encoder reasoning layer 将前面得到的结果输入到门控网络进行特征融合，进行特征保留或遗忘，得到最终的Encoder结果。</p></li></ul><h3 id="maxout-pointer-decoder">Maxout Pointer Decoder</h3><p>模型采用单向LSTM作为解码器，而Maxout pointer这一模块也并不是由作者提出的，而是参考了他人的模型，用这一模块减少生成结果中的重复项。</p><h2 id="实验">实验</h2><p>实验部分，作者分别做了与现有multi-hop QG模型对比以及消融实验，取得了SOTA结果，并且证明了框架中每个模块的意义。 <img src="/images/pd6/4.png" title="纵向对比" /> <img src="/images/pd6/5.png" title="消融实验" /></p><h2 id="总结">总结</h2><p>本文提出的框架总体来说比较复杂。往牛了说可以理解为整个框架模拟了人类的问题生成的过程，包括整体文本和答案的阅读，进行大概了解，而后对文本和答案中的实体进行关注，并寻找他们的联系，最后在生成问题时确定核心和次要信息，生成相关的问题。不过事实上整个框架就是对几个现有模型中的部分模块进行组装，类似“搭积木”的过程。而新加入的multi-hop图卷积部分整体方法也不具有很亮点的想法，从消融实验结果中也可以看出这一模块对最终结果的提升也并不是很明显。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.09240.pdf&quot;&gt;Multi-hop Question Generation with Graph Convolutional Network&lt;/a&gt;&lt;br /&gt;
Code: &lt;a href=&quot;ht</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="QA" scheme="http://example.com/tags/QA/"/>
    
  </entry>
  
  <entry>
    <title>2021 MAXP命题赛 基于DGL的图机器学习任务</title>
    <link href="http://example.com/2021/10/23/maxp/"/>
    <id>http://example.com/2021/10/23/maxp/</id>
    <published>2021-10-23T07:33:06.000Z</published>
    <updated>2021-10-26T12:52:51.527Z</updated>
    
    <content type="html"><![CDATA[<p>使用<a href="https://www.dgl.ai/">Deep Graph Library (DGL)</a>进行图节点分类任务，使用的图数据是基于微软学术文献生成的论文关系图，其中的节点是论文，边是论文间的引用关系。整个图包括约150万个节点，2000万条边。节点包含300维的特征，来自论文的标题和摘要等内容。节点属于约50个类别。<br />比赛地址: <a href="https://biendata.xyz/competition/maxp_dgl/">MAXP</a></p><p><img src="/images/maxp/1.png" title="比赛数据集" /></p><h2 id="数据预处理">数据预处理</h2><p>根据所给的数据集，我们需要读取节点及其对应的特征，以及边。根据论文id构建对应的节点id，并分配他们的特征和类别。读取边之后发现存在部分论文没有出现在论文数据中，这部分的节点id分配到最后，这类论文没有类别和特征，这些点的特征可以选择用邻节点特征均值进行赋值。节点特征使用Numpy保存为.npy格式，方便后续读取。<br />对train文件中的数据进行Train/Valid分割，用于模型评估（9:1），将train/valid/test节点id和labels等数据保存为二进制文件方便快速读取 <img src="/images/maxp/3.png" /></p><h2 id="图构建">图构建</h2><p>根据上面的到的原论文id-引用论文id以及他们对应的节点id，借助DGL包构建论文引用关系图。 <img src="/images/maxp/2.png" title="构图" /> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">g = dgl.graph((u,v))</span><br><span class="line">g.nodes() <span class="comment">#获取节点id</span></span><br><span class="line">g.edges() <span class="comment">#获取边对应的节点（输出的是两个tensor）</span></span><br><span class="line">g.ndata(<span class="string">&#x27;feature&#x27;</span>) <span class="comment">#访问节点属性</span></span><br><span class="line">g.edata <span class="comment">#访问边属性</span></span><br></pre></td></tr></table></figure></p><h2 id="model_baseline">Model_baseline</h2><p>预处理和构图之后，我们模型输入的数据包括： <img src="/images/maxp/4.png" /></p><p>竞赛的baseline包括三个模型：</p><ul><li>graphsage</li><li>graphconvolution</li><li>graphattention</li></ul><p>其中各个网络由DGL.nn模块调库搭建。经过初步调参后发现网络深度为3层时三个模型结果最好，其中graphsage表现最好，在验证集上准确率接近54。 <img src="/images/maxp/5.png" title="可视化效果" /></p><h2 id="proposed-model">Proposed model</h2><p>数据中的节点特征已经给定，因此只能改变模型的网络结构，我参考了2020 acl的论文：Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks 所用的网络架构，具体采用的是三层图卷积后连接一层图注意力，并在图卷积与attention直接增加了skip-connection。在验证集上的到准确率为55+，目前排行榜上前10，后续将会做进一步调参来得到更好的结果。<br />另外，根据给定的特征直接使用MLP等前向传播网络虽然无法利用引用信息，但可以用来辅助最终的分类。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;使用&lt;a href=&quot;https://www.dgl.ai/&quot;&gt;Deep Graph Library (DGL)&lt;/a&gt;进行图节点分类任务，使用的图数据是基于微软学术文献生成的论文关系图，其中的节点是论文，边是论文间的引用关系。整个图包括约150万个节点，2000万条边。节</summary>
      
    
    
    
    <category term="Project" scheme="http://example.com/categories/Project/"/>
    
    
    <category term="Competition" scheme="http://example.com/tags/Competition/"/>
    
  </entry>
  
  <entry>
    <title>A Survey of Graph Neural Networks in Natural Language Processing [Arxiv]</title>
    <link href="http://example.com/2021/10/18/gnn-nlp/"/>
    <id>http://example.com/2021/10/18/gnn-nlp/</id>
    <published>2021-10-18T05:58:53.000Z</published>
    <updated>2021-10-26T15:36:48.929Z</updated>
    
    <content type="html"><![CDATA[<p>截止2021年初，关于现有的图神经网络应用在自然语言处理领域的综述<br />link: <a href="https://arxiv.org/abs/2106.06090">Graph Neural Networks for Natural Language Processing: A Survey</a></p><p>另外，本篇博客还包含另外几篇图相关的综述性文章内容：<br />Graph Representation Learning<br />Geometric Deep Learning</p><p>对于图的机器学习，非常容易想到的就是节点分类，边预测、以及图层级的分类等。对于传统的NLP问题，我们将输入的文本序列表示为图结构时，就可以借助图深度学习技术进行处理。整篇综述根据这一思路从图构造、图表示学习、基于图的Encoder-Decoder模型三方面进行介绍。</p><h2 id="图构造">图构造</h2><p>对于AI处理的数据类型，大概可以分类3类：Euclidean Structure、Sequence Structure、Graph Structure。<br />Eucildean data比如图，Sequence data如文本，这类数据都有一个特点：规则，即排列整齐；而图结构这类非欧几何数据，样本是不规则的，每个样本的邻居节点数量都是不同的，因此图像中的卷积操作就无法在图结构中应用。</p><p>针对输入的数据，包括文本、树等，我们根据一定的规则自动化构造构建不同类型的图，如无向图、有向图、多关系图、异构图并使用特点的GNN结构来进行学习。</p><h3 id="静态图构建">静态图构建</h3><p>利用规则或现有的关系解析工具在文本预处理时构造图结构，常见的静态图构建有：</p><ul><li><p>Dependency Graph<br />依赖图可以用于捕捉给定句子中两个主语之间的关系。对于给定的句子，可以借助现有的解析工具包得到dependency parsing tree，而后抽取出依赖关系，构建dependency graph</p></li><li><p>Constituency Graph<br />语言学中constituency relation指符合短语结构语法的关系，比如主语NP和谓语VP的关系 <img src="/images/pd3_1.png" /></p></li><li><p>Information Extraction Graph<br />IE Graph抽取出文本中跨越不同句的结构化的信息。构建IE图首先要提取三元组，而后通过不同三元组间共同参数来确定相同含义的实体进行合并，从而减少节点的数量，消除模糊性。 <img src="/images/pd3_2.png" /></p></li><li><p>Knowledge Graph<br />知识图谱能捕获实体以及关系，被广泛用于推理、关系抽取等任务中。知识图谱可以作为文本到embedding之间的一个精练且可解释的中间表示。KG可以表示为 <span class="math inline">\(\mathcal{G}(\mathcal{V}, \mathcal{E})\)</span>，由三元组<span class="math inline">\(\left(e_{1}, r e l, e_{2}\right)\)</span>。KG在不同的下游任务中起不同的作用，如机器翻译可以用于数据增强，阅读理解中用于构建子图。 <img src="/images/pd3_3.png" /></p></li><li><p>Co-occurrence Graph</p></li></ul><p>共现关系描述了两个词在固定大小的上下文窗口内共现的频率，而后单词和词与词间共现频率构建图。<br />除了上述几类图构建方法外，针对具体的任务还有很多不同的图构造方法。</p><h3 id="动态图构建">动态图构建</h3><p>静态图可以将数据的部分先验知识编码到图中，但是这需要大量的人力试验以及领域专业知识，且容易包含噪声。另外，静态图的构建是基于构建者自身的经验，得到的并不一定是对于某一下游任务最优的图。<br />而动态图则是动态学习图结构（加权邻接矩阵），图构造模块和后续图表示学习一起针对下游任务联合优化。图结构学习也是机器学习领域研究的热点问题 <img src="/images/pd3_4.png" alt="动态图构建方法" /></p><p><strong>Graph similarity metric learning</strong><br />图结构学习可以转化为节点相似度度量问题 (相似度矩阵<span class="math inline">\(S\)</span>)，对于相似度度量函数，可以分为两类：</p><ul><li>基于节点嵌入的相似度度量学习</li><li>基于结构感知的相似度度量学习</li></ul><p><em>基于节点嵌入的相似度函数</em>通过计算嵌入空间中节点的成对相似度来学习加权邻接矩阵。常见的度量函数包括基于注意力的度量函数和基于余弦的度量函数。 <span class="math display">\[S_{i, j}=\operatorname{ReLU}\left(\vec{W} \vec{v}_{i}\right)^{T} \operatorname{ReLU}\left(\vec{W} \vec{v}_{j}\right)\]</span> 上式为基于注意力的度量函数，<span class="math inline">\(\vec{W}\)</span>为可学习的权重。类似的，基于cosine的度量函数为： <span class="math display">\[\begin{aligned}S_{i, j}^{p} &amp;=\cos \left(\vec{w}_{p} \odot \vec{v}_{i}, \vec{w}_{p} \odot \vec{v}_{j}\right) \\S_{i, j} &amp;=\frac{1}{m} \sum_{p=1}^{m} S_{i j}^{p}\end{aligned}\]</span></p><p><em>基于结构感知的相似性函数</em>在节点信息之外还考虑了边的信息，如 <span class="math display">\[S_{i, j}^{l}=\operatorname{softmax}\left(\vec{u}^{T} \tanh \left(\vec{W}\left[\vec{h}_{i}^{l}, \vec{h}_{j}^{l}, \vec{v}_{i}, \vec{v}_{j}, \vec{e}_{i, j}\right]\right)\right)\]</span> 其中<span class="math inline">\(\vec{v}_{i}\)</span> 代表节点i的embedding， <span class="math inline">\(i\vec{e}_{i, j}\)</span> 代表边的embedding $ _{i}^{l}$ 代表节点i在GNN中第i层的embedding， <span class="math inline">\(\vec{u}\)</span>和<span class="math inline">\(\vec{W}\)</span> 是可训练的权重。</p><p><strong>Graph sparsification</strong><br />现实世界中大多数的图都是稀疏图，而通过相似度度量函数会得到任意两个节点之间的边，最终生成一个全连通图，这会极大增大开销，并引入噪声，因此需要进行图稀疏化处理。常用的方法包括取k个相似度最高的邻节点，或者给节点间的相似度设定一个阈值。</p><p>另外，静态图和动态图也可以结合起来，既可以加速训练，提高稳定性，也能提高下游任务的表现 <span class="math display">\[\widetilde{A}=\lambda L^{(0)}+(1-\lambda) \mathrm{f}(A)\]</span> 上式中<span class="math inline">\(L^{(0)}\)</span>表示静态图结构，<span class="math inline">\(\mathrm{f}(A)\)</span>表示可学习的动态图结构。</p><h2 id="图表示学习">图表示学习</h2><p>由于图的类型多种多样，如同构图、异构图、多关系图等等，这些不同的图上进行图表示学习的具体模型或有出入，但总体的步骤和思路基本类似。下面介绍的图表示学习方法是基于同构图，且节点与节点之间仅有一条无向边。</p><h3 id="basic-gnn">Basic GNN</h3><p>图神经网络对图中的节点进行embedding，并根据需求给出最终的node embedding或graph embedding。图神经网络的特征传播总体可以分成两个步骤，包括聚合-Aggregation和更新-Update。 <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\operatorname{AGGREGATE}^{(k)}\left(\left\{\mathbf{h}_{v}^{(k)}, \forall v \in \mathcal{N}(u)\right\}\right)\]</span> <span class="math display">\[\operatorname{UPDATE}\left(\mathbf{h}_{u}, \mathbf{m}_{\mathcal{N}(u)}\right)=\sigma\left(\mathbf{W}_{\text {self }} \mathbf{h}_{u}+\mathbf{W}_{\operatorname{neigh}} \mathbf{m}_{\mathcal{N}(u)}\right)\]</span></p><p>其中<span class="math inline">\(\mathcal{N}(u)\)</span>表示节点<span class="math inline">\(u\)</span>的邻节点，<span class="math inline">\(\mathbf{h}_{u}\)</span>则代表节点特征，<span class="math inline">\(\mathbf{W}\)</span>为可学习的权重矩阵。</p><h3 id="aggregation">Aggregation</h3><p>聚合操作将节点的邻节点特征进行汇总，常用的方法包括：</p><ul><li><p>Normalization：最基本的聚合方法就是对邻节点embedding求平均，并针对节点的度进行归一化 <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\frac{\sum_{v \in \mathcal{N}(u)} \mathbf{h}_{v}}{|\mathcal{N}(u)|}\]</span></p></li><li>Pooling：基于MLP这类的置换不变(permutation invariant)网络进行聚合，通用的pooling aggregator可以表示为： <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\operatorname{MLP}_{\theta}\left(\sum_{v \in N(u)} \operatorname{MLP}_{\phi}\left(\mathbf{h}_{v}\right)\right)\]</span> 另一种Janossy pooling则是赋予邻节点一个次序，并使用对于时序敏感的函数进行聚合 <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\operatorname{MLP}_{\theta}\left(\frac{1}{|\Pi|} \sum_{\pi \in \Pi} \rho_{\phi}\left(\mathbf{h}_{v_{1}}, \mathbf{h}_{v_{2}}, \ldots, \mathbf{h}_{v_{|\mathcal{N}(u)|}}\right)_{\pi_{i}}\right)\]</span></li><li><p>Attention： 对邻节点分配不同的权重，权重可以基于邻节点的embedding，也可以基于边的权值 <span class="math display">\[\mathbf{m}_{\mathcal{N}(u)}=\sum_{v \in \mathcal{N}(u)} \alpha_{u, v} \mathbf{h}_{v}\]</span> <span class="math display">\[\alpha_{u, v}=\frac{\exp \left(\mathbf{a}^{\top}\left[\mathbf{W h}_{u} \oplus \mathbf{W h}_{v}\right]\right)}{\sum_{v^{\prime} \in \mathcal{N}(u)} \exp \left(\mathbf{a}^{\top}\left[\mathbf{W h}_{u} \oplus \mathbf{W h}_{v^{\prime}}\right]\right)}\]</span></p></li></ul><h3 id="updates">Updates</h3><p>在经过多层图神经网络后，某些节点自身的特性会因为不断聚合邻节点的信息而淡化或被抹去，这就导致深度图神经网络的over-smoothing问题。<br />为缓解over-smoothing问题的一些技巧，比如Concatenation和Skip-Connections等在节点信息聚合后的更新操作入手。 <span class="math display">\[\text { UPDATE }_{\text {concat }}\left(\mathbf{h}_{u}, \mathbf{m}_{\mathcal{N}(u)}\right)=\left[\text { UPDATE }_{\text {base }}\left(\mathbf{h}_{u}, \mathbf{m}_{\mathcal{N}(u)}\right) \oplus \mathbf{h}_{u}\right]\]</span></p><h3 id="graph-convolutional-networks-gcn">Graph Convolutional Networks (GCN)</h3><p>图卷积就如同CV中的卷积，被提出后受到了广泛关注和研究。欧氏空间中的离散卷积我们很好理解，而对于非欧数据中的卷积，它的提出流程可以概括为：图信号处理GSP学者提出图的Fourier Transformation，进而得到Graph convolution，从而拓展到神经网络的图卷积网络。</p><p>图的卷积定义在spectral domain，相应的邻接矩阵<span class="math inline">\(A\)</span>用图的Laplacian 矩阵<span class="math inline">\(L\)</span>替代。<span class="math inline">\(L = D - A\)</span>，<span class="math inline">\(D\)</span>为度矩阵。把传统的傅里叶变换以及卷积迁移到Graph上, 核心工作就是把拉普拉斯算子的特征函数 <span class="math inline">\(e^{-i \omega t}\)</span> 变为Graph对应的拉普拉斯矩阵的特征向量。这其中具体的推导过程在此不再赘述。基本的GCN中第k层可以写为下式： <span class="math display">\[\mathbf{H}^{(k)}=\sigma\left(\tilde{\mathbf{A}} \mathbf{H}^{(k-1)} \mathbf{W}^{(k)}\right)\]</span> 其中<span class="math inline">\(\tilde{\mathbf{A}}=(\mathbf{D}+\mathbf{I})^{-\frac{1}{2}}(\mathbf{I}+\mathbf{A})(\mathbf{D}+\mathbf{I})^{-\frac{1}{2}}\)</span>，是拉普拉斯矩阵的一个变形形式。</p><h3 id="graphsage">GraphSAGE</h3><p>GraphSAGE这一图模型是归纳式 (inductive) 学习。不同于之前的transducer模型，GraphSAGE的目标不是学习到每个节点的embedding，而是学习生成embedding的聚合函数。 <img src="/images/pd3_5.png" /> 整个框架如上图所示，包括采样、聚合、预测三个步骤。在采样时会选择恒定数量的邻节点，且不仅仅选择1-hop的节点，而是考虑multi-hop。</p><h2 id="基于图的encoder-decoder模型">基于图的Encoder-Decoder模型</h2><p>Encoder-Decoder是深度学习模型中非常常见的架构，因此到了图深度学习领域，图到树、graph-graph等模型也应运而生。</p><h3 id="graph-to-sequence-model">Graph-to-Sequence Model</h3><p>这类模型通常用GNN作为Encoder，RNN/Transformer作为Decoder。此外，这类模型中多使用CNN进行节点特征初始化，用于捕捉GNN不敏感的连续词的潜在信息。这类模型在多关系图或异构图的处理上有所局限。</p><h3 id="graph-to-tree-model">Graph-to-Tree Model</h3><p>类似端到端的模型，在NLP任务中，树也具有很强大的表达能力。由于树广义上来讲也是一种图，因此Graph-Tree这类模型的核心在于借助self-attention进行获取局部邻节点的权重，再由decoder 生成包含语义的tree结构。这类模型的应用比如语义解析、数学应用问题（模型输出为由树来表示的方程）。</p><h3 id="graph-to-graph-model">Graph-to-Graph Model</h3><figure><img src="/images/pd3_6.png" alt="图的生成式模型(VAE)" /><figcaption>图的生成式模型(VAE)</figcaption></figure><h2 id="补充">补充</h2><h3 id="图神经网络在nlp中的下游任务">图神经网络在NLP中的下游任务</h3><p>已有的基于图相关技术的NLP任务包括自然语言生成、机器翻译、情感分类、文本分类、知识图谱补全、信息抽取（命名实体识别、关系抽取）、自然语言推理、解数学问题（文本）等</p><h3 id="关于图的semi-supervised">关于图的semi-supervised</h3><p>对于监督学习，如一个分类问题，我们的样本需要满足i.i.d assumption：样本之间是独立同分布的（不然还需要建模样本之间的联系）。然而在图结构上做诸如节点分类问题时，节点之间相互联系，且这些联系在节点分类中起到了重要的作用。因此基于图的很多深度学习是semi-supervised。这意味着在训练图模型时，我们利用了测试节点的信息，但不包括label。</p><h3 id="深度学习模型的迁移">深度学习模型的迁移</h3><p>近几年随着图神经网络的兴起，许多人都涌向这块处女地，深度学习模型中的一些经典思想和模型也被迁移到图相关的模型中，比如基于self-attention的GAT、GraphGAN、Graph Transformer等。另外，在NLP落地的经典搜索、广告、推荐算法中，图神经网络也被广泛应用。<br /><a href="https://arxiv.org/abs/1711.08267">GraphGAN: Graph Representation Learning with Generative Adversarial Nets</a><br /><a href="https://arxiv.org/pdf/1911.07470.pdf">Graph Transformer for Graph-to-Sequence Learning</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;截止2021年初，关于现有的图神经网络应用在自然语言处理领域的综述&lt;br /&gt;
link: &lt;a href=&quot;https://arxiv.org/abs/2106.06090&quot;&gt;Graph Neural Networks for Natural Language Proce</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
  </entry>
  
  <entry>
    <title>Vocabulary Learning via Optimal Transport for Neural Machine Translation [ACL2021]</title>
    <link href="http://example.com/2021/10/10/pd2/"/>
    <id>http://example.com/2021/10/10/pd2/</id>
    <published>2021-10-10T10:22:13.000Z</published>
    <updated>2021-10-26T15:36:20.914Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/2012.15671.pdf">Vocabulary Learning via Optimal Transport for Neural Machine Translation</a><br />ACL2021 Best paper Code: <a href="https://github.com/Jingjing-NLP/VOLT">link</a></p><p>这篇也就是被ICLR2021拒了后被评为ACL2021 best paper的文章，来自字节跳动的AI Lab。</p><h2 id="related-work">Related Work</h2><h3 id="subword-model">Subword model</h3><p>英文中传统分词方法基于空格进行tokenization。但这一方法面临OOV (Out Of Vocabulary)问题和同一单词的不同形态造成的冗余。因此如今BERT等模型多使用Subword模型，它的划分粒度介于词与字符之间。主流的(指某些中文网站上有博客介绍的）Subword model有Byte Pair Encoding (BPE), WordPiece和Unigram Language Model。</p><h3 id="byte-pair-encodingbpe">Byte-Pair-Encoding(BPE)</h3><p>&quot;Neural machine translation of rare words with subword units.&quot;arXiv preprint arXiv:1508.07909(2015).<br />BPE算法被用于处理NMT (Neural Machine Translation)任务中的OOV问题。<br />BPE是一种自下而上的压缩算法。将单词作为单词片段处理（word pieces），以便于处理未出现单词。</p><blockquote><p>we adopt BPE generated tokens as the token candidates.</p></blockquote><p>论文提出的算法要先用BPE...</p><h2 id="概要">概要</h2><p>机器翻译中，token vocabulary对最终结果会产生很大的影响。论文研究了词表的评价指标以及如何不通过训练直接找到最优的词表。文章的主要内容包括 1. 从信息论角度分析词表的作用 2.借助Optimal transport来找到最佳token词典 3. 更小的词表but更高的BLEU。</p><h2 id="intro">Intro</h2><p>词汇量（vocabulary size）会影响机器翻译任务的绩效，而通过遍历搜索来寻找最优的词汇量需要极高的计算开销，因此现有的研究大多采用统一的大小，如30k-40k。BPE通过选择频率最高的sub-words做为词典的token以进行数据压缩，以此减少熵。</p><p>语料熵随着词汇量的增加而减少，有利于模型学习。另一方面，过多的字符会导致字符稀疏化，这会损害模型学习。本文通过同时考虑熵和词汇量大小来探索自动词汇化，需要找到一个合适的目标函数来同时优化它们。其次，假设给出了适当的度量，由于指数搜索空间（<span class="math inline">\(2^N\)</span>)，解决这种离散优化问题仍然具有挑战性。</p><p>针对上述问题，论文提出VOcabulary Learning approach via optimal Transport, VOLT——最优传输的词汇学习方法</p><p>总的来说，论文的目标是1.得到“简洁而不臃肿”的词汇表 —— entropy-size trade off 2. 优化搜索过程。</p><h2 id="marginal-utility-of-vocabularization-muv">Marginal Utility of Vocabularization (MUV)</h2><p>借用经济学中的边际效应的概念，以词汇的边际效应（MUV）作为衡量标准， 然后将目标转向在可处理的时间复杂度中最大化 MUV。 <img src="/images/pd2_3.png" title="vocabulary的边际效益（没有显示给出MUV）" /></p><p>在经济学中，边际效应用于平衡收益和成本，因此论文使用 MUV 来平衡熵（收益）和词汇量（成本）。也就是从成本（大小）的增加中获得多大的收益（熵）。</p><p><img src="/images/pd2_1.png" title="MUV 与三分之二翻译任务的下游性能相关" /></p><h3 id="definition-of-muv">Definition of MUV</h3><p>MUV 表示熵对大小的负导数 <span class="math display">\[\mathcal{M}_{v(k+m)}=\frac{-\left(\mathcal{H}_{v(k+m)}-\mathcal{H}_{v(k)}\right)}{m}\]</span> 其中 <span class="math inline">\(v(k), v(k+m)\)</span> 是两个分别带有 <span class="math inline">\(k\)</span> 和 <span class="math inline">\(k+m\)</span> 个字符的词汇。<span class="math inline">\(\mathcal{H}_{v}\)</span> 表示词汇表 <span class="math inline">\(v\)</span> 语料库的樀，它由字符樀的总和定义。用字符的平均长度对熵进行归一化来避免字符长度的影响。最终的熵定义为： <span class="math display">\[\mathcal{H}_{v}=-\frac{1}{l_{v}} \sum_{j \in v} P(j) \log P(j)\]</span> <span class="math inline">\(P(i)\)</span> 是训练语料库中token <span class="math inline">\(i\)</span> 的相对频率, <span class="math inline">\(l_{v}\)</span> 是词汇表 <span class="math inline">\(v\)</span> 中token的平均长度。</p><h3 id="preliminary-results">Preliminary Results</h3><p>为了验证 MUV 作为词汇化衡量标准的有效性，作者对来自 TED 的 45 个语言对进行了实验，并计算了 MUV 和 BLEU 分数之间的Spearman相关系数(<span class="math inline">\(\rho\)</span>)。Spearman 得分为 0.4。</p><blockquote><p>We believe that it is a good signal to show MUV matters</p></blockquote><p>有了MUV作为评价指标，我们有两个选择来获得最终词表：搜索和学习。作者认为基于学习是更高效的，因此进一步探索了一种基于学习的解决方案 VOLT。（当然最终借助实验比较了 MUV-Search 和 VOLT的性能。）</p><h2 id="maximizing-muv-via-optimal-transport">Maximizing MUV via Optimal Transport</h2><h3 id="优化问题">优化问题</h3><p>首先引入一个辅助变量<span class="math inline">\(S\)</span>，<span class="math inline">\(\boldsymbol{S}=\{k, 2 \cdot k, \ldots,(t-1) \cdot k, \cdots\}\)</span>。 <span class="math inline">\(S\)</span>是一个递增序列，对于每个时间戳t，<span class="math inline">\(S[t]\)</span>代表<strong>不多于<span class="math inline">\(S[t]\)</span>个词条的词表集合</strong>。引入这一变量，根据递推关系来计算任意一个词表的MUV（借助前一个时间戳s[t-1]上的词表递进计算）</p><p><span class="math inline">\(k\)</span>代表前后两个词表<span class="math inline">\(v(t)\)</span>和<span class="math inline">\(v(t-1)\)</span>之间的大小差（size gap）。我们的目标是找到MUV最高的<span class="math inline">\(v[t]\)</span> <span class="math display">\[\begin{array}{l}\underset{t}{\arg \max } \underset{v(t-1) \in \mathbb{V}_{\boldsymbol{S}[t-1]}, v(t) \in \mathbb{V}_{S[t]}}{\arg \max } \mathcal{M}_{v(t)}= \\\underset{t}{\arg \max } \underset{v(t-1) \in \mathbb{V}_{\boldsymbol{S}[t-1]}, v(t) \in \mathbb{V}_{S[t]}}{\arg \max }-\frac{1}{k}\left[\mathcal{H}_{v(t)}-\mathcal{H}_{v(t-1)}\right]\end{array}\]</span> <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t-1]}\)</span>和 <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t]}\)</span>表示两个词表的集合，其中每个词表大小的上界为<span class="math inline">\(s[t-1]\)</span>和<span class="math inline">\(s[t]\)</span></p><blockquote><p>The inner arg max represents that the target is to find the vocabulary from <span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t]}\)</span> with the maximum MUV scores. The outer arg max means that the target is to enumerate all timesteps and find the vocabulary with the maximum MUV scores.</p></blockquote><p>遍历t，遍历<span class="math inline">\(\mathbb{V}_{\boldsymbol{S}[t-1]}\)</span>。<br />（词表越大熵越小）上述公式意味着从v(t-1)这个词表，增加i个词/tokens之后，期望新得到的v(t)词表的熵降低的最多。即两个词表对应的熵的差值越大越好。</p><blockquote><p>Due to exponential search space, we propose to optimize its upper bound: <span class="math display">\[\underset{t}{\arg \max } \frac{1}{k}\left[\underset{v(t) \in \mathbb{V}_{S[t]}}{\arg \max } \mathcal{H}_{v(t)}-\underset{v(t-1) \in \mathbb{V}_{S[t-11}}{\arg \max } \mathcal{H}_{v(t-1)}\right]\]</span></p></blockquote><p>(论文ArXiv上的前一版本中写的还是lower bound...而最新版放的是upper bound...)<br />anyway至此整个方法可以分成两个步骤：</p><ul><li>每个时间步t上，寻找最优的词表（按照最大化熵来寻找）</li><li>枚举每个时间步t，并输出满足上一个公式的词表（对应的就是时间步t的”最优词表“）</li></ul><p>step1的目标就是最大化： <span class="math display">\[\underset{v(t) \in \mathbb{V}_{\boldsymbol{S}[t]}}{\arg \max }-\frac{1}{l_{v(t)}} \sum_{j \in v(t)} P(j) \log P(j)\]</span> <span class="math inline">\(l_{v}\)</span>是每个token的平均字符长度，<span class="math inline">\(P(j)\)</span>是token j的概率（频率）</p><blockquote><p>However, notice that this problem is in general intractable due to the extensive vocabulary size. Therefore, we instead propose a relaxation in the formulation of discrete optimal transport, which can then be solved efficiently via the Sinkhorn algorithm</p></blockquote><p>借助最优传输OT的思想，松弛原优化问题，进而用信息论中的Sinkhorn algorithm求解。</p><h3 id="optimal-transport-不太懂">Optimal Transport （不太懂）</h3><p><img src="/images/pd2_2.png" title="寻找一个从“character分布、单字分布”到“词表词条分布”的一个最优的运输矩阵的过程" /></p><ul><li>每个transport matrix对应一个词表；</li><li>transport matrix决定有多少chars被“运输”到token候选（词条候选）；</li><li>长度为0的tokens（包含0个chars)，不会被增加到词表。</li></ul><p>不同的”运输矩阵“会带来不同的”运输开销”。而最优化运输（路径）问题的目标就是寻找一个“运输矩阵“，使得”运输开销“(即，负熵)最小化。</p><p>目标函数： <span class="math display">\[\begin{array}{c}\min _{v \in \mathbb{V}_{S[t]}} \frac{1}{l_{v}} \sum_{j \in v} P(j) \log P(j) \\\text { s.t. } \quad P(j)=\frac{\operatorname{Token}(j)}{\sum_{j \in v} \operatorname{Token}(j)}, l_{v}=\frac{\sum_{j \in v} \operatorname{len}(j)}{|v|}\end{array}\]</span></p><p>近似(obtain a tractable lower bound of entropy) - 启发式规则(最长词条匹配原则) - 变换为两部分损失</p><p>复杂的推导后得到： <span class="math display">\[\min _{\boldsymbol{P} \in \mathbb{R}^{m \times n}}\langle\boldsymbol{P}, \boldsymbol{D}\rangle-\gamma H(\boldsymbol{P})\]</span></p><p><span class="math display">\[\boldsymbol{D}(j, i)=\left\{\begin{array}{ll}-\log P(i \mid j)=+\infty, &amp; \text { if } i \notin j \\-\log P(i \mid j)=-\log \frac{1}{\operatorname{len}(j)}, &amp; \text { otherwise }\end{array}\right.\]</span></p><p><img src="/images/pd2_4.png" title="算法（不太复杂）" /></p><h2 id="实验">实验</h2><p>3个数据集上NMT任务的BLEU比较（双语语料、多语语料等）<br />VOLT对比BPE、MUV search更加高效</p><p>最后，全论文的核心应该是MUV的提出以及用OT进行优化这一trick，实验结果也比较solid，虽然最终的算法并不复杂，但是OT部分Sinkhorn算法需要较强的信息论背景，本CS专业看了半年仍是一脸懵。<br />指路一篇介绍Sinkhorn算法的链接： <a href="https://arxiv.org/pdf/1803.00567.pdf" class="uri">https://arxiv.org/pdf/1803.00567.pdf</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/2012.15671.pdf&quot;&gt;Vocabulary Learning via Optimal Transport for Neural Machine Translation&lt;/a&gt;&lt;br /&gt;
ACL2021</summary>
      
    
    
    
    <category term="Paper_daily" scheme="http://example.com/categories/Paper-daily/"/>
    
    
    <category term="ACL2021" scheme="http://example.com/tags/ACL2021/"/>
    
  </entry>
  
</feed>
